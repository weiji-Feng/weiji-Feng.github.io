<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation | XDU微积冯</title><meta name="author" content="weiji-Feng"><meta name="copyright" content="weiji-Feng"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="要在图像翻译领域使用对比学习？请用足够强悍的负样本！">
<meta property="og:type" content="article">
<meta property="og:title" content="Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation">
<meta property="og:url" content="https://weiji-feng.github.io/2023/06/09/NEGCUT/index.html">
<meta property="og:site_name" content="XDU微积冯">
<meta property="og:description" content="要在图像翻译领域使用对比学习？请用足够强悍的负样本！">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.postimg.cc/8PjpQGYb/meinv.jpg">
<meta property="article:published_time" content="2023-06-09T06:31:37.000Z">
<meta property="article:modified_time" content="2023-06-26T17:25:57.195Z">
<meta property="article:author" content="weiji-Feng">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="papers reproduced">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.postimg.cc/8PjpQGYb/meinv.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://weiji-feng.github.io/2023/06/09/NEGCUT/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-27 01:25:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/starlight.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_4098087_izllhdh31rn.css"><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/resume/cv_zh.pdf"><i class="fa-fw fa fa-graduation-cap"></i><span> 简历</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-user-lock"></i><span> 暗格</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.postimg.cc/8PjpQGYb/meinv.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="XDU微积冯"><span class="site-name">XDU微积冯</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/resume/cv_zh.pdf"><i class="fa-fw fa fa-graduation-cap"></i><span> 简历</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-user-lock"></i><span> 暗格</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-09T06:31:37.000Z" title="发表于 2023-06-09 14:31:37">2023-06-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-26T17:25:57.195Z" title="更新于 2023-06-27 01:25:57">2023-06-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%8F%E4%BB%A4%E8%90%A5%E7%BB%8F%E5%8E%86/">夏令营经历</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Semantic-segmentation/">Semantic segmentation</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="论文阅读-Instance-wise-Hard-Negative-Example-Generation-for-Contrastive-Learning-in-Unpaired-Image-to-Image-Translation"><a href="#论文阅读-Instance-wise-Hard-Negative-Example-Generation-for-Contrastive-Learning-in-Unpaired-Image-to-Image-Translation" class="headerlink" title="论文阅读 - Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation"></a>论文阅读 - Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</h1><p>论文有一个重要的结论：在基于对比学习的图像翻译任务中，负样本(negative examples)的质量和难度是非常重要的。和之前的方法<code>CUT</code>相比，本文提出的方法能够产生足够challenge的负样本，从而使得对比学习可以捕捉那些具有细粒度的可区分特征。</p>
<p>我们可以从图1中看到，与本文提出的方法相比，<code>CUT</code>方法产生的负例与query相似度不高，这也限制了<code>CUT</code>方法的效果：</p>
<p><figure class="half">
<img src="/2023/06/09/NEGCUT/1.png" width="400">
</figure></p>
<center>图1. The distribution of similarity in CUT and NEGCUT</center>

<p>由于论文是对<code>CUT</code>方法的一个改进，故我们首先引入<code>CUT</code>方法，来探索其使用对比学习的动机；再详细谈谈论文的方法<code>NEGCUT</code>是如何改进负样本的生成方法的；最后，我们沿着<code>负样本的质量和难度</code>这一线索，尝试阐述自己的一些想法。</p>
<h2 id="1-论文介绍"><a href="#1-论文介绍" class="headerlink" title="1. 论文介绍"></a>1. 论文介绍</h2><h3 id="1-1-CUT-Contrastive-Learning-for-Unpaired-Image-to-Image-Translation"><a href="#1-1-CUT-Contrastive-Learning-for-Unpaired-Image-to-Image-Translation" class="headerlink" title="1.1 CUT - Contrastive Learning for Unpaired Image-to-Image Translation"></a>1.1 CUT - Contrastive Learning for Unpaired Image-to-Image Translation</h3><p>图像翻译任务的核心工作就是在跨域生成图像时<u><strong>分离content和appearance</strong></u>，保留原图中的内容，将外观改变为目标domain的样式。举例而言，图像中对应<code>zebra forehead</code>的patch在经过generator生成后，应当是<code>horse forehead</code>而不是其他的部位。</p>
<p><code>cycle-consistency</code>为了在非配对图像翻译任务中<strong>保证翻译(生成)前后图像信息的一致性</strong>，假设图像翻译任务的两个domain满足双射关系，这个严格的限制使其在相当一部分任务上(尤其是两个domain信息量差距较大时)效果不佳。同时，其需要一对generator来生成不同域的图像，对于只需要one-sided translation的任务而言无疑是增加了工作量。</p>
<p>而<code>CUT</code>则是基于互信息最大化思想，利用对比学习思想去捕捉对应的输入输出图像之间的共性部分，从而鼓励保留那些content信息。同时，作者注意到图像在每个patch上也是满足content不变的性质。于是作者采用patchwise的<code>InfoNCE Loss</code>来完成这个任务。</p>
<p>结构上，论文需要一个generator进行domain adaptation，并且需要一个encoder捕获content信息并进行对比学习，框架图2所示：</p>
<p><figure class="half">
<img src="/2023/06/09/NEGCUT/2.png" width="800">
</figure></p>
<center>图1. CUT模型结构</center>

<p>于是他自然需要以下2个损失函数部分：</p>
<ul>
<li>Adversarial loss：鼓励生成的图像和目标domain有一致的“风格”；</li>
<li>Mutual information maximization：鼓励输入和输出的对应位置有更紧密的联系，或是互信息。</li>
</ul>
<h4 id="1-1-1-对比损失"><a href="#1-1-1-对比损失" class="headerlink" title="1.1.1 对比损失"></a>1.1.1 对比损失</h4><p>由于对比损失是GAN-based模型所必须的损失，我们不再详细探讨它，直接给出公式如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text{GAN}}(G,D,X,Y) = \Bbb{E}_{\pmb{y}\sim Y}\log D(\pmb{y}) + \Bbb{E}_{\pmb{x}\sim X}\log (1 - D(G(\pmb{x})))\tag{1}</script><h4 id="1-1-2-互信息最大化"><a href="#1-1-2-互信息最大化" class="headerlink" title="1.1.2 互信息最大化"></a>1.1.2 互信息最大化</h4><p>如果要使用对比学习来实现互信息的最大化，应当使<code>query</code>和<code>positive</code>两个信号关联起来，而数据集中的其他信息都被定义为<code>negative</code>。如果我们令 $\text{query}=v,\ \text{positive}=v^+,\ \text{negative}=v^-$，我们当然希望 $v$ 和 $v^+$ 的距离/相似度最高，和negative的相似度越低，故我们可以定义一个<code>InfoNCE Loss</code>：</p>
<script type="math/tex; mode=display">
\mathscr{l}(v,v^+,v^-) = -\log\left[\frac{\exp(v\cdot v^+/\tau)}{\exp(v\cdot v^+/\tau) + \sum_{n=1}^N\exp(v\cdot v^-_n/\tau)} \right]\tag{2}</script><p>上式实际上是通过交叉熵损失进行计算的。</p>
<p>现在，我们的重点转移到如何选择对比学习的对象(即image or patch)、如何获取负样本。</p>
<p>论文提到，在图像翻译任务中，不仅生成前后的图像共享content，对应的patch也是共享content的，他们采取了一个patch-based、multilayer的对比学习策略。</p>
<p>对于每一层，模型设计了一个encoder(来自generator)+MLP的编码结构，如下图所示：</p>
<p><figure class="half">
<img src="/2023/06/09/NEGCUT/3.png" width="800">
</figure></p>
<center>图3. Patchwise Contrastive Loss</center>

<p>一个patch在第$l$层上的编码向量表示为 </p>
<script type="math/tex; mode=display">
\{z_l\}_{L} = \{H_l(G_{enc}^l(x))\}_{L}</script><p>设这一层的特征有 $S_l$ 个position，则其第 $s$ 个position的向量应该为 $z_l^s$ 。同理我们也给目标domain类似的定义(将 $z$ 转换为 $\hat{z}$ )，那么式(2)可以更新为：</p>
<script type="math/tex; mode=display">
\mathscr{L}_{\text{PatchNCE}}(G,H,X) = \Bbb{E}_{x\sim X}\sum_{l=1}^L\sum_{s=1}^{S_l} \mathscr{l}\left(\hat z_l^s, z_l^s, z_l^{S \backslash s}\right)\tag{3}</script><p>其中我们将目标domain的patch设为<code>query</code>，将input的对应patch设为<code>positive</code>，图片中其他位置的patch设为<code>negative</code>。我们只分享within图像的负样本，不在赘述external的情况。</p>
<p>最后，为了避免生成图像产生额外不必要的变化并且让generator更focus on那些content的信息，论文引入了一个<code>identity loss</code>(与CycleGAN中提到的类似) $\mathscr{L}_{\text{PatchNCE}}(G,H,Y)$，总的训练损失函数定义为：</p>
<script type="math/tex; mode=display">
\mathcal{L} = \mathcal{L}_{\text{GAN}}(G,D,X,Y) + \lambda_X \mathcal{L}_{\text{PatchNCE}}(G,H,X) + \lambda_Y \mathcal{L}_{\text{PatchNCE}}(G,H,Y)\tag{4}</script><h3 id="1-2-NEGCUT"><a href="#1-2-NEGCUT" class="headerlink" title="1.2 NEGCUT"></a>1.2 NEGCUT</h3><p>论文的模型结构和<code>CUT</code>方法中的类似，都是通过一个generator生成目标domain的图像，并通过generator的encoder去获取特征向量，进行多层的对比学习。论文的模型结构如下图所示：<br><img src="/2023/06/09/NEGCUT/4.png" alt="Alt text"><br><a id="fig4"><center>图4. NEGCUT framework</center> </a></p>
<p>下面，我们着重理解论文制造<code>Hard Negative Examples</code>的方法。对于从encoder中的某一层 $l$ 获得的图像特征，论文使用一个由2-layer MLP组成的<code>Representation Network</code> $H^i(\cdot)$来进一步提取高维表示。</p>
<p>与<code>CUT</code>类似，论文对空间维数中的S个位置进行随机抽样，并以归一化向量作为<code>query</code>和<code>positive</code>进行对比学习，公式如下：</p>
<script type="math/tex; mode=display">
q = \frac{H_s^i(F_i^{\pmb{\text{Y}}})}{\Vert H_s^i(F_i^{\pmb{\text{Y}}}) \Vert_2},\quad
k^+ = \frac{H_s^i(F_i^{\pmb{\text{X}}})}{\Vert H_s^i(F_i^{\pmb{\text{X}}}) \Vert_2} 
\tag{5}</script><p>其中 $F_i$ 是encoder第i层获取的特征，$Y$ 是目标域，$X$ 是源域(source domain)。下标 $s$ 表示采样到的样本位置。</p>
<p>现在，我们需要第i层的<code>negative</code>样本特征。由于图像内的patch不够challenge，论文采用生成的办法去创造负样本。于是论文为第 $i$ 层设计了一个独立的<code>negative generator</code> $N^i$，它将接受<code>Representation Network</code>中的spatially-average特征 $\overline{H^i(F_i^{\pmb{\text{X}}})}$，和一个提供多样性的噪声 $z_n$，输出一个生成的<a id="tag6">负样本</a>：</p>
<script type="math/tex; mode=display">
k_{\text{adv},n}^- = \frac{N^i(\overline{H^i(F_i^{\pmb{\text{X}}})};z_n)}{\Vert  N^i(\overline{H^i(F_i^{\pmb{\text{X}}})};z_n) \Vert_2}\tag{6}</script><p>其中我们可以为每一个正样本采样多个 $z_n\sim N(0,1)$ 从而生成多个负样本。</p>
<p>为了使<code>negative generator</code>能够生成足够有挑战的负样本，论文选择将encoder(包含representation network)和negative generator进行对抗学习，损失如下所示：</p>
<script type="math/tex; mode=display">
\min_{\theta_{\mathcal{H}},\theta_{\mathcal{G}}}\max_{\theta_{\mathcal{N}}} \mathcal{l}(\text{q},\text{k}^+,\text{k}_{\text{adv}}^-) = -\log \left[ 
    \frac{\exp(\text{q}\cdot \text{k}^+/\tau)}{\exp(\text{q}\cdot \text{k}^+/\tau) + \sum_{n=1}^N \exp(\text{q}\cdot \text{k}_{\text{adv},n}^-/\tau)}
 \right]\tag{7}</script><p>其中，<code>negative generator</code>通过产生challenge的负样本来影响式(7)的分母使其最大化。而encoder和<code>Representation Network</code>则尽可能让query和正样本接近而使损失最小化，从而达到均衡。</p>
<p>由于论文采用了多层的对比学习，所以最终的对比学习损失如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{AdCount} = \Bbb{E}_{x\sim X}\sum_{l=1}^L\sum_{s=1}^{S_l} \mathcal{l}(\text{q}_{l,s},\text{k}^+_{l,a},\text{k}^-_{\text{adv},l,s})\tag{8}</script><p>为了避免模式崩溃mode collapse，论文引入一个<code>diversity loss</code>，迫使生成的负样本具有多样性：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{div} = -\Vert N^i(\overline{H^i(\pmb{\text{X}}_i)};z_1) - N^i(\overline{H^i(\pmb{\text{X}}_i)};z_2) \Vert_1 \tag{9}</script><p>当然，由于论文也是GAN-based，需要一个对抗损失来保证生成器生成的图片足够真实，符合目标域的特性：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L_{gan}^D &= \Bbb{E}_{x_r}\left[(1-\pmb{\text{D}}(x_r))^2 \right] + \Bbb{E}_{x_f}\left[\pmb{\text{D}}(x_f)^2 \right],\\
L_{gan}^G &= \Bbb{E}_{x_f}\left[(1-\pmb{\text{D}}(x_f))^2 \right]
\end{aligned}\tag{10}</script><p>论文还将损失准确分配给了不同的模型，如：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}_{\mathcal{H}} &= \mathcal{L}_{AdCount}, \\
\mathcal{L}_{\mathcal{G}} &= \mathcal{L}_{AdCount} + \lambda_1 L_{gan}^G, \\
\mathcal{L}_{\mathcal{G}} &= -\mathcal{L}_{AdCount} + \lambda_2 L_{div}, \\
\end{aligned}\tag{11}</script><p>仔细看图<a href="#fig4">4</a>，其实可以看到损失的反向传播过程：例如<code>contrastive loss</code>(应该是公式(7)中要最小化的部分)将从representation network开始传播，而<code>adversarial contrastive loss</code>则仅用以训练负例生成器等。</p>
<h3 id="1-3-Idea-Mining"><a href="#1-3-Idea-Mining" class="headerlink" title="1.3 Idea Mining"></a>1.3 Idea Mining</h3><p>鉴于<code>NEGCUT</code>做了一个很好的改进，使得我认为对比学习在unpaired图像翻译任务中还能发挥更强烈的作用。下面，我来阐述一下我通过阅读论文做的一些思考：</p>
<ul>
<li><p><strong>为什么公式<a href="#tag6">6</a>会选择将空间的均值作为负生成器的输入</strong>：初看文章，我就有这个疑惑。我完全可以选择patch附近的空间做均值，这应该会比使用所有position的均值更具challenge。但与<code>CUT</code>对比后，我发现这个想法应该源于<code>CUT</code>模型将图像内patch作为负样例的思想：做了均值后的向量总会包含整张图片的信息，添加噪声 $z$ 后理论上可以与图片中的任意一个patch近似。如图5所示，可以发现<code>negative generator</code>总是可以生成与当前patch较为相似的负例向量。所以，该方法其实可以获取很多与当前patch接近，但不局限于图像内的负样例。最后，使用附近patch的均值可能会产生太过困难的负样本，阻碍了正样本和query的学习。<br><figure class="half">
<img src="/2023/06/09/NEGCUT/5.png" width="800">
</figure></p>
<center>图5. Negative examples visualization</center>
</li>
<li><p><strong>如何保证生成的负样本的分布是真实的</strong>：从消融实验里，可以看到负例确实能够检索到与正样本相近的patch，但没有对其分布做进一步探索，例如：它真的属于source domain吗？这似乎并没有确切的证明他们。我想，设计一个判别器来判别生成的负例是否服从真实负样本的分布可能是一个解决方法。</p>
</li>
</ul>
<p>另外，根据这个问题，我依然想通过<code>CUT</code>的方法——即图像内部的patch进行解决。<code>CUT</code>方法中将所有 $S-s$ positions的向量引入<code>PatchNCE Loss</code>中，给予了它们<strong>相同的权重</strong>，即：</p>
<script type="math/tex; mode=display">
\text{Total}_{neg} = \sum_{n=1}^N \exp(v\cdot v_n^- / \tau)\tag{12}</script><p>那如果我们可以分配给hard negative examples一个更高的权重，或许又会有更好的效果。</p>
<ul>
<li><strong>对比学习如何保证translate前后content的一致性</strong>：准确来说，对比学习通过拉近<code>query</code>图像patch和<code>positive</code>图像patch从而保留其内容相近。但是patch级的局部对比学习或许会影响图像级的content信息，例如source domain两个接近的patch反而在目标域不再接近。因此我认为引入patch之间的距离或相似度来监督图像translation的质量也是有一定意义的。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://weiji-feng.github.io">weiji-Feng</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://weiji-feng.github.io/2023/06/09/NEGCUT/">https://weiji-feng.github.io/2023/06/09/NEGCUT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://weiji-feng.github.io" target="_blank">XDU微积冯</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning</a><a class="post-meta__tags" href="/tags/papers-reproduced/">papers reproduced</a></div><div class="post_share"><div class="social-share" data-image="https://i.postimg.cc/8PjpQGYb/meinv.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/06/28/PTH/" title=""><img class="cover" src="https://i.postimg.cc/8PjpQGYb/meinv.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2023/06/09/unimatch/" title="Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation"><img class="cover" src="https://i.postimg.cc/8PjpQGYb/meinv.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/06/09/unimatch/" title="Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation"><img class="cover" src="https://i.postimg.cc/8PjpQGYb/meinv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-09</div><div class="title">Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</div></div></a></div><div><a href="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/" title="Contrastive Learning based Vision-Language Pre-Training"><img class="cover" src="https://i.postimg.cc/RhHyTvGM/chifan.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-04</div><div class="title">Contrastive Learning based Vision-Language Pre-Training</div></div></a></div><div><a href="/2023/06/03/Survey4MATH/" title="Large-Language-Model For Math"><img class="cover" src="https://i.postimg.cc/RhHyTvGM/chifan.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-03</div><div class="title">Large-Language-Model For Math</div></div></a></div><div><a href="/2023/05/23/Image2Poem/" title="Image2Poem"><img class="cover" src="https://i.postimg.cc/fyDqYY3v/shaonv.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-23</div><div class="title">Image2Poem</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">weiji-Feng</div><div class="author-info__description">数学鬼才</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiji-Feng"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiji-Feng" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/khfeng@stu.xidian.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.zhihu.com/people/wan-shi-sheng-yi-lx" target="_blank" title="知乎"><i class="iconfont icon-shejiaotubiao-10"></i></a><a class="social-icon" href="https://blog.csdn.net/FKH20009200446?type=blog" target="_blank" title="CSDN"><i class="iconfont icon-csdn"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Instance-wise-Hard-Negative-Example-Generation-for-Contrastive-Learning-in-Unpaired-Image-to-Image-Translation"><span class="toc-number">1.</span> <span class="toc-text">论文阅读 - Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%AE%BA%E6%96%87%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">1. 论文介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-CUT-Contrastive-Learning-for-Unpaired-Image-to-Image-Translation"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 CUT - Contrastive Learning for Unpaired Image-to-Image Translation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">1.1.1 对比损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-%E4%BA%92%E4%BF%A1%E6%81%AF%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">1.1.2 互信息最大化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-NEGCUT"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 NEGCUT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Idea-Mining"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 Idea Mining</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/06/28/PTH/" title="无题"><img src="https://i.postimg.cc/8PjpQGYb/meinv.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2023/06/28/PTH/" title="无题">无题</a><time datetime="2023-06-28T08:02:01.649Z" title="发表于 2023-06-28 16:02:01">2023-06-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/09/NEGCUT/" title="Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation"><img src="https://i.postimg.cc/8PjpQGYb/meinv.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation"/></a><div class="content"><a class="title" href="/2023/06/09/NEGCUT/" title="Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation">Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</a><time datetime="2023-06-09T06:31:37.000Z" title="发表于 2023-06-09 14:31:37">2023-06-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/09/unimatch/" title="Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation"><img src="https://i.postimg.cc/8PjpQGYb/meinv.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation"/></a><div class="content"><a class="title" href="/2023/06/09/unimatch/" title="Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation">Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</a><time datetime="2023-06-09T06:31:37.000Z" title="发表于 2023-06-09 14:31:37">2023-06-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/" title="Contrastive Learning based Vision-Language Pre-Training"><img src="https://i.postimg.cc/RhHyTvGM/chifan.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Contrastive Learning based Vision-Language Pre-Training"/></a><div class="content"><a class="title" href="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/" title="Contrastive Learning based Vision-Language Pre-Training">Contrastive Learning based Vision-Language Pre-Training</a><time datetime="2023-06-04T13:14:05.000Z" title="发表于 2023-06-04 21:14:05">2023-06-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/03/Survey4MATH/" title="Large-Language-Model For Math"><img src="https://i.postimg.cc/RhHyTvGM/chifan.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Large-Language-Model For Math"/></a><div class="content"><a class="title" href="/2023/06/03/Survey4MATH/" title="Large-Language-Model For Math">Large-Language-Model For Math</a><time datetime="2023-06-03T15:36:37.000Z" title="发表于 2023-06-03 23:36:37">2023-06-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By weiji-Feng</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script src="/js/jquery.js"></script><script src="/js/foot.js"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script src="/js/switch.js" async></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<div class="recent-post-item" id="gitcalendarBar" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://git-calender.nickxu.top/api?weiji-Feng",['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'],'weiji-Feng')
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/koharu.model.json"},"display":{"position":"left","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.9},"log":false});</script></body></html>