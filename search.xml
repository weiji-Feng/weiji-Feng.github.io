<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2023/06/28/PTH/"/>
      <url>/2023/06/28/PTH/</url>
      
        <content type="html"><![CDATA[<h1 id="论文阅读：Post-Tuned-Hashing-A-New-Approach-to-Indexing-High-dimensional-Data"><a href="#论文阅读：Post-Tuned-Hashing-A-New-Approach-to-Indexing-High-dimensional-Data" class="headerlink" title="论文阅读：Post Tuned Hashing: A New Approach to Indexing High-dimensional Data"></a>论文阅读：Post Tuned Hashing: A New Approach to Indexing High-dimensional Data</h1><h2 id="1-方法介绍"><a href="#1-方法介绍" class="headerlink" title="1. 方法介绍"></a>1. 方法介绍</h2><p><code>Learning to hash</code>是进行高维数据索引的有效方法，它的目标是：学习一个<code>low-dimensional</code>的、<code>similarity-preserving</code>的二进制编码表示，使该其可以代表原高维空间的数据。本文基于<code>Unsupervised hashing method</code>思想，通过探索数据内在特征与属性，来学习二进制码表示。</p><p>传统的无监督hashing基本都通过2阶段的范式来实现：</p><ol><li>mapping：将高维原始数据投影到低维空间；</li><li>binarization：将投影向量二值化，变成一个“二进制”编码。</li></ol><p>并且为了保证原有的<code>neighborhood relationships</code>，他们大都选择在投影阶段或者二值化阶段做出优化。</p><p>但是，论文指出，尽管他们努力地在某个阶段保证了邻域关系(例如在降维时保证了邻域关系)，但是二值化过程不可避免地造成了<code>neighborhood error</code>：即原数据中相邻的数据可能会产生不相似的二进制编码，而不相邻数据可能会产生相似编码。于是论文提出了一个全新的步骤，<code>post-tuning stage</code>，意图使用该方法在二值化后重建邻域关系，减小邻域损失。图1展示了加入新的步骤前后，邻域损失情况。</p><p><figure class="half"><img src="/2023/06/28/PTH/1.png" width="1000"></figure></p><center>图1. Toy illustration of the superiority of post-tuning.</center><p>值得注意的是，该步骤独立于前两个步骤，因此可以广泛适配传统的2阶段方法。</p><h2 id="2-方法框架分析与推导"><a href="#2-方法框架分析与推导" class="headerlink" title="2. 方法框架分析与推导"></a>2. 方法框架分析与推导</h2><p>下面我们对该方法的理论做一定的阐述与推导：</p><p>首先，我们设数据集 $X\in \Bbb{R}^{d\times n}$ 包含n个数据，每个数据的维度为d。为了完成无监督hashing的两个阶段，我们可以通过一个线性或非线性的函数 $P:\Bbb{R}^d \rightarrow \Bbb{R}^m$完成高维到低维的投影，再通过一个二值化函数$sgn(\cdot)$ 将低维向量二值化：</p><script type="math/tex; mode=display">Z = H(X) = sgn(P(X))\tag{1}</script><p>其中 $sgn:\Bbb{R}^m \rightarrow {-1, 1}^m$，其根据每个元素是否大于0进行二值划分。</p><p>由于论文指出上述2阶段方法会造成严重的<code>neighborhood error</code>，因此他们定义了该损失函数的表达式：</p><script type="math/tex; mode=display">\mathcal{L} = \Vert S - V \Vert_F^2\tag{2}</script><p>其中 $S\in \Bbb{R}^{n\times n}$ 是原始数据的邻域关系矩阵，定义为</p><script type="math/tex; mode=display">S_{ij} = \begin{cases}\begin{aligned}1 \quad &\text{if}\ d(x_i, x_j) < \epsilon\\-1\quad &\text{otherwise}\end{aligned}\end{cases}</script><p>$d(\cdot)$ 衡量了原始数据空间的欧氏距离。而 $V$ 是二值化空间的邻域关系矩阵，定义为</p><script type="math/tex; mode=display">V_{ij} = \left( b_i\cdot b_j \right) / m \tag{3}</script><p>其中，b<sub>i</sub> 是数据 x<sub>i</sub> 的二值化向量。可以看到V<sub>ij</sub> 是值域为-1到1的离散变量。那么通过式(3)，损失函数可以改写为：</p><script type="math/tex; mode=display">\mathcal{L} = \Vert S - \frac{1}{m}B^TB \Vert_F^2\tag{4}</script><p>其中 $B\in {-1, 1}^{m\times n}$是n个数据的二值化向量矩阵。</p><p>结合公式(1)和公式(4)，很容易得到在传统方法中，$B = H(x)$。现在，作者希望通过加入第3个步骤，即Post Tuned Hashing (PTH)，来实现二值化向量的重建。具体而言，论文将定义一个新的变换 $R:{-1,1}^m\rightarrow {-1,1}^m$，使得重建后的二值向量能够最小化损失：</p><script type="math/tex; mode=display">PTH(X) = R(H(X))\tag{5}</script><p>对于这个新的变换，论文设计了一个二值(-1,1)的矩阵<code>post-tuning matrix</code>，表示为 $U\in \Bbb{R}^{m\times n}$，通过学习这个矩阵来使损失最小化。这样，公式(4)可以进一步写为</p><script type="math/tex; mode=display">\min \mathcal{Q}(U) = \Vert S - \gamma (U\circ Z)^T(U\circ Z) \Vert_F^2\tag{6}</script><p>为了清楚地推导论文的优化算法，这里对于步骤做了细化的分析：</p><script type="math/tex; mode=display">\begin{aligned}U\circ Z &= \begin{bmatrix}{u_{11}}&{u_{12}}&{\cdots}&{u_{1n}}\\{u_{21}}&{u_{22}}&{\cdots}&{u_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{u_{m1}}&{u_{m2}}&{\cdots}&{u_{mn}}\\\end{bmatrix}\circ\begin{bmatrix}{z_{11}}&{z_{12}}&{\cdots}&{z_{1n}}\\{z_{21}}&{z_{22}}&{\cdots}&{z_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{z_{m1}}&{z_{m2}}&{\cdots}&{z_{mn}}\\\end{bmatrix}\\&= \begin{bmatrix}{u_{11}z_{11}}&{u_{12}z_{12}}&{\cdots}&{u_{1n}z_{1n}}\\{u_{21}z_{21}}&{u_{22}z_{22}}&{\cdots}&{u_{2n}z_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{u_{m1}z_{m1}}&{u_{m2}z_{m2}}&{\cdots}&{u_{mn}z_{mn}}\\\end{bmatrix}\end{aligned}\tag{7}</script><script type="math/tex; mode=display">\begin{aligned}&S - \gamma(U\circ Z)^T(U\circ Z)\\&= S - \gamma\begin{bmatrix}{u_{11}z_{11}}&{u_{21}z_{21}}&{\cdots}&{u_{m1}z_{m1}}\\{u_{12}z_{12}}&{u_{22}z_{22}}&{\cdots}&{u_{m2}z_{m2}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{u_{1n}z_{1n}}&{u_{2n}z_{2n}}&{\cdots}&{u_{mn}z_{mn}}\\\end{bmatrix}\begin{bmatrix}{u_{11}z_{11}}&{u_{12}z_{12}}&{\cdots}&{u_{1n}z_{1n}}\\{u_{21}z_{21}}&{u_{22}z_{22}}&{\cdots}&{u_{2n}z_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{u_{m1}z_{m1}}&{u_{m2}z_{m2}}&{\cdots}&{u_{mn}z_{mn}}\\\end{bmatrix}\\&= S - \gamma\begin{bmatrix}{\sum_{k=1}^mu_{k1}z_{k1}u_{k1}z_{k1}}&{\sum_{k=1}^mu_{k1}z_{k1}u_{k2}z_{k2}}&{\cdots}&{\sum_{k=1}^mu_{k1}z_{k1}u_{kn}z_{kn}}\\{\sum_{k=1}^mu_{k2}z_{k2}u_{k1}z_{k1}}&{\sum_{k=1}^mu_{k2}z_{k2}u_{k2}z_{k2}}&{\cdots}&{\sum_{k=1}^mu_{k2}z_{k2}u_{kn}z_{kn}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{\sum_{k=1}^mu_{kn}z_{kn}u_{k1}z_{k1}}&{\sum_{k=1}^mu_{kn}z_{kn}u_{k2}z_{k2}}&{\cdots}&{\sum_{k=1}^mu_{kn}z_{kn}u_{kn}z_{kn}}\\\end{bmatrix}\end{aligned}\tag{8}</script><script type="math/tex; mode=display">\Rightarrow \left[(U\circ Z)^T(U\circ Z) \right]_{ij} = \sum_{k=1}^mu_{ki}z_{ki}u_{kj}z_{kj}\tag{9}</script><p>根据上面的详细推导，我们的目标函数可以写为</p><script type="math/tex; mode=display">\begin{aligned}&\min \mathcal{Q}(U) = \Vert S - \gamma (U\circ Z)^T(U\circ Z) \Vert_F^2\\\Leftrightarrow &\min_{U} \sum_{ij}\left[    S_{ij} - \gamma\sum_{k=1}^mu_{ki}z_{ki}u_{kj}z_{kj}\right]^2\end{aligned}\tag{10}</script><p>我们也可以统计U的第p行的部分：</p><script type="math/tex; mode=display">\begin{aligned}&\sum_{ij}\left[    S_{ij} - \gamma\sum_{k=1}^mu_{ki}z_{ki}u_{kj}z_{kj}\right]^2\\=&\sum_{ij}\left[    S_{ij} - \gamma\left(        u_{pi}z_{pi}u_{pj}z_{pj} + \sum_{k=1,k\neq p}^mu_{ki}z_{ki}u_{kj}z_{kj}    \right)\right]^2\\\mathop{=}\limits_{x=u_{pi}z_{pi}u_{pj}z_{pj}}^{\mathrm{set}\ A=\sum_{k=1,k\neq p}^mu_{ki}z_{ki}u_{kj}z_{kj}}&\sum_{ij}\left[    S_{ij} - \gamma\left(        x + A    \right)\right]^2\\=& \sum_{ij}S_{ij}^2 - 2\gamma S_{ij}(x+A) + \gamma^2(x+A)^2\\\mathop{=}\limits^{\mathrm{delete}\ \mathrm{without}\ x}&\sum_{ij} -2\gamma S_{ij}x + \gamma^2x^2 + 2\gamma^2A\cdot x\end{aligned}\tag{11}</script><p><strong>所以一次项，即文章中的linear terms</strong>，可以表示为</p><script type="math/tex; mode=display">\begin{aligned}&\sum_{ij}-2\gamma S_{ij}x+2\gamma^2A\cdot x\\=& -2 \sum_{ij}u_{pi}z_{pi}u_{pj}z_{pj}\left[    \gamma S_{ij} - \gamma^2\left(        \sum_{k=1,k\neq p}^mu_{ki}z_{ki}u_{kj}z_{kj}    \right)\right]\end{aligned}\tag{12}</script><p>论文设 $Z$ 的第p行的数据为 $z$<sub>p</sub>，其转置(即column向量)为 $\overline{z}$<sub>p</sub>，设  <script type="math/tex">Q=\overline{z}_p \overline{z}_p^T</script> ， 设 <script type="math/tex">(U \circ Z)_{\backslash p}</script> 是将公式(1)的第p行删除的新矩阵，设 <script type="math/tex">O = \left[(U \circ Z)_{\backslash p} \right]^T\left[(U \circ Z)_{\backslash p}\right]</script> ，我们可以轻易地将公式(5)改写为</p><script type="math/tex; mode=display">-2\gamma\sum_{ij}u_{pi} Q_{ij}\left(S_{ij} - \gamma O_{ij} \right) u_{pj}\tag{13}</script><p>值得指出，<script type="math/tex">Q_{ij} = z_{pi}z_{pj} = Q_{ji}, \quad O_{ij} = \sum_{k=1,k\neq p}^mu_{ki}z_{ki}u_{kj}z_{kj} = O_{ji},\quad S_{ij} = S_{ji}</script> (详见论文公式(4))，那么如果我们设 $ C = Q\circ (S - \gamma O) $ ，那么 $C$ 就是对称矩阵。因此公式6可以进一步化简：</p><script type="math/tex; mode=display">-2\gamma\sum_{ij}u_{pi}c_{ij}u_{pj}\tag{14}</script><p>其中，每一个涉及$u_{pq}$的量均可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}-2\gamma\sum_{i=q,j}u_{pq}c_{qj}u_{pj} - 2\gamma\sum_{i,j=q}u_{pi}c_{iq}u_{pq} &= - 4\gamma\sum_{k}u_{pk}c_{qk}u_{pq}\\&= -4\gamma\sum_{k=1,k\neq q}^nu_{pk}c_{qk}u_{pq} + 二次项\end{aligned}\tag{15}</script><p>对于每一个元素 <script type="math/tex">u_{pq}</script> ，最小化损失函数公式(6)即最小化公式(15)。我们可以将 <script type="math/tex">u_{pq}</script> 前面的系数当成是权重，那么为了最小化损失，当权重为负时，我们希望 <script type="math/tex">u_{pq}=1</script> ，权重为正时反之。</p><h2 id="更新与优化策略"><a href="#更新与优化策略" class="headerlink" title="更新与优化策略"></a>更新与优化策略</h2><p>论文提供了一种更新策略：</p><blockquote><p>update the current entry if its coefficient is larger than a fixed threshold $\eta$.</p></blockquote><p>这个策略可以按照矩阵$U$内的元素顺序进行依次更新，减小了算法复杂度。并且，在一行元素更新时，矩阵 $C$并不需要更新，这是因为$C$受到 $U$所有元素的影响，对一行数据的改变不敏感。</p><p>论文提供了一种剪枝策略：</p><blockquote><ol><li>Only tune the elements whose projection results (value before binarization) are close to 0, or smaller than a threshold $\delta$.</li></ol></blockquote><p>即，我们只调整那些投影后值<strong>与0接近的元素</strong>(<u>因为0是二值化的一个阈值</u>)，因为这些元素更有可能被二值化为不正确的值。这有效减少了post-tuning的复杂度，并且缓解了过拟合。</p><p>结合这两种策略，最终的优化算法如下图所示：</p><p><figure class="half"><img src="/2023/06/28/PTH/2.png" width="500"></figure></p><center>图2. Post-tuning Algorithm</center><h2 id="Out-of-Sample-Post-Tuning"><a href="#Out-of-Sample-Post-Tuning" class="headerlink" title="Out-of-Sample Post-Tuning"></a>Out-of-Sample Post-Tuning</h2><p>由于是无监督学习方法，故方法必须兼容新的数据 $q \notin X$。与K近邻的思想类似，新来的数据 $q$ 理应与原来的数据 $X$产生联系。因此，我们称原来的数据为<code>skeleton points</code>。这样，完整的post-tuning应该包含以下2个任务：</p><ol><li>post-tuning skeleton points $X$；</li><li>post-tuning out-of-sample $q$.</li></ol><p>与公式(6)类似的，我们建立如下损失：</p><script type="math/tex; mode=display">\begin{aligned}\min \mathcal{R}(u^q) &= \Vert S^q - \frac{1}{m}(u^q\circ z^q)^T B \Vert_F^2\\\text{s.t.} &\quad u^q\in \{-1,1 \}^m\end{aligned}\tag{16}</script><p>其中:</p><ul><li>$S^q\in \Bbb{R}^{1\times n}$：$q$ 和 $X$的邻域关系矩阵；</li><li>$u^q\in \Bbb{R}^{1\times m}$：即 $q$ 的<code>post-tuning matrix</code>；</li><li>$z^q\in \Bbb{R}^{1\times m}$：即 $q$ 经过二值化后的向量；</li><li>$B = (U\circ Z) \in \Bbb{R}^{m\times n}$：$X$经过post-tuning的新二值化向量。</li></ul><p>总之上式衡量了hashing前后 $q$ 和 $X$ 的邻域关系。论文也提到，由于post-tuning完全独立于前面2个步骤，因此这里的<code>skeleton points</code>( $X$ )在实践中并不是全部训练集，而是选择少部分数据点以提高效率。数据点数量的选择和最终方法效果的关系可从下图得出</p><p><figure class="half"><img src="/2023/06/28/PTH/3.png" width="900"></figure></p><center>图2. Post-tuning Algorithm</center><p>适量的数据点即可达到性能上限。太多的数据点不仅不能提高性能，还会消耗过多空间/时间资源。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</title>
      <link href="/2023/06/09/NEGCUT/"/>
      <url>/2023/06/09/NEGCUT/</url>
      
        <content type="html"><![CDATA[<h1 id="论文阅读-Instance-wise-Hard-Negative-Example-Generation-for-Contrastive-Learning-in-Unpaired-Image-to-Image-Translation"><a href="#论文阅读-Instance-wise-Hard-Negative-Example-Generation-for-Contrastive-Learning-in-Unpaired-Image-to-Image-Translation" class="headerlink" title="论文阅读 - Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation"></a>论文阅读 - Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</h1><p>论文有一个重要的结论：在基于对比学习的图像翻译任务中，负样本(negative examples)的质量和难度是非常重要的。和之前的方法<code>CUT</code>相比，本文提出的方法能够产生足够challenge的负样本，从而使得对比学习可以捕捉那些具有细粒度的可区分特征。</p><p>我们可以从图1中看到，与本文提出的方法相比，<code>CUT</code>方法产生的负例与query相似度不高，这也限制了<code>CUT</code>方法的效果：</p><p><figure class="half"><img src="/2023/06/09/NEGCUT/1.png" width="400"></figure></p><center>图1. The distribution of similarity in CUT and NEGCUT</center><p>由于论文是对<code>CUT</code>方法的一个改进，故我们首先引入<code>CUT</code>方法，来探索其使用对比学习的动机；再详细谈谈论文的方法<code>NEGCUT</code>是如何改进负样本的生成方法的；最后，我们沿着<code>负样本的质量和难度</code>这一线索，尝试阐述自己的一些想法。</p><h2 id="1-论文介绍"><a href="#1-论文介绍" class="headerlink" title="1. 论文介绍"></a>1. 论文介绍</h2><h3 id="1-1-CUT-Contrastive-Learning-for-Unpaired-Image-to-Image-Translation"><a href="#1-1-CUT-Contrastive-Learning-for-Unpaired-Image-to-Image-Translation" class="headerlink" title="1.1 CUT - Contrastive Learning for Unpaired Image-to-Image Translation"></a>1.1 CUT - Contrastive Learning for Unpaired Image-to-Image Translation</h3><p>图像翻译任务的核心工作就是在跨域生成图像时<u><strong>分离content和appearance</strong></u>，保留原图中的内容，将外观改变为目标domain的样式。举例而言，图像中对应<code>zebra forehead</code>的patch在经过generator生成后，应当是<code>horse forehead</code>而不是其他的部位。</p><p><code>cycle-consistency</code>为了在非配对图像翻译任务中<strong>保证翻译(生成)前后图像信息的一致性</strong>，假设图像翻译任务的两个domain满足双射关系，这个严格的限制使其在相当一部分任务上(尤其是两个domain信息量差距较大时)效果不佳。同时，其需要一对generator来生成不同域的图像，对于只需要one-sided translation的任务而言无疑是增加了工作量。</p><p>而<code>CUT</code>则是基于互信息最大化思想，利用对比学习思想去捕捉对应的输入输出图像之间的共性部分，从而鼓励保留那些content信息。同时，作者注意到图像在每个patch上也是满足content不变的性质。于是作者采用patchwise的<code>InfoNCE Loss</code>来完成这个任务。</p><p>结构上，论文需要一个generator进行domain adaptation，并且需要一个encoder捕获content信息并进行对比学习，框架图2所示：</p><p><figure class="half"><img src="/2023/06/09/NEGCUT/2.png" width="800"></figure></p><center>图1. CUT模型结构</center><p>于是他自然需要以下2个损失函数部分：</p><ul><li>Adversarial loss：鼓励生成的图像和目标domain有一致的“风格”；</li><li>Mutual information maximization：鼓励输入和输出的对应位置有更紧密的联系，或是互信息。</li></ul><h4 id="1-1-1-对比损失"><a href="#1-1-1-对比损失" class="headerlink" title="1.1.1 对比损失"></a>1.1.1 对比损失</h4><p>由于对比损失是GAN-based模型所必须的损失，我们不再详细探讨它，直接给出公式如下：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{GAN}}(G,D,X,Y) = \Bbb{E}_{\pmb{y}\sim Y}\log D(\pmb{y}) + \Bbb{E}_{\pmb{x}\sim X}\log (1 - D(G(\pmb{x})))\tag{1}</script><h4 id="1-1-2-互信息最大化"><a href="#1-1-2-互信息最大化" class="headerlink" title="1.1.2 互信息最大化"></a>1.1.2 互信息最大化</h4><p>如果要使用对比学习来实现互信息的最大化，应当使<code>query</code>和<code>positive</code>两个信号关联起来，而数据集中的其他信息都被定义为<code>negative</code>。如果我们令 $\text{query}=v,\ \text{positive}=v^+,\ \text{negative}=v^-$，我们当然希望 $v$ 和 $v^+$ 的距离/相似度最高，和negative的相似度越低，故我们可以定义一个<code>InfoNCE Loss</code>：</p><script type="math/tex; mode=display">\mathscr{l}(v,v^+,v^-) = -\log\left[\frac{\exp(v\cdot v^+/\tau)}{\exp(v\cdot v^+/\tau) + \sum_{n=1}^N\exp(v\cdot v^-_n/\tau)} \right]\tag{2}</script><p>上式实际上是通过交叉熵损失进行计算的。</p><p>现在，我们的重点转移到如何选择对比学习的对象(即image or patch)、如何获取负样本。</p><p>论文提到，在图像翻译任务中，不仅生成前后的图像共享content，对应的patch也是共享content的，他们采取了一个patch-based、multilayer的对比学习策略。</p><p>对于每一层，模型设计了一个encoder(来自generator)+MLP的编码结构，如下图所示：</p><p><figure class="half"><img src="/2023/06/09/NEGCUT/3.png" width="800"></figure></p><center>图3. Patchwise Contrastive Loss</center><p>一个patch在第$l$层上的编码向量表示为 </p><script type="math/tex; mode=display">\{z_l\}_{L} = \{H_l(G_{enc}^l(x))\}_{L}</script><p>设这一层的特征有 $S_l$ 个position，则其第 $s$ 个position的向量应该为 $z_l^s$ 。同理我们也给目标domain类似的定义(将 $z$ 转换为 $\hat{z}$ )，那么式(2)可以更新为：</p><script type="math/tex; mode=display">\mathscr{L}_{\text{PatchNCE}}(G,H,X) = \Bbb{E}_{x\sim X}\sum_{l=1}^L\sum_{s=1}^{S_l} \mathscr{l}\left(\hat z_l^s, z_l^s, z_l^{S \backslash s}\right)\tag{3}</script><p>其中我们将目标domain的patch设为<code>query</code>，将input的对应patch设为<code>positive</code>，图片中其他位置的patch设为<code>negative</code>。我们只分享within图像的负样本，不在赘述external的情况。</p><p>最后，为了避免生成图像产生额外不必要的变化并且让generator更focus on那些content的信息，论文引入了一个<code>identity loss</code>(与CycleGAN中提到的类似) $\mathscr{L}_{\text{PatchNCE}}(G,H,Y)$，总的训练损失函数定义为：</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{\text{GAN}}(G,D,X,Y) + \lambda_X \mathcal{L}_{\text{PatchNCE}}(G,H,X) + \lambda_Y \mathcal{L}_{\text{PatchNCE}}(G,H,Y)\tag{4}</script><h3 id="1-2-NEGCUT"><a href="#1-2-NEGCUT" class="headerlink" title="1.2 NEGCUT"></a>1.2 NEGCUT</h3><p>论文的模型结构和<code>CUT</code>方法中的类似，都是通过一个generator生成目标domain的图像，并通过generator的encoder去获取特征向量，进行多层的对比学习。论文的模型结构如下图所示：<br><img src="/2023/06/09/NEGCUT/4.png" alt="Alt text"><br><a id="fig4"><center>图4. NEGCUT framework</center> </a></p><p>下面，我们着重理解论文制造<code>Hard Negative Examples</code>的方法。对于从encoder中的某一层 $l$ 获得的图像特征，论文使用一个由2-layer MLP组成的<code>Representation Network</code> $H^i(\cdot)$来进一步提取高维表示。</p><p>与<code>CUT</code>类似，论文对空间维数中的S个位置进行随机抽样，并以归一化向量作为<code>query</code>和<code>positive</code>进行对比学习，公式如下：</p><script type="math/tex; mode=display">q = \frac{H_s^i(F_i^{\pmb{\text{Y}}})}{\Vert H_s^i(F_i^{\pmb{\text{Y}}}) \Vert_2},\quadk^+ = \frac{H_s^i(F_i^{\pmb{\text{X}}})}{\Vert H_s^i(F_i^{\pmb{\text{X}}}) \Vert_2} \tag{5}</script><p>其中 $F_i$ 是encoder第i层获取的特征，$Y$ 是目标域，$X$ 是源域(source domain)。下标 $s$ 表示采样到的样本位置。</p><p>现在，我们需要第i层的<code>negative</code>样本特征。由于图像内的patch不够challenge，论文采用生成的办法去创造负样本。于是论文为第 $i$ 层设计了一个独立的<code>negative generator</code> $N^i$，它将接受<code>Representation Network</code>中的spatially-average特征 $\overline{H^i(F_i^{\pmb{\text{X}}})}$，和一个提供多样性的噪声 $z_n$，输出一个生成的<a id="tag6">负样本</a>：</p><script type="math/tex; mode=display">k_{\text{adv},n}^- = \frac{N^i(\overline{H^i(F_i^{\pmb{\text{X}}})};z_n)}{\Vert  N^i(\overline{H^i(F_i^{\pmb{\text{X}}})};z_n) \Vert_2}\tag{6}</script><p>其中我们可以为每一个正样本采样多个 $z_n\sim N(0,1)$ 从而生成多个负样本。</p><p>为了使<code>negative generator</code>能够生成足够有挑战的负样本，论文选择将encoder(包含representation network)和negative generator进行对抗学习，损失如下所示：</p><script type="math/tex; mode=display">\min_{\theta_{\mathcal{H}},\theta_{\mathcal{G}}}\max_{\theta_{\mathcal{N}}} \mathcal{l}(\text{q},\text{k}^+,\text{k}_{\text{adv}}^-) = -\log \left[     \frac{\exp(\text{q}\cdot \text{k}^+/\tau)}{\exp(\text{q}\cdot \text{k}^+/\tau) + \sum_{n=1}^N \exp(\text{q}\cdot \text{k}_{\text{adv},n}^-/\tau)} \right]\tag{7}</script><p>其中，<code>negative generator</code>通过产生challenge的负样本来影响式(7)的分母使其最大化。而encoder和<code>Representation Network</code>则尽可能让query和正样本接近而使损失最小化，从而达到均衡。</p><p>由于论文采用了多层的对比学习，所以最终的对比学习损失如下：</p><script type="math/tex; mode=display">\mathcal{L}_{AdCount} = \Bbb{E}_{x\sim X}\sum_{l=1}^L\sum_{s=1}^{S_l} \mathcal{l}(\text{q}_{l,s},\text{k}^+_{l,a},\text{k}^-_{\text{adv},l,s})\tag{8}</script><p>为了避免模式崩溃mode collapse，论文引入一个<code>diversity loss</code>，迫使生成的负样本具有多样性：</p><script type="math/tex; mode=display">\mathcal{L}_{div} = -\Vert N^i(\overline{H^i(\pmb{\text{X}}_i)};z_1) - N^i(\overline{H^i(\pmb{\text{X}}_i)};z_2) \Vert_1 \tag{9}</script><p>当然，由于论文也是GAN-based，需要一个对抗损失来保证生成器生成的图片足够真实，符合目标域的特性：</p><script type="math/tex; mode=display">\begin{aligned}L_{gan}^D &= \Bbb{E}_{x_r}\left[(1-\pmb{\text{D}}(x_r))^2 \right] + \Bbb{E}_{x_f}\left[\pmb{\text{D}}(x_f)^2 \right],\\L_{gan}^G &= \Bbb{E}_{x_f}\left[(1-\pmb{\text{D}}(x_f))^2 \right]\end{aligned}\tag{10}</script><p>论文还将损失准确分配给了不同的模型，如：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\mathcal{H}} &= \mathcal{L}_{AdCount}, \\\mathcal{L}_{\mathcal{G}} &= \mathcal{L}_{AdCount} + \lambda_1 L_{gan}^G, \\\mathcal{L}_{\mathcal{G}} &= -\mathcal{L}_{AdCount} + \lambda_2 L_{div}, \\\end{aligned}\tag{11}</script><p>仔细看图<a href="#fig4">4</a>，其实可以看到损失的反向传播过程：例如<code>contrastive loss</code>(应该是公式(7)中要最小化的部分)将从representation network开始传播，而<code>adversarial contrastive loss</code>则仅用以训练负例生成器等。</p><h3 id="1-3-Idea-Mining"><a href="#1-3-Idea-Mining" class="headerlink" title="1.3 Idea Mining"></a>1.3 Idea Mining</h3><p>鉴于<code>NEGCUT</code>做了一个很好的改进，使得我认为对比学习在unpaired图像翻译任务中还能发挥更强烈的作用。下面，我来阐述一下我通过阅读论文做的一些思考：</p><ul><li><p><strong>为什么公式<a href="#tag6">6</a>会选择将空间的均值作为负生成器的输入</strong>：初看文章，我就有这个疑惑。我完全可以选择patch附近的空间做均值，这应该会比使用所有position的均值更具challenge。但与<code>CUT</code>对比后，我发现这个想法应该源于<code>CUT</code>模型将图像内patch作为负样例的思想：做了均值后的向量总会包含整张图片的信息，添加噪声 $z$ 后理论上可以与图片中的任意一个patch近似。如图5所示，可以发现<code>negative generator</code>总是可以生成与当前patch较为相似的负例向量。所以，该方法其实可以获取很多与当前patch接近，但不局限于图像内的负样例。最后，使用附近patch的均值可能会产生太过困难的负样本，阻碍了正样本和query的学习。<br><figure class="half"><img src="/2023/06/09/NEGCUT/5.png" width="800"></figure></p><center>图5. Negative examples visualization</center></li><li><p><strong>如何保证生成的负样本的分布是真实的</strong>：从消融实验里，可以看到负例确实能够检索到与正样本相近的patch，但没有对其分布做进一步探索，例如：它真的属于source domain吗？这似乎并没有确切的证明他们。我想，设计一个判别器来判别生成的负例是否服从真实负样本的分布可能是一个解决方法。</p></li></ul><p>另外，根据这个问题，我依然想通过<code>CUT</code>的方法——即图像内部的patch进行解决。<code>CUT</code>方法中将所有 $S-s$ positions的向量引入<code>PatchNCE Loss</code>中，给予了它们<strong>相同的权重</strong>，即：</p><script type="math/tex; mode=display">\text{Total}_{neg} = \sum_{n=1}^N \exp(v\cdot v_n^- / \tau)\tag{12}</script><p>那如果我们可以分配给hard negative examples一个更高的权重，或许又会有更好的效果。</p><ul><li><strong>对比学习如何保证translate前后content的一致性</strong>：准确来说，对比学习通过拉近<code>query</code>图像patch和<code>positive</code>图像patch从而保留其内容相近。但是patch级的局部对比学习或许会影响图像级的content信息，例如source domain两个接近的patch反而在目标域不再接近。因此我认为引入patch之间的距离或相似度来监督图像translation的质量也是有一定意义的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 夏令营经历 </category>
          
          <category> Semantic segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> papers reproduced </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</title>
      <link href="/2023/06/09/unimatch/"/>
      <url>/2023/06/09/unimatch/</url>
      
        <content type="html"><![CDATA[<h1 id="论文复现-Revisiting-Weak-to-Strong-Consistency-in-Semi-Supervised-Semantic-Segmentation"><a href="#论文复现-Revisiting-Weak-to-Strong-Consistency-in-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="论文复现 - Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation"></a>论文复现 - Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</h1><blockquote><p>前记：这篇文章基于20年半监督学习的SOTA：FixMatch。他们发现FixMatch可以在半监督语义分割任务上媲美最近的SOTA，故在此基础上进行了多角度优化，思路很有借鉴意义。</p><p>下面，我们将通过论文介绍和实验复现两部分详细展示论文复现工作。</p></blockquote><h2 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h2><p>因为论文是基于<code>FixMatch</code>做的一些工作，所以我们先回归一下FixMatch</p><h3 id="1-FixMatch"><a href="#1-FixMatch" class="headerlink" title="1. FixMatch"></a>1. FixMatch</h3><p>根据论文的意思，由于半监督学习SSL的先进方法都引入了太多复杂的结构，<code>FixMatch</code>希望可以构建一个simple却又精确的模型。如图1所示，对于一张未标记的图片，模型通过预测<code>weakly-augmented</code>后的图片得到伪标签(注意，只有<strong>置信度高于阈值的才被使用</strong>，否则忽略)，然后最小化<code>strongly-augmented</code>后图片的<u>预测分布和伪标签的“距离”</u>。这里距离的衡量是使用<code>H(p,q): Cross Entropy</code>.<br><img src="/2023/06/09/unimatch/2.png" alt></p><center>图1. FixMatch框架</center><p>那么，为什么可以这么做？为什么<code>weak</code>和<code>strong</code>的预测分布是相近的？于是我们引出<code>FixMatch</code>的2个核心思想：</p><h4 id="Consistency-regularization"><a href="#Consistency-regularization" class="headerlink" title="Consistency regularization"></a>Consistency regularization</h4><p>一致性正则化，它有一个很强的假设，就是<u>同一图像经过不同扰动后输入模型，其输出的预测应该是接近或者类似的</u>。故其loss fuction表示为：</p><script type="math/tex; mode=display">\sum_{b=1}^{\mu B}\Vert p_m(y|\alpha(u_b)) - p_m(y|\alpha(u_b)) \Vert_2^2\tag{1}</script><p>其中$\mu B$代表未标记的数据量，$p_m$是模型，$\alpha$是一个弱扰动(简单的数据增广)。因为数据增广是具有随机性的，所以上式两项上并不一定相同。</p><h4 id="Pseudo-labeling"><a href="#Pseudo-labeling" class="headerlink" title="Pseudo-labeling"></a>Pseudo-labeling</h4><p>伪标签的思想，是希望利用置信度高的数据进行自我训练，从而提高模型性能，具体而言，体现为下面的损失函数：</p><script type="math/tex; mode=display">\frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \tau\right)\text{H}(\hat{q}_b, q_b)\tag{2}</script><p>可以看到，上式设置了一个阈值$\tau$用于控制置信度，只有置信度高于阈值的才进行loss的计算。式子中 $ \hat{q}_b=\arg \max q_b $ ，是指预测分布中分数最高的那个类别，也称为硬标签。</p><p><code>FixMatch</code>结合了这两个思想：即使用弱扰动的图像通过模型生成的伪标签，来监督强扰动的预测结果。具体来说，模型先预测弱扰动图像的分布，并得到硬标签。如果该标签的置信度高于阈值，则将强扰动图像的预测输出和该标签做一个交叉熵损失：</p><script type="math/tex; mode=display">\begin{aligned}q_b &= p_m(y|\alpha (u_b))\\\hat{q}_b &= \arg \max q_b\\\mathcal{L}_u &= \frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \tau\right)\text{H}(\hat{q}_b, p_m(y|\mathcal{A}(u_b)))\end{aligned}\tag{3}</script><p>其中 $\mathcal{A}(\cdot)$ 就是强扰动(强数据增广)。</p><p>最后，数据增广也是<code>FixMatch</code>关键的一环，在原论文中，作者对增广做了如下设置：</p><ul><li>弱扰动：标准的<code>flip-and-shift</code>增广，即水平翻转或垂直翻转；</li><li>强扰动：作者认为基于强化学习的AutoAugment需要很多带标签的数据，并不适合SSL任务。所以作者采用了以下两个增广方式：<ol><li><code>RandAugment</code>：只需要搜索增强操作的数量<code>N</code>和全局的增强幅度<code>M</code>(分为10个等级，10为最强)，代码如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Identity是恒等变换，不做任何增强</span></span><br><span class="line">transforms = [<span class="string">&#x27;Identity&#x27;</span>, <span class="string">&#x27;AutoContrast&#x27;</span>, <span class="string">&#x27;Equalize&#x27;</span>, <span class="string">&#x27;Rotate&#x27;</span>, <span class="string">&#x27;Solarize&#x27;</span>, </span><br><span class="line">              <span class="string">&#x27;Color&#x27;</span>, <span class="string">&#x27;Posterize&#x27;</span>, <span class="string">&#x27;Contrast&#x27;</span>, <span class="string">&#x27;Brightness&#x27;</span>, <span class="string">&#x27;Sharpness&#x27;</span>, </span><br><span class="line">              <span class="string">&#x27;ShearX&#x27;</span>, <span class="string">&#x27;ShearY&#x27;</span>, <span class="string">&#x27;TranslateX&#x27;</span>, <span class="string">&#x27;TranslateY&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">randaugment</span>(<span class="params">N, M</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;Generate a set of distortions.</span></span><br><span class="line"><span class="string">Args:</span></span><br><span class="line"><span class="string">N: Number of augmentation transformations to apply sequentially.</span></span><br><span class="line"><span class="string">M: Magnitude for all the transformations.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">sampled_ops = np.random.choice(transforms, N)</span><br><span class="line"><span class="keyword">return</span> [(op, M) <span class="keyword">for</span> op <span class="keyword">in</span> sampled_ops]</span><br></pre></td></tr></table></figure></li><li><code>CTAugment</code>：一种在线学习的方法。该方法先定义一组transforms(如旋转、裁剪等)以及每种变换可能的幅度(旋转的角度等)，然后维护一个<code>变换-幅度-概率表</code>，记录每种变换和幅度的概率，初始化为均匀分布。对于每一张unlabelled图像，从表中随机采样弱增强(变换+幅度)和强增强(变换+幅度)，然后利用弱增强生成伪标签。如果置信度高于阈值则计算伪标签和强增强的预测分布之间的交叉熵损失。最后，根据损失大小更新概率表，损失小，则提高相应概率。</li></ol></li></ul><h3 id="2-UniMatch"><a href="#2-UniMatch" class="headerlink" title="2. UniMatch"></a>2. UniMatch</h3><p><code>UniMatch</code>建立在FixMatch引入图像级强扰动的思想上。直观地说，它的成功在于该模型更有可能对 $x^w$ 产生高置信度和质量的预测，而 $x^s$ 对我们的模型学习更有效，因为强扰动引入了额外的信息，并减轻了确认偏差。具体来说，作者认为<code>FixMatch</code>的强扰动是性能优越的关键(或者说weak-to-strong框架非常优越)。于是作者认为可以进一步发挥强扰动的潜力。它们做了2个方向的改进：</p><ul><li>同时探索image和feature两个层面的扰动 - 探索更广泛的扰动空间</li><li>Dual-stream 扰动，充分利用预定义的image扰动空间。<br><figure class="half"><img src="/2023/06/09/unimatch/3.png" width="400"></figure><center>图2. FixMatch和UniMatch对比</center></li></ul><h4 id="2-1-UniPerb-Perturbations-for-Images-and-Features"><a href="#2-1-UniPerb-Perturbations-for-Images-and-Features" class="headerlink" title="2.1 UniPerb - Perturbations for Images and Features"></a>2.1 UniPerb - Perturbations for Images and Features</h4><p>即同时对image和feature扰动的方法。作者将模型 $f$ 拆成了<code>encoder</code> $g$ 和<code>decoder</code> $h$，$x^w$ 在通过编码器后得到特征，再施加一个特征扰动 $\mathcal{P}$ 得到特征扰动后的新特征 $FP$。这样经过解码器，我们同时可获得(图像)弱扰动的预测分布、特征强扰动的预测分布、(图像)强扰动的预测分布，如下图所示：</p><p><figure class="half"><img src="/2023/06/09/unimatch/4.png" width="180"></figure></p><center>图3. UniPerb</center><p>使用公式可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}p^w &= \hat{F}\left(\mathcal{A}^w(x^u) \right);\quad p^s = F\left(\mathcal{A}^s\left(\mathcal{A}^w(x^u)\right) \right)\\e^w &= g(x^w)\\p^{fp} &= h\left(\mathcal{P}\left(e^w \right)\right)\\\mathcal{L}_u &= \frac{1}{B_u}\sum \Bbb{I}\left(\max(p^w)\geq \tau \right)\left(\text{H}(p^w, p^s), \text{H}(p^w, p^{fp}) \right)\end{aligned}\tag{4}</script><p>其中， $x^w$ 是$x^u$经过弱扰动的图像； $\hat F$ 是弱扰动图像的教师模型， $F$ 是强扰动图像使用的学生模型，这里他们俩完全相同。</p><h4 id="2-2-DusPerb-Dual-Stream-Perturbations"><a href="#2-2-DusPerb-Dual-Stream-Perturbations" class="headerlink" title="2.2 DusPerb - Dual-Stream Perturbations"></a>2.2 DusPerb - Dual-Stream Perturbations</h4><p>作者受到其他工作的影响，认为为无标签图像数据构建多个view作为输入可以更好的利用扰动空间。简单地，他们为一张图像设置2个强扰动视图：$x^{s_1}$和$x^{s_2}$。因为 $\mathcal{A}^s$ 具有随机性，故这两个视图不同。该模式的结构如下图所示：</p><p><figure class="half"><img src="/2023/06/09/unimatch/5.png" width="180"></figure></p><center>图4. DusPerb</center><p>作者将该结构的优越性归功于 <strong>对比学习</strong> (而不是单纯doubled unlabeled batch size)：$x^{s_1}$ 和 $x^{s_2}$ 都应该和 $x^w$ 中预测概率最高的类别接近，等价于 $x^{s_1}$ 和 $x^{s_2}$ 互相接近，这可以使用InfoNCE Loss实现：</p><p><figure class="half"><img src="/2023/06/09/unimatch/6.png" width="600"></figure></p><center>论文原文</center><p>最终模型结合了<code>UniPerb</code>和<code>DusPerb</code>，Loss表示为</p><script type="math/tex; mode=display">\mathcal{L}_u = \frac{1}{B_u} \sum \Bbb{I}\left(\max(p^w)\geq \tau \right)\cdot \left( \lambda \text{H}(p^w, p^{fp}) + \frac{\mu}{2}\left(\text{H}(p^w, p^{s_1}) + \text{H}(p^w, p^{s_2})\right) \right)\tag{5}</script><h3 id="3-实验结果和消融研究"><a href="#3-实验结果和消融研究" class="headerlink" title="3. 实验结果和消融研究"></a>3. 实验结果和消融研究</h3><p>由于是新的SOTA，论文在三个数据集上展现出了强劲表现。这里简单摘录在<code>pascal voc 2012</code>数据集上的一些表现(因为下文复现时<u>仅使用该数据集</u> )。</p><h4 id="labelled-data数量"><a href="#labelled-data数量" class="headerlink" title="labelled data数量"></a>labelled data数量</h4><p><figure class="half"><img src="/2023/06/09/unimatch/7.png" width="500"></figure></p><center>图5. 在不同标注数据使用量下，模型在Pascal上的表现</center><p>可以看到UniMatch不仅准确率高，还比较稳定，在标注数据较少(如，92)的情况下依然还有较高精度。</p><h4 id="labelled-data占比"><a href="#labelled-data占比" class="headerlink" title="labelled data占比"></a>labelled data占比</h4><p><figure class="half"><img src="/2023/06/09/unimatch/8.png" width="500"></figure></p><center>图6. 在不同标注数据占比下，模型在Pascal上的表现</center><p>UniMatch在标注数据占比较少(如，1/16)的情况下依然还有较高精度。</p><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>这一块我们简单列举一下论文的结论：</p><ol><li><strong>The improvement of diverse perturbations is non-trivial</strong>：即多种类型的强扰动(2×image+feature)是比简单设置3个image强扰动有效的；</li><li><strong>The improvement of dual-stream perturbation is non-trivial</strong>：论文证明双流扰动的成功不是因为增加了一个batch内的unlabelled data；</li><li><strong>The necessity of separating image- and feature-level perturbations into independent streams</strong>：即分离不同类型的扰动是有效的；</li><li><strong>More perturbation streams</strong>：论文证明图像级多流扰动提升有限，双流以已经足够了；</li></ol><p>…</p><hr><h2 id="实验结果复现"><a href="#实验结果复现" class="headerlink" title="实验结果复现"></a>实验结果复现</h2><p>下文分析和修改的代码源自论文仓库： <a href="https://github.com/LiheYoung/UniMatch">https://github.com/LiheYoung/UniMatch</a> .</p><h3 id="1-下载代码、模型和数据"><a href="#1-下载代码、模型和数据" class="headerlink" title="1. 下载代码、模型和数据"></a>1. 下载代码、模型和数据</h3><h4 id="1-1-代码下载"><a href="#1-1-代码下载" class="headerlink" title="1.1 代码下载"></a>1.1 代码下载</h4><p>关于代码的<code>Installation</code>，直接按照默认方法：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd UniMatch</span><br><span class="line">conda create -n unimatch python=3.10.4</span><br><span class="line">conda activate unimatch</span><br><span class="line">pip install -r requirements.txt # 别急,请先按照下面的第一条修改文件;</span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure><br>值得强调的是，代码其实包含一些Bug，需要简单处理一下：</p><ol><li>在<code>requirements.txt</code>中，需要将<code>sklearn</code>改成<code>scikit-learn</code>，保证pip install 顺利进行；</li><li>在<code>unimatch.py</code>中，切记将下面这行代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--local_rank&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure> 改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--local-rank&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure> 不然你的local-rank参数不被识别；</li><li>如果你是单机单卡或者单机多卡(例如我)，可以将<code>train.sh</code>配置为 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1,2,3</span><br><span class="line"></span><br><span class="line">python -m torch.distributed.launch \</span><br><span class="line">    --nnodes 1 \</span><br><span class="line">    --nproc_per_node=$1 \</span><br><span class="line">    $method.py \</span><br><span class="line">    --config=$config --labeled-id-path $labeled_id_path --unlabeled-id-path $unlabeled_id_path \</span><br><span class="line">    --save-path $save_path 2&gt;&amp;1 | tee $save_path/$now.log</span><br></pre></td></tr></table></figure> 无需设置port等参数。</li></ol><h4 id="1-2-预训练模型下载"><a href="#1-2-预训练模型下载" class="headerlink" title="1.2 预训练模型下载"></a>1.2 预训练模型下载</h4><p>预训练的模型在原仓库中有3种：<code>ResNet50</code>/<code>ResNet101</code>/<code>xception</code>，在复现时默认使用resnet101，如果时间允许，我们将尝试其他模型的复现。</p><h4 id="1-3-数据集下载"><a href="#1-3-数据集下载" class="headerlink" title="1.3 数据集下载"></a>1.3 数据集下载</h4><p>数据集由于时间和资源有限，仅仅复现关于<code>Pascal VOC 2012</code>数据集的一些结果。</p><ul><li>Pascal: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar">JPEGImages</a> | <a href="https://drive.google.com/file/d/1ikrDlsai5QSf2GiSUR3f8PZUzyTubcuF/view?usp=sharing">SegmentationClass</a></li></ul><p>其他数据集见原仓库。</p><h3 id="2-训练的实现"><a href="#2-训练的实现" class="headerlink" title="2. 训练的实现"></a>2. 训练的实现</h3><p>我们准备好了数据，可以按照下面的<a id="fig1">算法图</a>完成模型训练：<br><img src="/2023/06/09/unimatch/1.png" alt></p><center>图7. 算法示意图</center><h4 id="2-1-数据增广"><a href="#2-1-数据增广" class="headerlink" title="2.1 数据增广"></a>2.1 数据增广</h4><blockquote><p>这里说的数据增广，其实是指<code>strong view</code>的强扰动和<code>weak view</code>的弱扰动。</p><p>相关文件：<strong>./dataset/semi.py</strong></p></blockquote><p>在<a href="#fig1">图7</a>中指代下面这几行代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one weak view and two strong views as input</span></span><br><span class="line">x_w = aug_w(x)</span><br><span class="line">x_s1, x_s2 = aug_s(x_w), aug_s(x_w)</span><br></pre></td></tr></table></figure></p><p>源代码在实现这几行时，首先让每一张图像完成<strong>弱扰动</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img, mask = resize(img, mask, (<span class="number">0.5</span>, <span class="number">2.0</span>))</span><br><span class="line">ignore_value = <span class="number">254</span> <span class="keyword">if</span> self.mode == <span class="string">&#x27;train_u&#x27;</span> <span class="keyword">else</span> <span class="number">255</span></span><br><span class="line">img, mask = crop(img, mask, self.size, ignore_value)</span><br><span class="line">img, mask = hflip(img, mask, p=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>其中<code>img</code>是RGB图像，<code>mask</code>是分割的掩码。现在，这个经过弱扰动的图像<code>img</code>就是$x^w$，同时，我们将其复制两份，得到$x^{s_1}$和$x^{s_2}$。当然$x^{s_1}$和$x^{s_2}$还需要经过强扰动，成为2个<code>strong view</code>，实现<code>Dual-Stream Perturbations</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_w, img_s1, img_s2 = deepcopy(img), deepcopy(img), deepcopy(img)</span><br></pre></td></tr></table></figure></p><p>进行强扰动的代码(<strong>以处理$x^{s_1}$为例</strong>)如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">    <span class="comment"># 随机调整亮度、对比度、饱和度和色调</span></span><br><span class="line">    img_s1 = transforms.ColorJitter(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.25</span>)(img_s1)</span><br><span class="line">img_s1 = transforms.RandomGrayscale(p=<span class="number">0.2</span>)(img_s1)  <span class="comment"># 随机灰度化</span></span><br><span class="line">img_s1 = blur(img_s1, p=<span class="number">0.5</span>)    <span class="comment"># 随机模糊</span></span><br><span class="line">cutmix_box1 = obtain_cutmix_box(img_s1.size[<span class="number">0</span>], p=<span class="number">0.5</span>)  <span class="comment"># 随机获取CutMix的区域</span></span><br></pre></td></tr></table></figure></p><p>因为这些数据增强方法设置了概率，故不同的epoch或者是$x^{s_1}$和$x^{s_2}$之间，增强的效果都是不同的。其中我对于<code>CutMix</code>操作还比较好奇，去查看了函数定义。发现CutMix就是mask掉一块区域(该区域的宽高和位置都是一定程度随机的)，然后用其他图片中<strong>相同位置的区域</strong>来<a href="#padding">填充</a>。</p><p>由于<code>Pascal</code>数据集的标注图像<code>mask</code>中包含254这个无效像素值，没有对应类别，作者使用<code>ignore_mask</code>忽略它：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ignore_mask = Image.fromarray(np.zeros((mask.size[<span class="number">1</span>], mask.size[<span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">img_s1, ignore_mask = normalize(img_s1, ignore_mask)</span><br><span class="line">img_s2 = normalize(img_s2)</span><br><span class="line"></span><br><span class="line">mask = torch.from_numpy(np.array(mask)).long()</span><br><span class="line">ignore_mask[mask == <span class="number">254</span>] = <span class="number">255</span></span><br></pre></td></tr></table></figure><br>取值为255是因为<code>crop</code>操作对哪些裁剪时遇到的padding都设置值为255，同样也是无效区域，这里相当于合并了。于是，经过图像增广等操作后，我们的输入数据可能就包含以下几个部分：</p><ul><li>$x^w$: 即<code>img_w</code>，在return时还需要normalize一下；</li><li>$x^{s_1}$: 即<code>img_s1</code>，经过强扰动，且已经normalize；</li><li>$x^{s_2}$: 即<code>img_s2</code>，经过强扰动，且已经normalize；</li><li>ignore_mask: 用于忽略无效的像素；</li><li>cutmix_box1: 从$x^{s_1}$获取的mask掉的CutMix区域；</li><li>cutmix_box2: 从$x^{s_2}$获取的mask掉的CutMix区域；</li></ul><p>了解增广的细节后，我们可以构建3个数据集，分别是有标签监督数据、无标签数据、和验证数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">trainset_u = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;train_u&#x27;</span>,</span><br><span class="line">                         cfg[<span class="string">&#x27;crop_size&#x27;</span>], args.unlabeled_id_path)</span><br><span class="line">trainset_l = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;train_l&#x27;</span>,</span><br><span class="line">                         cfg[<span class="string">&#x27;crop_size&#x27;</span>], args.labeled_id_path, </span><br><span class="line">                         nsample=<span class="built_in">len</span>(trainset_u.ids))</span><br><span class="line">valset = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;val&#x27;</span>)</span><br></pre></td></tr></table></figure><br>将它们分别转为Dataloader后，通过<a id="canshu">下面的代码</a>进行分批训练：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loader = <span class="built_in">zip</span>(trainloader_l, trainloader_u, trainloader_u)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, ((img_x, mask_x),</span><br><span class="line">        (img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2),</span><br><span class="line">        (img_u_w_mix, img_u_s1_mix, img_u_s2_mix, ignore_mask_mix, _, _)) </span><br><span class="line">        <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br></pre></td></tr></table></figure><br><u><strong>接下来的分析，都在上述循环中，请关注从<code>loader</code>中取出的这些数据！</strong></u></p><p><a id="padding">填充CutMix </a></p><p>最后一步，将<code>cutmix</code>操作完成，具体来说，我们用第二个<code>trainloader_u</code>中获取的数据来填充我们的$s_1$和$s_2$：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img_u_s1[cutmix_box1.unsqueeze(<span class="number">1</span>).expand(img_u_s1.shape) == <span class="number">1</span>] = \</span><br><span class="line">    img_u_s1_mix[cutmix_box1.unsqueeze(<span class="number">1</span>).expand(img_u_s1.shape) == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">img_u_s2[cutmix_box2.unsqueeze(<span class="number">1</span>).expand(img_u_s2.shape) == <span class="number">1</span>] = \</span><br><span class="line">    img_u_s2_mix[cutmix_box2.unsqueeze(<span class="number">1</span>).expand(img_u_s2.shape) == <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><h4 id="2-2-模型预测"><a href="#2-2-模型预测" class="headerlink" title="2.2 模型预测"></a>2.2 模型预测</h4><p>在<a href="#fig1">图7</a>中，这部分表示为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature of weakly perturbed image</span></span><br><span class="line">feat_w = g(x_w)</span><br><span class="line"><span class="comment"># perturbed feature</span></span><br><span class="line">feat_fp = nn.Dropout2d(<span class="number">0.5</span>)(feat_w)</span><br><span class="line"><span class="comment"># four predictions from four forward streams</span></span><br><span class="line">p_w, p_fp = h(torch.cat((feat_w, feat_fp))).chunk(<span class="number">2</span>)</span><br><span class="line">p_s1, p_s2 = f(torch.cat((x_s1, x_s2))).chunk(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>在<code>unimatch.py</code>中，并没有展现出将$f$拆分为 $h(g(x))$ 的细节，而是直接通过model生成预测，所以<code>dropout2d</code>应该包含在model里了。我们截取了<a id="pred_x">下面代码</a>，作为上述部分的实现，并提供解释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_lb, num_ulb = img_x.shape[<span class="number">0</span>], img_u_w.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># img_x是带标签的监督图像数据，通过计算pred_x可以进行有监督训练.</span></span><br><span class="line">preds, preds_fp = model(torch.cat((img_x, img_u_w)), <span class="literal">True</span>)  <span class="comment"># need_fp=True,进行dropout</span></span><br><span class="line">pred_x, pred_u_w = preds.split([num_lb, num_ulb])   <span class="comment"># pred_u_w =&gt; p_w</span></span><br><span class="line">pred_u_w_fp = preds_fp[num_lb:] <span class="comment"># pred_u_w_fp =&gt; p_fp,进行特征层面的自监督训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pred_u_s1 =&gt; p_s1, pred_u_s2 =&gt; p_s2</span></span><br><span class="line">pred_u_s1, pred_u_s2 = model(torch.cat((img_u_s1, img_u_s2))).chunk(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><h4 id="2-3-Loss计算"><a href="#2-3-Loss计算" class="headerlink" title="2.3 Loss计算"></a>2.3 Loss计算</h4><p>在<a href="#fig1">图7</a>中指代一下部分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hard (one-hot) pseudo mask</span></span><br><span class="line">mask_w = p_w.argmax(dim=<span class="number">1</span>).detach()</span><br><span class="line"><span class="comment"># loss from image- and feature-level perturbation</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">p_s = torch.cat((p_s1, p_s2))</span><br><span class="line">loss_s = criterion(p_s, mask_w.repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">loss_fp = criterion(p_fp, mask_w)</span><br><span class="line"><span class="comment"># final unsupervised loss</span></span><br><span class="line">loss_u = (loss_s + loss_fp) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure></p><p>由于$x^{s_1}$和$x^{s_2}$进行过<code>CutMix</code>，而$x^{w}$并没有做这些强扰动，所以想得到无标签自监督的标签<code>mask_w</code>很复杂，因此损失的计算并不简单。我们基于现有<a href="#canshu">参数</a>逐步分析：</p><p>首先，对于用来填充cutmix的数据<code>img_u_w_mix</code>，我们利用模型预测其分割结果<code>mask_u_w_mix</code>；同时，$x^w$也通过模型获得了<code>pred_u_w</code>(见<a href="#ED">2.2</a>)，我们同样可以获得<code>mask_u_w</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    pred_u_w_mix = model(img_u_w_mix).detach()</span><br><span class="line">    conf_u_w_mix = pred_u_w_mix.softmax(dim=<span class="number">1</span>).<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    mask_u_w_mix = pred_u_w_mix.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">pred_u_w = pred_u_w.detach()</span><br><span class="line">conf_u_w = pred_u_w.softmax(dim=<span class="number">1</span>).<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">mask_u_w = pred_u_w.argmax(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>由于我们知道了$s_1$和$s_2$<code>CutMix</code>框的位置，所以我们直接将上面的两个图像的mask结合，就可以得到自监督label：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于cutmix框不一样，这里分别获得cutmixed1和cutmixed2的mask及其conf等</span></span><br><span class="line">mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = \</span><br><span class="line">    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()</span><br><span class="line">mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = \</span><br><span class="line">    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()</span><br><span class="line"></span><br><span class="line">mask_u_w_cutmixed1[cutmix_box1 == <span class="number">1</span>] = mask_u_w_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line">conf_u_w_cutmixed1[cutmix_box1 == <span class="number">1</span>] = conf_u_w_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line">ignore_mask_cutmixed1[cutmix_box1 == <span class="number">1</span>] = ignore_mask_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">mask_u_w_cutmixed2[cutmix_box2 == <span class="number">1</span>] = mask_u_w_mix[cutmix_box2 == <span class="number">1</span>]</span><br><span class="line">conf_u_w_cutmixed2[cutmix_box2 == <span class="number">1</span>] = conf_u_w_mix[cutmix_box2 == <span class="number">1</span>]</span><br><span class="line">ignore_mask_cutmixed2[cutmix_box2 == <span class="number">1</span>] = ignore_mask_mix[cutmix_box2 == <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><p>最后，<strong>我们给出4个损失</strong>：</p><p>第一个损失：有监督损失。使用img_x的<a href="#pred_x">预测结果</a><code>pred_x</code>和标签<code>mask_x</code>计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_x = criterion_l(pred_x, mask_x)</span><br></pre></td></tr></table></figure></p><p>第二&amp;三个损失：图像层面自监督损失。通过$s_1$和$s_2$的预测结果及其对应label计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)</span><br><span class="line">loss_u_s1 = loss_u_s1 * (</span><br><span class="line">    (conf_u_w_cutmixed1 &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask_cutmixed1 != <span class="number">255</span>))</span><br><span class="line">loss_u_s1 = loss_u_s1.<span class="built_in">sum</span>() / (ignore_mask_cutmixed1 != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)</span><br><span class="line">loss_u_s2 = loss_u_s2 * (</span><br><span class="line">    (conf_u_w_cutmixed2 &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask_cutmixed2 != <span class="number">255</span>))</span><br><span class="line">loss_u_s2 = loss_u_s2.<span class="built_in">sum</span>() / (ignore_mask_cutmixed2 != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br></pre></td></tr></table></figure></p><p>第四个损失：特征层面的自监督损失。通过$x^{fp}$的<a href="#pred_x">预测结果</a><code>pred_u_w_fp</code>和$x^w$的预测结果计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss_u_w_fp = criterion_u(pred_u_w_fp, mask_u_w)</span><br><span class="line">loss_u_w_fp = loss_u_w_fp * ((conf_u_w &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask != <span class="number">255</span>))</span><br><span class="line">loss_u_w_fp = loss_u_w_fp.<span class="built_in">sum</span>() / (ignore_mask != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br></pre></td></tr></table></figure></p><p><strong>Total</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = (loss_x + loss_u_s1 * <span class="number">0.25</span> + loss_u_s2 * <span class="number">0.25</span> + loss_u_w_fp * <span class="number">0.5</span>) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure></p><h3 id="3-模型结构解析"><a href="#3-模型结构解析" class="headerlink" title="3. 模型结构解析"></a>3. 模型结构解析</h3><p>模型结构在本文中显然是Encoder-Decoder架构，具体而言，我们进行如下分析：</p><h4 id="3-1-Encoder"><a href="#3-1-Encoder" class="headerlink" title="3.1 Encoder"></a>3.1 Encoder</h4><p>该论文的Encoder设置为ResNet和xception，为了便于讨论，只以ResNet101为例。这里不再展示模型的结构，直接看它的<code>forward</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">base_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.conv1(x)       <span class="comment"># (3, 224, 224) =&gt; (128, 112, 112)</span></span><br><span class="line">    x = self.bn1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.maxpool(x)     <span class="comment"># (128, 112, 112) =&gt; (128, 56, 56)</span></span><br><span class="line"></span><br><span class="line">    c1 = self.layer1(x)     <span class="comment"># 3个Bottleneck, (128, 56， 56) =&gt; (64*4, 56, 56)</span></span><br><span class="line">    c2 = self.layer2(c1)    <span class="comment"># 4个Bottleneck, stride=2, (256, 56, 56) =&gt; (128*4, 28, 28)</span></span><br><span class="line">    c3 = self.layer3(c2)    <span class="comment"># 23个Bottleneck, stride=2, (512, 28, 28) =&gt; (256*4, 14, 14)</span></span><br><span class="line">    c4 = self.layer4(c3)    <span class="comment"># 3个Bottleneck, stride=2, (1024, 14, 14) =&gt; (512*4, 7, 7)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c1, c2, c3, c4</span><br></pre></td></tr></table></figure><br>我们以一张大小为<code>(3,224,224)</code>的图片为例，相关提示已经在上面的注释中。通过resnet，我们已得到两种视角的特征：<code>c1</code>和<code>c4</code>。</p><h4 id="3-2-Decoder"><a href="#3-2-Decoder" class="headerlink" title="3.2 Decoder"></a>3.2 Decoder</h4><p>首先介绍Decoder的一个模块<code>ASPPModule</code>，由<code>ASPPConv</code>和<code>ASPPPooling</code>等组合而成。</p><p><code>ASPPConv</code>引入了空洞卷积，其维度计算公式为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">H_out = (H_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1</span><br><span class="line">W_out = (W_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1</span><br></pre></td></tr></table></figure><br><img src="https://picx.zhimg.com/70/v2-a08645e392a6a5cb49e271e5310f0dd8_1440w.awebp?source=172ae18b&amp;biz_tag=Post" alt><br>其实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ASPPConv</span>(<span class="params">in_channels, out_channels, atrous_rate</span>):</span><br><span class="line">    block = nn.Sequential(nn.Conv2d(in_channels, out_channels, <span class="number">3</span>, padding=atrous_rate,</span><br><span class="line">                                    dilation=atrous_rate, bias=<span class="literal">False</span>),</span><br><span class="line">                          nn.BatchNorm2d(out_channels),</span><br><span class="line">                          nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> block</span><br></pre></td></tr></table></figure><br><code>ASPPPooling</code>的代码为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ASPPPooling</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPPPooling, self).__init__()</span><br><span class="line">        self.gap = nn.Sequential(nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">                                 nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                 nn.BatchNorm2d(out_channels),</span><br><span class="line">                                 nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        h, w = x.shape[-<span class="number">2</span>:]</span><br><span class="line">        pool = self.gap(x)</span><br><span class="line">        <span class="keyword">return</span> F.interpolate(pool, (h, w), mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>举例而言，数据维度经过如下变化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">设 x.size = (<span class="number">2048</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line">|__nn.AdaptiveAvgPool2d(<span class="number">1</span>) =&gt;  (<span class="number">2048</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__nn.Conv2d(<span class="number">2048</span>, <span class="number">256</span>, <span class="number">1</span>, bias=<span class="literal">False</span>) =&gt; (<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__nn.BatchNorm2d(out_channels) =&gt; (<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__nn.ReLU(<span class="literal">True</span>) =&gt; (<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__F.interpolate =&gt; (<span class="number">256</span>, <span class="number">7</span>, <span class="number">7</span>)     <span class="comment"># 插值</span></span><br></pre></td></tr></table></figure></p><p>最后，<code>ASPPModule</code>通过这些模块组合而成，代码为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ASPPModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, atrous_rates</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPPModule, self).__init__()</span><br><span class="line">        out_channels = in_channels // <span class="number">8</span></span><br><span class="line">        rate1, rate2, rate3 = atrous_rates</span><br><span class="line"></span><br><span class="line">        self.b0 = nn.Sequential(nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                nn.BatchNorm2d(out_channels),</span><br><span class="line">                                nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.b1 = ASPPConv(in_channels, out_channels, rate1)</span><br><span class="line">        self.b2 = ASPPConv(in_channels, out_channels, rate2)</span><br><span class="line">        self.b3 = ASPPConv(in_channels, out_channels, rate3)</span><br><span class="line">        self.b4 = ASPPPooling(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">        self.project = nn.Sequential(nn.Conv2d(<span class="number">5</span> * out_channels, out_channels,</span><br><span class="line">                                               <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                     nn.BatchNorm2d(out_channels),</span><br><span class="line">                                     nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        feat0 = self.b0(x)</span><br><span class="line">        feat1 = self.b1(x)</span><br><span class="line">        feat2 = self.b2(x)</span><br><span class="line">        feat3 = self.b3(x)</span><br><span class="line">        feat4 = self.b4(x)</span><br><span class="line">        y = torch.cat((feat0, feat1, feat2, feat3, feat4), <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.project(y)</span><br></pre></td></tr></table></figure><br>经过该模块的数据，例如<code>(2048,7,7)</code>，最终变为<code>(256, 7, 7)</code>。Decoder的具体实现见3.3.</p><h4 id="3-3-Total-model"><a href="#3-3-Total-model" class="headerlink" title="3.3 Total model"></a>3.3 Total model</h4><p>我们将分析写成了注释，添加在下面的代码中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DeepLabV3Plus</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>):</span><br><span class="line">        <span class="built_in">super</span>(DeepLabV3Plus, self).__init__()</span><br><span class="line">        <span class="comment"># Encoder,默认为resnet101</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;resnet&#x27;</span> <span class="keyword">in</span> cfg[<span class="string">&#x27;backbone&#x27;</span>]:</span><br><span class="line">            self.backbone = resnet.__dict__[cfg[<span class="string">&#x27;backbone&#x27;</span>]](</span><br><span class="line">                pretrained=<span class="literal">True</span>, </span><br><span class="line">                replace_stride_with_dilation=cfg[<span class="string">&#x27;replace_stride_with_dilation&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> cfg[<span class="string">&#x27;backbone&#x27;</span>] == <span class="string">&#x27;xception&#x27;</span></span><br><span class="line">            self.backbone = xception(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        low_channels = <span class="number">256</span>      <span class="comment"># 经过self.head后的通道数,即ASPPModule.out_channels</span></span><br><span class="line">        high_channels = <span class="number">2048</span>    <span class="comment"># 经过resnet后的c4的通道数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># cfg[&#x27;dilations&#x27;] = [6, 12, 18],提供了不同感受野的信息,并经过融合。</span></span><br><span class="line">        self.head = ASPPModule(high_channels, cfg[<span class="string">&#x27;dilations&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 减少c1的通道数，256 =&gt; 48, 浓缩信息+减少参数量</span></span><br><span class="line">        self.reduce = nn.Sequential(nn.Conv2d(low_channels, <span class="number">48</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                    nn.BatchNorm2d(<span class="number">48</span>),</span><br><span class="line">                                    nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 融合c1(低级信息)和c4(高级信息)的信息,强制输出维度为256</span></span><br><span class="line">        self.fuse = nn.Sequential(nn.Conv2d(high_channels // <span class="number">8</span> + <span class="number">48</span>, <span class="number">256</span>, <span class="number">3</span>, </span><br><span class="line">                                            padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                  nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">                                  nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">                                  nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                  nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">                                  nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">        self.classifier = nn.Conv2d(<span class="number">256</span>, cfg[<span class="string">&#x27;nclass&#x27;</span>], <span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, need_fp=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        设x.size = (2,3,224,224), need_fp=True,模拟将s1,s2这2个数据cat后进入模型</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h, w = x.shape[-<span class="number">2</span>:]     <span class="comment"># h=224,w=224</span></span><br><span class="line"></span><br><span class="line">        feats = self.backbone.base_forward(x)   <span class="comment"># 获得4种特征图c1~c4</span></span><br><span class="line">        c1, c4 = feats[<span class="number">0</span>], feats[-<span class="number">1</span>]            <span class="comment"># c1=(2,256,56,56),c4=(2,2048,7,7)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> need_fp:</span><br><span class="line">            <span class="comment"># 这里nn.Dropout2d(0.5)就是构建fp</span></span><br><span class="line">            <span class="comment"># 这里做了一些拼接,例如</span></span><br><span class="line">            <span class="comment"># torch.cat((c1, nn.Dropout2d(0.5)(c1))).size = (4,256,56,56)</span></span><br><span class="line">            <span class="comment"># torch.cat((c4, nn.Dropout2d(0.5)(c4))).size = (4,2048,7,7)</span></span><br><span class="line">            outs = self._decode(torch.cat((c1, nn.Dropout2d(<span class="number">0.5</span>)(c1))),</span><br><span class="line">                                torch.cat((c4, nn.Dropout2d(<span class="number">0.5</span>)(c4))))</span><br><span class="line">            <span class="comment"># outs.size = (4,21,56,56),注意这是因为self.classifier将channel设为class数21</span></span><br><span class="line">            outs = F.interpolate(outs, size=(h, w), mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># out和out_fp分别代表x_w和经过特征扰动后的fp的输出</span></span><br><span class="line">            out, out_fp = outs.chunk(<span class="number">2</span>)    </span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> out, out_fp</span><br><span class="line"></span><br><span class="line">        out = self._decode(c1, c4)      <span class="comment"># out.size = (2,21,56,56)</span></span><br><span class="line">        out = F.interpolate(out, size=(h, w), mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># # out.size = (2,21,224,224)</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_decode</span>(<span class="params">self, c1, c4</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 设c1.size=(256,56,56), c4.size=(2048,7,7), 详见resnet &quot;&quot;&quot;</span></span><br><span class="line">        c4 = self.head(c4)      <span class="comment"># c4: (2048,7,7) =&gt; (256,7,7)</span></span><br><span class="line">        <span class="comment"># c4: (256,7,7) =&gt; (256,56,56)</span></span><br><span class="line">        c4 = F.interpolate(c4, size=c1.shape[-<span class="number">2</span>:], mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        c1 = self.reduce(c1)    <span class="comment"># c1: (256,56,56) =&gt; (48,56,56)</span></span><br><span class="line"></span><br><span class="line">        feature = torch.cat([c1, c4], dim=<span class="number">1</span>)    <span class="comment"># feature.size=(256+48,56,56)</span></span><br><span class="line">        feature = self.fuse(feature)            <span class="comment"># =&gt; feature.size=(256,56,56)</span></span><br><span class="line"></span><br><span class="line">        out = self.classifier(feature)          <span class="comment"># 像素级分类，out=(21,56,56)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="4-复现结果"><a href="#4-复现结果" class="headerlink" title="4. 复现结果"></a>4. 复现结果</h3><p>由于时间仓促，目前只复现了backbone为<code>ResNet101</code>在数据集<code>Pascal</code>上的表现，如下表所示:</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Pascal</strong> / UniMatch &#124; ResNet101</th><th style="text-align:center">92</th><th style="text-align:center">183</th><th style="text-align:center">366</th><th style="text-align:center">732</th><th style="text-align:center">1464</th></tr></thead><tbody><tr><td style="text-align:center">Paper</td><td style="text-align:center"><strong>75.2</strong></td><td style="text-align:center"><strong>77.2</strong></td><td style="text-align:center"><strong>78.8</strong></td><td style="text-align:center"><strong>79.9</strong></td><td style="text-align:center"><strong>81.2</strong></td></tr><tr><td style="text-align:center">OurWork</td><td style="text-align:center"><strong>75.2</strong></td><td style="text-align:center">76.8</td><td style="text-align:center">78.5</td><td style="text-align:center">79.2</td><td style="text-align:center">80.8</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Pascal</strong> / UniMatch &#124; ResNet101</th><th style="text-align:center">1/16</th><th style="text-align:center">1/8</th><th style="text-align:center">1/4</th></tr></thead><tbody><tr><td style="text-align:center">Paper &#124; <font color="gray"><strong>321</strong></font></td><td style="text-align:center">76.5</td><td style="text-align:center">77.0</td><td style="text-align:center">77.2</td></tr><tr><td style="text-align:center">OurWork &#124; <font color="gray"><strong>321</strong></font></td><td style="text-align:center"><strong>76.6</strong></td><td style="text-align:center"><strong>77.4</strong></td><td style="text-align:center"><strong>77.4</strong></td></tr></tbody></table></div><p>我们的复现基本接近或者达到论文中的精度，证明有效。我们展示两张复现时的截图，可供参考：<br><img src="/2023/06/09/unimatch/366_101.png" alt></p><center>图8. resnet101 | pascal-366</center><p><img src="/2023/06/09/unimatch/732_101.png" alt></p><center>图8. resnet101 | pascal-732</center><h2 id="3-基于FlexMatch的改进"><a href="#3-基于FlexMatch的改进" class="headerlink" title="3. 基于FlexMatch的改进"></a>3. <a name="基于flexmatch的改进"></a>基于FlexMatch的改进</h2><p>由于作者认为<code>FixMatch</code>足够强大、足够简单，所以以其为baseline。我们尝试使用<code>FlexMatch</code>方法为baseline设计一个类似的<code>UniMatch</code>模型。<br><img src="/2023/06/09/unimatch/9.png" alt></p><center>图9. FlexMatch阈值调整方法</center><p><code>FlexMatch</code>方法，就是将下式固定的 $\tau$ 转化为可以动态调整的形式，但又不显示引入参数：</p><script type="math/tex; mode=display">\mathcal{L}_u = \frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \tau\right)\text{H}(\hat{q}_b, p_m(y|\mathcal{A}(u_b)))\tag{6}</script><p>这种动态调整方法被称为<code>Curriculum Pseudo Labeling (CPL)</code>方法。</p><p>FlexMatch认为一个<u><strong>类别预测的置信度越低，说明对该类的学习仍不够充分，应该降低阈值鼓励学习</strong></u>，即阈值和类别的学习效果有关。论文中使用预测属于该类且置信度大于阈值的无标签数据数量衡量一个类别的学习效果：<br><a id="tag7"></a></p><script type="math/tex; mode=display">\sigma_{t}(c) = \sum_{n=1}^{N}\Bbb{I}\left(\max(p_{m,t}(y|u_n))\geq \tau\right)\Bbb{I}\left(\arg \max(p_{m,t}(y|u_n)) = c \right)\tag{7}</script><p>&lt;/a&gt;</p><p>其中 $t$ 是指step t时刻。我们将上式得到的学习效果 $\sigma_{t}(c)$ 归一化，得到step t时每个类别的阈值：<br><a id="tag8"></a></p><script type="math/tex; mode=display">\beta_t(c) = \frac{\sigma_t(c)}{\max_{c} \sigma_t} \tag{8}</script><p>&lt;/a&gt;<br><a id="tag9"></a></p><script type="math/tex; mode=display">\mathcal{T}_t(c) = \beta_t(c)\cdot\tau\tag{9}</script><p>&lt;/a&gt;</p><p>实际上这个动态阈值$\mathcal{T}_t(c)$还会施加一个非线性函数：</p><script type="math/tex; mode=display">\mathcal{T}_t(c) = \mathcal{M}\left(\beta_t(c)\right)\cdot \tau\tag{10}</script><p>最后损失函数修改为：</p><script type="math/tex; mode=display">\mathcal{L}_{u,t} = \frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \mathcal{T}_t(\arg \max q_b) \right)\text{H}(\hat{q}_b, p_m(y|\mathcal{A}(u_b)))\tag{11}</script><h3 id="1-改进的代码"><a href="#1-改进的代码" class="headerlink" title="1. 改进的代码"></a>1. 改进的代码</h3><p>通过调研<code>TorchSSL</code>代码库，我们可以对<code>FlexMatch</code>方法有更清晰的认识。我们考虑在<code>fixmatch.py</code>和<code>unimatch.py</code>上修改代码，加入动态阈值。</p><h4 id="1-1-fixmatch-py"><a href="#1-1-fixmatch-py" class="headerlink" title="1.1 fixmatch.py"></a>1.1 fixmatch.py</h4><p>在每一轮开始之前，我们要预定义2个变量：</p><ul><li><code>selected_label</code>：一个存储分类情况的变量。在flexmatch的源码实现中，该参数将记录<strong>所有未标记图片</strong>的类别硬标签。但在语义分割任务中，一张图片的类别标签大小为<code>(W,H)</code>，一旦图像数量较大则会导致空间占用较多、运行速度变慢，所以这里采用一个<code>队列queue</code>来实现它。当队列已满时，将最早进入队列的batch移除，将新的batch移入。默认的队列长度<code>queue_length</code>为<code>batch_size</code>的100倍。</li><li><code>classwise_acc</code>：记录每个类别的学习情况，即公式(<a href="#tag7">7</a>)的$\sigma_t(c)$。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># selected_label.size = (N,W,H),记录每个像素的类别</span></span><br><span class="line"><span class="comment"># classwise_acc.size = (C,), 记录每个类别的准确率</span></span><br><span class="line">queue_length = cfg[<span class="string">&quot;batch_size&quot;</span>] * cfg[<span class="string">&quot;queue_num&quot;</span>]     <span class="comment"># 记录队列总长度</span></span><br><span class="line">head_length = cfg[<span class="string">&quot;batch_size&quot;</span>]                         <span class="comment"># 记录队列的头</span></span><br><span class="line">selected_label = torch.ones(</span><br><span class="line">    (queue_length, cfg[<span class="string">&quot;crop_size&quot;</span>], cfg[<span class="string">&quot;crop_size&quot;</span>]), dtype=torch.long</span><br><span class="line">    ) * cfg[<span class="string">&quot;nclass&quot;</span>]</span><br><span class="line">selected_label = selected_label.cuda().detach()</span><br><span class="line">classwise_acc = torch.zeros((cfg[<span class="string">&quot;nclass&quot;</span>],)).cuda()</span><br></pre></td></tr></table></figure><p>在每一个step计算loss之前，我们需要根据式(<a href="#tag8">8</a>)得到归一化值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pseudo_counter = torch.bincount(selected_label.reshape(-<span class="number">1</span>)) <span class="comment"># 各类别预测数量</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">max</span>(pseudo_counter) &lt; selected_label.shape[<span class="number">0</span>] * (cfg[<span class="string">&quot;crop_size&quot;</span>] ** <span class="number">2</span>):</span><br><span class="line">    classwise_acc = pseudo_counter[:cfg[<span class="string">&quot;nclass&quot;</span>]] / <span class="built_in">max</span>(pseudo_counter)</span><br></pre></td></tr></table></figure><br>接着我们根据式(<a href="#tag9">9</a>,10)可以得到动态阈值，并以此计算loss：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># u_w_cutmixed_thresh为动态阈值</span></span><br><span class="line">u_w_cutmixed_thresh = torch.nn.functional.one_hot(</span><br><span class="line">    mask_u_w_cutmixed, num_classes=cfg[<span class="string">&quot;nclass&quot;</span>]).to(torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># u_w_cutmixed_thresh.size = (B,W,H,C)</span></span><br><span class="line">u_w_cutmixed_thresh =  torch.matmul(u_w_cutmixed_thresh, classwise_acc)  <span class="comment"># size=(B,W,H)</span></span><br><span class="line">u_w_cutmixed_thresh = <span class="number">0.95</span> * u_w_cutmixed_thresh / (<span class="number">2.</span> - u_w_cutmixed_thresh)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mask_u_w_cutmixed是硬标签(B,W,H)，pred_u_s是软标签(B,C,W,H)</span></span><br><span class="line">loss_u_s = criterion_u(pred_u_s, mask_u_w_cutmixed)     </span><br><span class="line">loss_u_s = loss_u_s * ((conf_u_w_cutmixed - u_w_cutmixed_thresh &gt;= <span class="number">0</span>) &amp; \</span><br><span class="line">                                       (ignore_mask_cutmixed != <span class="number">255</span>))</span><br><span class="line">loss_u_s = loss_u_s.<span class="built_in">sum</span>() / (ignore_mask_cutmixed != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新selected_label</span></span><br><span class="line">select = (conf_u_w_cutmixed &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]).long()</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(select.shape[<span class="number">0</span>]):</span><br><span class="line">    selected_label[head_length-cfg[<span class="string">&quot;batch_size&quot;</span>]:head_length][k][select[k] == <span class="number">1</span>] =\</span><br><span class="line">                                                mask_u_w_cutmixed[k][select[k] == <span class="number">1</span>]</span><br></pre></td></tr></table></figure><br>一个batch的阈值矩阵大小为<code>(B,W,H)</code>，我们使用one-hot编码使其可以直接与<code>classwise_acc</code>相乘。在计算完loss之后，我们要更新<code>selected_label</code>(记录队列中图像的语义分割标签)，以供下一个step使用。一般我们只需要更新队列末尾的那个batch即可。</p><p>最后，我们需要代码来完成队列的push和pop，以实现动态的变化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">new_batch_data = cfg[<span class="string">&quot;nclass&quot;</span>] * torch.ones(</span><br><span class="line">    (cfg[<span class="string">&quot;batch_size&quot;</span>], cfg[<span class="string">&quot;crop_size&quot;</span>], cfg[<span class="string">&quot;crop_size&quot;</span>]), dtype=torch.long).cuda()</span><br><span class="line"><span class="keyword">if</span> head_length &lt; queue_length:</span><br><span class="line">    head_length += cfg[<span class="string">&quot;batch_size&quot;</span>]    <span class="comment"># 添加新数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    selected_label[:-cfg[<span class="string">&quot;batch_size&quot;</span>]] = selected_label.clone()[cfg[<span class="string">&quot;batch_size&quot;</span>]:]</span><br><span class="line">    selected_label[-cfg[<span class="string">&quot;batch_size&quot;</span>]:] = new_batch_data</span><br></pre></td></tr></table></figure></p><h3 id="2-简单的实验验证"><a href="#2-简单的实验验证" class="headerlink" title="2. 简单的实验验证"></a>2. 简单的实验验证</h3><p>由于时间比较局促，目前只验证了<code>model=ResNet101</code>，<code>dataset=Pascal</code>中的部分实验：</p><p><strong>实验1：在crop=321的情况下：</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Pascal</strong> / ResNet101</th><th style="text-align:center">92</th><th style="text-align:center">183</th><th style="text-align:center">366</th><th style="text-align:center">732</th><th style="text-align:center">1464</th></tr></thead><tbody><tr><td style="text-align:center">Paper - <strong>UniMatch</strong></td><td style="text-align:center"><strong>75.2</strong></td><td style="text-align:center"><strong>77.2</strong></td><td style="text-align:center"><strong>78.8</strong></td><td style="text-align:center"><strong>79.9</strong></td><td style="text-align:center"><strong>81.2</strong></td></tr><tr><td style="text-align:center">OurWork - <strong>FlexUniMatch</strong></td><td style="text-align:center">/</td><td style="text-align:center">73.9</td><td style="text-align:center">76.2</td><td style="text-align:center">78.5</td><td style="text-align:center">/</td></tr></tbody></table></div><p><strong>实验2：FixMatch vs. FlexMatch</strong></p><p>目前只测试了一组数据，在<code>pascal-crop_321-732-resnet101</code>的设置下，结果为<code>76.79</code>，与<code>FixMatch</code>的对应值<code>77.8</code>还有不小的差距。</p><h3 id="3-一些实验感悟与未来探讨"><a href="#3-一些实验感悟与未来探讨" class="headerlink" title="3. 一些实验感悟与未来探讨"></a>3. 一些实验感悟与未来探讨</h3><p>很遗憾，在实验中并没有把FlexMatch方法做到与FixMatch方法接近。其中<code>loss_u</code>和<code>loss_fp</code>损失依然较大，没能下降到原有水平。</p><p>我对造成这个问题的原因的进行了简单分析：</p><ol><li>我们为了提高效率选择了使用一个队列，损失了较多数据的信息，队列中的类别可能不能反映整体数据分布；</li><li>我们还没有对类别分布展开分析，如果每一个step的类别分布不均衡的话会影响效果；</li><li>我们没有调整任何超参数(即，和<code>UniMatch</code>完全一致)，可能会导致lr等不合适的情况；</li><li>我们没有修改扰动的组合，也没有尝试验证特征扰动和多流扰动的其他可能；</li><li>尽管<code>FlexMatch</code>并没有显式增加参数，但由于对于动态阈值调整的变量$\beta_t(c)$涉及到<strong>全部数据的类别信息</strong>、以及非线性函数的选择，在语义分割的像素级分类上应用并不简单。</li></ol><p>不过，我们也发现增加队列的长度对提升模型效果有一定帮助，但效果增长有限。</p>]]></content>
      
      
      <categories>
          
          <category> 夏令营经历 </category>
          
          <category> Semantic segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> papers reproduced </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Contrastive Learning based Vision-Language Pre-Training</title>
      <link href="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/"/>
      <url>/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/</url>
      
        <content type="html"><![CDATA[<center><h1>基于对比学习的多模态预训练方法</h1></center><center><h2>——以Vision-Language PTMs为例<h2></h2></h2></center><blockquote><p>前记：这篇文章是我在面试中科大毛震东老师组时写的一份报告，整理一下2023年之前基于对比学习的多模态预训练模型文献。算是给我的<code>Image-to-Poem</code>项目做的一个综述性调查，给自己一个方向。</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">随着大量图像-文本对数据集的涌现，使用对比学习进行多模态模型预训练的方法也愈发成熟。</span><br><span class="line">在本文中，我们首先介绍了图像-文本对比学习任务(ITC)，</span><br><span class="line">接着按照时间线回顾了CLIP、ALBEF、BLIP&amp;BLIP2三个重要的多模态预训练模型。</span><br><span class="line">最后，鉴于多模态预训练模型强大的性能，我们尝试了Image-to-Poem的特殊字幕生成任务。</span><br></pre></td></tr></table></figure><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a><em>1. Introduction</em></h2><p>最近的工作证明，使用互联网中的大量图像-文本对进行多模态模型预训练可以显著提升模型的表征提取能力和泛化性能，但也对图像表征(Representation)和文本表征的融合提出了挑战。如何将图像表征与文本表征进行匹配是重要的问题。</p><p>基本地，这里将多模态模型预训练过程分为3个阶段：</p><ol><li>将图像和文本分别编码为保留语义的表示向量(latent representations)；</li><li>设计一个模型或结构，来模拟两种模式的交融；</li><li>设计有效的预训练任务来预训练Vision-Language Pre-Trained Model.</li></ol><p>图像-文本对比学习(ITC)作为一种有效的预训练任务，欲将成对的图像-文本对的表示向量尽可能地拉近，而将不成对的负例样本对(negatives)的表示向量尽可能地远离。一般地，对比学习的损失函数(image-to-text为例)可表示为如下形式：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{i2t}} = -\Bbb{E}_{(W,V)\in \mathcal{D}}\left[\log\frac{s_\theta (\pmb{h}_v, \pmb{h}_w)}{\sum_{W'} s_\theta (\pmb{h}_v, \pmb{h}_{w'})} \right] \tag{1}</script><p>其中，$(W, V)$是图像-文本对数据集的一个样本，是关于$V$的负样本，$\textit{\pmb{h}}$是图像或文本的表征向量。在后续对多模态的各个预训练模型的调研中，可以发现上述对比学习思想和损失函数很少被改变；但是为了提升ITC任务的有效性，很多方法被引入，例如：正负样本对的定义、ITC任务的出现时机(如align before fusing)、Momentum Distillation等，这将在后续章节详细讨论。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/1.png" alt="图1. 多模态模型结构分类"></p><center>图1. 多模态模型结构分类</center><p>考虑到多模态模型结构具有多样性，需要对后续讨论的模型作一定的限制。根据主流分类，多模态模型分为单流(single-stream)和多流(dual-stream)两类，如图1所示。单流模型通常直接将图像编码向量和文本编码向量<strong>直接拼接</strong>，再接入融合编码器(通常是transformer结构)训练。显然，此类结构不适合引入ITC任务进行预训练，故将其剔除。</p><h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2 Approach"></a><em>2 Approach</em></h2><h3 id="2-1-CLIP-Contrastive-Language-Image-Pre-training"><a href="#2-1-CLIP-Contrastive-Language-Image-Pre-training" class="headerlink" title="2.1 CLIP - Contrastive Language-Image Pre-training"></a>2.1 CLIP - Contrastive Language-Image Pre-training</h3><p>我们首先介绍CLIP模型，是因为其是第一个使用对比学习而将zero-shot分类任务做到先进性能的。CLIP模型借用自然语言处理领域中使用自回归方式进行无监督训练的思想，意图用大量的文本监督信号训练视觉模型。</p><p>具体而言，CLIP模型的对比学习正负样本对定义比较简单：对于一个包含N个图像-文本对的batch数据，正样本为每张图像及其对应的文本(N个)，而其他任意的图像-文本组合都作为负样本(N(N-1)个)。训练方法如图2所示。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/2.png" alt="图2. CLIP模型预训练方法(图左)和zero-shot方法(图右)"></p><center>图2. CLIP模型预训练方法(图左)和zero-shot方法(图右)</center><p>从对比学习算法角度，模型通过几下几步进行训练：</p><ol><li>模态内编码：文本通过Text-Encoder编码为文本表示向量，图像通过Image-Encoder编码为图像表示向量。</li><li>模态间融合：即文本表示向量和图像表示向量的点积，计算相似度。</li><li>对比损失计算：这里使用CrossEntropy Loss作为损失。</li></ol><p>从上述简洁的训练步骤，亦可看出在数据规模足够大的前提下，对比学习是非常有潜力的。除了性能上的优异，对比学习给大模型预训练还带来了以下几个优势：</p><ol><li>训练效率的提升。以往工作表明，在ImageNet上训练一个大模型(<code>ResNeXt101-32x48d</code>)需要大量训练资源，像在4亿图文数据的开放视觉识别任务上，效率更是非常重要。与图像生成对应文本的预训练任务相比，对比学习能够提升至少4倍的效率。</li><li>Zero-shot能力的展现。对比学习通过拉近相关的图像和文本编码，从而使得每张图片总能找到最佳匹配的类别标签，并且不会像监督学习那样受到类别标签的限制。</li></ol><p>尽管CLIP模型的训练方法使得其在分类任务、图文检索任务上有出色的表现，但由于模态间的融合有限(仅仅是相似度计算和对比学习)，很难在QA或者生成任务上有比较好的性能。</p><h3 id="2-2-ALBEF-ALign-the-image-and-text-representation-BEfore-Fusing"><a href="#2-2-ALBEF-ALign-the-image-and-text-representation-BEfore-Fusing" class="headerlink" title="2.2 ALBEF - ALign the image and text representation BEfore Fusing"></a>2.2 ALBEF - ALign the image and text representation BEfore Fusing</h3><p>ALBEF模型的研究动机是极其具有价值的，并且给基于对比学习的多模态预训练做了一个有效过渡，成为了一个新的范式。具体而言，该模型从模型和数据2个角度做了改进：</p><ol><li>模型结构层面：指出利用目标检测器进行区域图像特征抽取的办法，由于文本embedding和图像embedding并未对齐而使得模态融合成为挑战；而利用巨大数据集进行对比学习的方法(如CLIP)也因融合不足而无法应用在更多任务上。</li><li>数据层面：web收集的图像-文本对含有大量噪声，直接进行对比学习可能会过拟合噪声样本。</li></ol><p>图3展示了ALBEF模型的结构。和CLIP模型相似，该模型引入12层的<code>ViT-B16</code>结构作为图像编码器，并引入BERT结构作为文本编码器。不同的是，BERT被拆为了前六层的文本编码器和后六层的multimodal融合器，既体现了融合编码器的重要性，也体现了图像编码器更为重要的实践结论。ALBEF的预训练包含3个任务，因为篇幅限制，我们重点讨论ITC任务。</p><p>首先讨论ITC任务的执行时间：align before fusing。这使得单模态编码器提前学习到低维单模态表示，进而实现更容易的模态融合。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/3.png" alt="图3. ALBEF模型架构"></p><center>图3. ALBEF模型架构</center><p>其次讨论ITC任务的正负样本对定义。借鉴MOCO论文的思想，作者将对比学习看作构建动态字典。以image-to-text角度为例，首先构建一个队列queue用以存放text数据(即key)，然后每一个image数据(即query)都进行字典查找：query总与匹配的key相似(正样本对)，而与其他key相异(负样本对)。在实际的ITC任务中，query是用image-Encoder编码的图像向量，而key则是用一个相同或相近的Encoder(被称为Momentum Model)编码的文本向量。最终的对比损失为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{itc}} = \frac{1}{2}\Bbb{E}_{(I,T)\sim \mathcal{D}}\left[H\left(\pmb{y}^{\text{i2t}}(I), \pmb{p}^{\text{i2t}}(I) \right), H\left(\pmb{y}^{\text{t2i}}(T), \pmb{p}^{\text{t2i}}(T) \right) \right]\tag{2}</script><p>最后我们讨论Momentum Distillation。用于预训练的图像-文本对通常是noisy的：正样本对很可能是弱相关的，而负样本对也可能相互匹配。那么简单地使用对比学习的one-hot label(正负2个类别)将会惩罚所有的负样本对，忽略那些更好的描述文本。因此引入soft label是必要的，具体地，利用Momentum Model生成图像-文本相似度(即soft label)作为伪标签，和ITC任务生成的图像-文本相似度计算KL散度，衡量两者的相似性。可以看出，动量蒸馏鼓励模型捕获那些稳定的语义信息表示，并最大化那些具有相似语义的图像和文本的互信息。加入Momentum Distillation(MoD)后，对比损失更新为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{itc}}^{\text{mod}} = (1-\alpha)\mathcal{L}_{\text{itc}} + \frac{\alpha}{2}\Bbb{E}_{(I,T)\sim \mathcal{D}}\left[\text{KL}\left(\pmb{q}^{\text{i2t}}(I) \Vert \pmb{p}^{\text{i2t}}(I) \right), H\left(\pmb{q}^{\text{t2i}}(T)\Vert \pmb{p}^{\text{t2i}}(T) \right) \right]\tag{3}</script><h3 id="2-3-BLIP-–-Bootstrapping-Language-Image-Pre-training"><a href="#2-3-BLIP-–-Bootstrapping-Language-Image-Pre-training" class="headerlink" title="2.3 BLIP – Bootstrapping Language-Image Pre-training"></a>2.3 BLIP – Bootstrapping Language-Image Pre-training</h3><p>BLIP系列模型设计的初衷，是实现统一的视觉语言理解和生成预训练，以弥补现有模型(Encoder-Based &amp; Encoder-Decoder)的不足。</p><p><figure class="half">    <img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/4.png" width="550"></figure></p><center>图4. BLIP模型结构(上)</center><p>对于BLIP模型(图4左)，它在ALBEF的基础上加入了权值共享，并将MLM任务替换为LM任务以加强模型生成的性能。在ITC任务上，其依旧应用了Momentum Model，以保证文本和视觉特征空间的对齐。此外，论文提出的CapFilt方法不仅可以提高数据集的质量，还可以增加数据集数量，大幅增强了预训练模型的性能，成为了多模态数据处理的新范式。</p><p><figure class="half">    <img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/4-2.png" width="400"></figure></p><center>图4. BLIP-2模型结构(下)</center><p>BLIP-2模型(图4右)期望使用图像和文本单模态里的先进的大规模预训练模型来提供高质量的单模态特征，并保证计算效率足够高。于是它们冻结了Image Encoder和LLM，通过两阶段的预训练步骤取得了先进结果：</p><ol><li>使用轻量的模块Q-Former从image Encoder中捕捉包含丰富文本信息的视觉特征；</li><li>使用冻结的LLM进行语言生成任务。</li></ol><p>有趣的是，Q-Former结构并不直接对图像特征和文本特征进行对比学习，而是定义了一组<code>learned query</code>(如图5所示)，在和Image Encoder的编码向量进行cross-attention后，与文本向量进行对比学习。这被认为是在提取与文本信息高度对应的视觉表示。同时为防止信息泄露，文本信息和query经过了<code>Uni-modal self-attention mask</code>。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/5.png" alt="图5. BLIP2模型的Q-Former示意图"></p><center>图5. BLIP2模型的Q-Former示意图</center><p>最后，BLIP2没有延续Momentum Distillation，这是因为冻结的Image Encoder无需反向传播(即Encoder无需变化)，无需动量变化，而且节省了GPU的容量，可以容纳更多的样本。因此in-batch的负样本就已经足够。关于BLIP2中有趣细节因篇幅原因不再展示。</p><h2 id="3-Our-Work"><a href="#3-Our-Work" class="headerlink" title="3 Our Work"></a>3 Our Work</h2><p>基于对比学习的多模态预训练模型因为捕获了丰富的多模态表征，展现出了强大性能，使得Image captioning成为可能。我们尝试进行图像生成古诗，一种特殊的字幕生成任务。该项目(Image2Poem)已在近期开源至：<a href="https://github.com/weiji-Feng/Image2Poem。">https://github.com/weiji-Feng/Image2Poem。</a></p><blockquote><p>如果你希望了解项目细节(可能性不大)，你可以点击上面的github链接，也可以跳转到我的另一篇博客<br><strong><a href="https://weiji-feng.github.io/2023/05/23/Image2Poem/">&lt;暗格&gt;Image-to-Poem</a></strong>.</p></blockquote><h3 id="3-1-Pre-training-Datasets"><a href="#3-1-Pre-training-Datasets" class="headerlink" title="3.1 Pre-training Datasets"></a>3.1 Pre-training Datasets</h3><p>根据现有资源，我们搜集了109727首来自各时期的绝句，并给出每首古诗的关键词。我们将数据保存为.json格式，每个样本的形式如下所示：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>  </span><br><span class="line">    <span class="attr">&quot;dynasty&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Tang&quot;</span><span class="punctuation">,</span>  </span><br><span class="line">    <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="string">&quot;王维&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;清浅白沙滩|绿蒲尚堪把|家住水东西|浣纱明月下&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;白石滩&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;keywords&quot;</span><span class="punctuation">:</span> <span class="string">&quot;清浅 明月 东西 白沙&quot;</span>  </span><br><span class="line"><span class="punctuation">&#125;</span> </span><br></pre></td></tr></table></figure><br>我们筛选了数据集中的关键词，组成了6000余个核心关键词集合。</p><h3 id="3-2-Model-Architecture"><a href="#3-2-Model-Architecture" class="headerlink" title="3.2 Model Architecture"></a>3.2 Model Architecture</h3><p>由于图像-古诗对数据的匮乏，我们的初代版本使用两阶段的生成方式。</p><p>如图6所示，我们通过两个步骤进行推理：</p><ol><li>CLIP编码：我们首先将关键词集合通过text encoder进行编码，保存以供多次使用。其次，将感兴趣的图像通过image encoder编码，获得图像embedding。我们比较图像embedding和所有关键词embedding的相似度，选择top-k个关键词作为古诗生成的prefix。</li><li>Decoder生成古诗：利用BERT tokenizer对prefix进行分词，进而通过预训练的Decoder模型生成古诗。</li></ol><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/6.png" alt></p><center>图6. CLIP + Decoder的Image2Poem结构</center><p>我们预训练了2个版本的生成式模型：<code>GPT2</code>(Decoder型)和<code>T5</code>(Encoder-Decoder型)。我们将keywords作为模型的输入，期望模型生成符合要求的古诗。训练期间，我们加入一定的MLM策略：对诗中出现在关键词中的字，我们采用15%的概率进行mask掩码，期望让模型学会在古诗生成中包含关键词。</p><h3 id="3-3-Model-Improvement"><a href="#3-3-Model-Improvement" class="headerlink" title="3.3 Model Improvement"></a>3.3 Model Improvement</h3><p>要做模型的改进，我们认为核心是收集高质量图像-古诗对数据集。我们准备进行如下两阶段的数据集获取：</p><ol><li>利用预训练的BLIP2/Chinese-CLIP进行图文检索，获取古诗的对应相关图像；</li><li>对于匹配度不高的图像-古诗对数据，我们考虑Stable Diffusion生成的方式。</li></ol><p>拥有数据后，我们使用图7的结构进行端到端的预训练方式。具体而言：我们冻结预训练的CLIP模型参数和GPT2模型参数，只训练transformer-based的Mapping Network。借鉴BLIP2的思想，我们期望Mapping Network可以对齐图像编码空间和GPT2的文本空间。与BLIP2类似，我们设计了一个固定的learned queries，与CLIP图像编码器的输出进行融合(使用concatenate或者cross-attention)，再将输出作为prefix embedding提供给GPT2模型。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/7.png" alt></p><p><center>图7. 改进的Image2Poem结构</center><br>由于还在进行图像-古诗数据的检索，还没能对改进的结构进行测试，但我们相信这个改进是有意义的。</p>]]></content>
      
      
      <categories>
          
          <category> 夏令营经历 </category>
          
          <category> multi-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Large-Language-Model For Math</title>
      <link href="/2023/06/03/Survey4MATH/"/>
      <url>/2023/06/03/Survey4MATH/</url>
      
        <content type="html"><![CDATA[<h1 id="Large-Language-Model-For-Math"><a href="#Large-Language-Model-For-Math" class="headerlink" title="Large-Language-Model For Math"></a>Large-Language-Model For Math</h1><p>让LLM大模型解决数学问题！ — #TODO</p><p>由于最近在做相关方向的科研，将阅读的论文整理在这里。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">💡 阅读记录格式:</span><br><span class="line"></span><br><span class="line">### Pretraining</span><br><span class="line"></span><br><span class="line">1. which foundation models are based on?</span><br><span class="line">2. what tokenizers are adopted?</span><br><span class="line">3. which datasets are collected specific for &quot;math&quot;?</span><br><span class="line">4. what types of pre-processing methods are introduced?</span><br><span class="line">5. other information that  you think is important</span><br><span class="line"></span><br><span class="line">### Fine-tuning</span><br><span class="line"></span><br><span class="line">1. which datasets are used?</span><br><span class="line">2. what types of pre-processing methods are used?</span><br><span class="line"></span><br><span class="line">### Evaluation</span><br><span class="line"></span><br><span class="line">1. which datasets are used?</span><br><span class="line">2. what type of pre-processing methods are used?</span><br><span class="line">3. what evaluation metrics are used?</span><br></pre></td></tr></table></figure><h3 id="阅读的论文列表："><a href="#阅读的论文列表：" class="headerlink" title="阅读的论文列表："></a>阅读的论文列表：</h3><p><a href="https://www.notion.so/Training-Verifiers-to-Solve-Math-Word-Problems-db6822f1cf9b45ad960b1dbb574ab4b8">Training Verifiers to Solve Math Word Problems</a></p><p><a href="https://www.notion.so/Solving-Quantitative-Reasoning-Problems-with-Language-Models-19b339fda58246fbadb22166a78b6ffd">Solving Quantitative Reasoning Problems with Language Models</a></p><p><a href="https://www.notion.so/MathPrompter-Mathematical-Reasoning-using-Large-Language-Models-92db0f041ac94c53884e799948af207d">MathPrompter: Mathematical Reasoning using Large Language Models</a></p><p><a href="https://www.notion.so/PAL-Program-aided-Language-Models-ddaecc337f414b8ea37985999bdec23c">PAL: Program-aided Language Models</a></p><p><a href="https://www.notion.so/Specializing-Smaller-Language-Models-towards-Multi-Step-Reasoning-6e8af05836f243058cc8d2374162c2c6">Specializing Smaller Language Models towards Multi-Step Reasoning</a></p>]]></content>
      
      
      <categories>
          
          <category> llm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image2Poem</title>
      <link href="/2023/05/23/Image2Poem/"/>
      <url>/2023/05/23/Image2Poem/</url>
      
        <content type="html"><![CDATA[<h1 id="Image-to-Poem"><a href="#Image-to-Poem" class="headerlink" title="Image-to-Poem"></a>Image-to-Poem</h1><p>此情此景，何不吟诗一首？Image-to-Poem帮你完成！</p><p>项目链接：<br><a href="https://github.com/weiji-Feng/Image2Poem">https://github.com/weiji-Feng/Image2Poem</a></p><p>9.28之前可能会忙于升学，本项目暂不更新。希望可以在10.31号之前完成这个项目的全部功能。</p><h2 id="1-项目介绍"><a href="#1-项目介绍" class="headerlink" title="1. 项目介绍"></a>1. 项目介绍</h2><p>图像生成古诗(Image to Poem)，旨在为给定的图像自动生成符合图像内容的古诗句。</p><p>使用对比学习预训练的CLIP模型拥有良好的迁移应用和zero-shot能力，是打通图像-文本多模态的重要模型之一。 本项目使用<a href="https://github.com/openai/CLIP">CLIP模型</a>生成古诗意象关键词向量和图像向量。</p><p>初始版本的生成方法为：搜集一个古诗词意象关键词数据集(close-set)，然后通过text-encoder(图1.右) 生成对应的关键词向量。对给定的一张图像，同样通过Image-encoder即可得到图像向量。比较图像向量和每个关键词向量的余弦相似度，可以得到top-k个相关关键词。将关键词送入语言模型，自动生成一首诗。</p><p>这种提取关键词的操作将<strong>会大大损失图像的语义信息</strong>，进而影响语言模型的古诗生成。但由于图像-古诗对数据集非常匮乏，我们很难&lt;/u&gt;像Dalle模型一样&lt;/u&gt;，直接将CLIP模型Image-encoder的输出向量，通过一个MappingNet(在DALLE-2中就是prior模块)送入解码器(语言模型)。所以如果有更好的想法欢迎指点。<br><img src="https://github.com/openai/CLIP/raw/main/CLIP.png" alt="img.png"></p><p>由于古诗的特殊性，本项目重头训练了一个用于生成古诗文的Language Model，尝试了T5 model（223M）和GPT2 model（118M），现公开该预训练模型以供大家娱乐。</p><p>以上模型均可通过调用 <a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a> 的<code>transformers</code>导入。</p><h2 id="2-引用和致谢"><a href="#2-引用和致谢" class="headerlink" title="2. 引用和致谢"></a>2. 引用和致谢</h2><p>在项目完成期间，我参考并使用了以下项目，这里表示感谢！ </p><ul><li>数据集来源：<a href="https://github.com/THUNLP-AIPoet/CCPM">https://github.com/THUNLP-AIPoet/CCPM</a></li><li>CLIP预训练模型来源： <a href="https://github.com/OFA-Sys/Chinese-CLIP">https://github.com/OFA-Sys/Chinese-CLIP</a></li><li>GPT2预训练部分代码：<a href="https://github.com/Morizeyao/GPT2-Chinese">https://github.com/Morizeyao/GPT2-Chinese</a></li></ul><h2 id="3-使用说明和生成样例"><a href="#3-使用说明和生成样例" class="headerlink" title="3. 使用说明和生成样例"></a>3. 使用说明和生成样例</h2><h3 id="安装依赖库"><a href="#安装依赖库" class="headerlink" title="安装依赖库"></a>安装依赖库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch torchvision torchaudio</span><br><span class="line">pip install transformers</span><br><span class="line">pip install tqdm matplotlib</span><br></pre></td></tr></table></figure><p>如果希望尝试预训练语言模型, 建议安装<code>torch+cudaxx.x</code>的GPU版本。</p><h3 id="快速体验古诗生成"><a href="#快速体验古诗生成" class="headerlink" title="快速体验古诗生成"></a>快速体验古诗生成</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python img2poem.py --image_path ./datasets/images/feiliu.jpg --model_type T5 --model_path ./config/t5_config</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>--image_path</code>: 图片所在位置</li><li><code>--model_type</code>: 模型名称,目前可选用’T5’,’GPT2’</li><li><code>--model_path</code>: 模型所在文件夹</li></ul><h3 id="生成样例"><a href="#生成样例" class="headerlink" title="生成样例"></a>生成样例</h3><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/feiliu.jpg" alt></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">飞鹤度湖山，青松半掩关。水林昼景凤，诸君自有闲。</span><br><span class="line">白苹洲渚流，丹青未有人。水林壑夜深，乱峰高几重。 </span><br><span class="line">仙人问道踪，壑深自坐禅。丹青一片云，月随风水林。   </span><br><span class="line">不见青林路，却忆庐陵西。老松犹未分，钟山水似难。</span><br><span class="line">飞鹤度湖山，青松半掩关。丹壑千人在，犹记林水心。</span><br></pre></td></tr></table></figure><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/chun.jpg" alt="image-20230408233621777" style="zoom:30%;"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">青碧绕瑶池，碧峰回九关。晚风吹画船，映水长成芳。</span><br><span class="line">犹自在藏新，不知是旧人。暮天三百年，桃源一片春。</span><br><span class="line">数年三两枝，却羡玉龙飞。桃源水一津，斜晖又一年。</span><br><span class="line">千骑鹤归飞，一曲茅亭去。天上桃源路，玉龙归白杳。</span><br><span class="line">相逢一笑飞，知有桃源人。何如写玉龙，斜晖送晚风</span><br></pre></td></tr></table></figure><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/pubu.jpg" alt="image-20230408233621777" style="zoom:90%;"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">清江起玉龙，时听瀑布声。飞曲满游子，一生天上流。</span><br><span class="line">萧然见山来，却恐归来晚。时见布泉飞，锦囊深掩门。</span><br><span class="line">时有一花村，俗人如此山。月溪风乱鸣，时有瀑布还。</span><br><span class="line">月明闲送风，瀑布飞仙雪。人间几度林，锦衣还说天。 </span><br><span class="line">谁人识此中，布泉老客行。月满江来路，不须频为谁。</span><br></pre></td></tr></table></figure><h2 id="4-一些解释"><a href="#4-一些解释" class="headerlink" title="4. 一些解释"></a>4. 一些解释</h2><ul><li>对于当前项目的评价<blockquote><p>提取关键词进行古诗生成是一个<strong>损失信息</strong>的过程，尤其是将图像映射到关键词的操作，损失了图像原本的语义(例如只能识别人，而不知道人在做什么)。所以效果上来看仍然差强人意。</p><p>没有给模型一些关于韵律、题材、体裁等的设定，导致不够专业。</p></blockquote></li><li><p>可不可以使用自己的古诗数据集尝试预训练？</p><blockquote><p>可以，不过由于CCPM数据集是<code>.json</code>文件格式,导入方式与<code>.txt</code>不同。所以在<code>datasets.py</code>文件里你需要重新写一下有关文件导入的部分。并且由于预训练方法多样，你也可以修改预训练时的一些策略。</p></blockquote></li><li><p>项目的预训练方法是什么？</p><blockquote><p>首先对于GPT2模型，常规预训练方法就是自回归，本项目尝试了mask关键词的方法，例如：</p><p><code>[CLS]关键词：明月 故乡 [EOS] 举头望明月，低头思故乡[SEP]</code> =&gt; <code>[CLS]关键词：明月 故乡 [EOS] 举头望[MASK][MASK]，低头思[MASK][MASK][SEP]</code></p><p>然后我额外对这些mask token的预测准确率进行了计算，加入了损失函数中。</p><p>对于T5模型，由于是encoder-decoder架构，我使用下列格式创建数据：</p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS][SEP]</code>, y = <code>[CLS]红豆生南国[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS][SEP]</code>, y = <code>[CLS]秋来发故枝[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS]秋来发故枝[EOS][SEP]</code>, y = <code>[CLS]愿君多采撷[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS]秋来发故枝[EOS]愿君多采撷[EOS][SEP]</code>, y = <code>[CLS]此物最相思[EOS][SEP]</code></p></blockquote></li><li><p>通过什么方式进行图像生成古诗？未来有什么进一步更新的方法？</p><blockquote><p>现在的实现比较简单，就是先搜集一个闭环的关键词数据集(<code>keyword.txt</code>)，然后使用CLIP对图像和所有关键词进行编码，计算它们之间的相似度，取相似度最高的K个关键词，然后放置于语言模型进行生成。</p><p>由于<code>图像-古诗对</code>数据集非常匮乏，似乎暂时做不到删去这个闭环关键词数据集。未来如果有充足的数据集，我会使用<code>CLIP-MappingNet-T5/GPT2</code>的模型架构进行训练，例如下图的<a href="https://arxiv.org/pdf/2111.09734.pdf">CLIPCap</a>架构：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*8RLzDpMfi6sLScqx2SguaA.png" alt></p></blockquote></li></ul><p>未来有古诗生成图像的想法，待进一步更新。现有的可以进行古诗生成图像的项目有：<a href="https://huggingface.co/IDEA-CCNL/Taiyi-Diffusion-532M-Nature-Chinese">https://huggingface.co/IDEA-CCNL/Taiyi-Diffusion-532M-Nature-Chinese</a></p>]]></content>
      
      
      <categories>
          
          <category> multi-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
