<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</title>
      <link href="/2023/06/09/unimatch/"/>
      <url>/2023/06/09/unimatch/</url>
      
        <content type="html"><![CDATA[<h1 id="论文复现-Revisiting-Weak-to-Strong-Consistency-in-Semi-Supervised-Semantic-Segmentation"><a href="#论文复现-Revisiting-Weak-to-Strong-Consistency-in-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="论文复现 - Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation"></a>论文复现 - Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</h1><blockquote><p>前记：这篇文章属于是用基本模型<u>通过一步步的探讨得出一个结合多个思路的终极模型</u>的方法，思路值得学习。</p><p>下面，我们将通过论文介绍和实验复现两部分详细展示论文复现工作。</p></blockquote><h2 id="实验结果复现"><a href="#实验结果复现" class="headerlink" title="实验结果复现"></a>实验结果复现</h2><p>该论文的代码开源至 <a href="https://github.com/LiheYoung/UniMatch">https://github.com/LiheYoung/UniMatch</a> .</p><h3 id="1-下载代码、模型和数据"><a href="#1-下载代码、模型和数据" class="headerlink" title="1. 下载代码、模型和数据"></a>1. 下载代码、模型和数据</h3><h4 id="1-1-代码下载"><a href="#1-1-代码下载" class="headerlink" title="1.1 代码下载"></a>1.1 代码下载</h4><p>关于代码的<code>Installation</code>，直接按照默认方法：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd UniMatch</span><br><span class="line">conda create -n unimatch python=3.10.4</span><br><span class="line">conda activate unimatch</span><br><span class="line">pip install -r requirements.txt # 别急,请先按照下面的第一条修改文件;</span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure><br>值得强调的是，代码其实包含一些Bug，需要简单处理一下：</p><ol><li>在<code>requirements.txt</code>中，需要将<code>sklearn</code>改成<code>scikit-learn</code>，保证pip install 顺利进行；</li><li>在<code>unimatch.py</code>中，切记将下面这行代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--local_rank&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure> 改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--local-rank&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure> 不然你的local-rank参数不被识别；</li><li>如果你是单机单卡或者单机多卡(例如我)，可以将<code>train.sh</code>配置为 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1,2,3</span><br><span class="line"></span><br><span class="line">python -m torch.distributed.launch \</span><br><span class="line">    --nnodes 1 \</span><br><span class="line">    --nproc_per_node=$1 \</span><br><span class="line">    $method.py \</span><br><span class="line">    --config=$config --labeled-id-path $labeled_id_path --unlabeled-id-path $unlabeled_id_path \</span><br><span class="line">    --save-path $save_path 2&gt;&amp;1 | tee $save_path/$now.log</span><br></pre></td></tr></table></figure> 无需设置port等参数。</li></ol><h4 id="1-2-预训练模型下载"><a href="#1-2-预训练模型下载" class="headerlink" title="1.2 预训练模型下载"></a>1.2 预训练模型下载</h4><p>预训练的模型在原仓库中有3种：<code>ResNet50</code>/<code>ResNet101</code>/<code>xception</code>，在复现时默认使用resnet101，如果时间允许，我们将尝试其他模型的复现。</p><h4 id="1-3-数据集下载"><a href="#1-3-数据集下载" class="headerlink" title="1.3 数据集下载"></a>1.3 数据集下载</h4><p>数据集由于时间和资源有限，仅仅复现关于<code>Pascal VOC 2012</code>数据集的一些结果。</p><ul><li>Pascal: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar">JPEGImages</a> | <a href="https://drive.google.com/file/d/1ikrDlsai5QSf2GiSUR3f8PZUzyTubcuF/view?usp=sharing">SegmentationClass</a></li></ul><p>其他数据集见原仓库。</p><h3 id="2-训练的实现"><a href="#2-训练的实现" class="headerlink" title="2. 训练的实现"></a>2. 训练的实现</h3><p>我们准备好了数据，可以按照下面的算法图完成模型训练：<br><img src="/2023/06/09/unimatch/1.png" alt></p><center><a id="fig1">图1. 算法示意图</a></center><h4 id="2-1-数据增广"><a href="#2-1-数据增广" class="headerlink" title="2.1 数据增广"></a>2.1 数据增广</h4><blockquote><p>这里说的数据增广，其实是指<code>strong view</code>的强扰动和<code>weak view</code>的弱扰动。</p><p>相关文件：<strong>./dataset/semi.py</strong></p></blockquote><p>在<a href="#fig1">图1</a>中指代下面这几行代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one weak view and two strong views as input</span></span><br><span class="line">x_w = aug_w(x)</span><br><span class="line">x_s1, x_s2 = aug_s(x_w), aug_s(x_w)</span><br></pre></td></tr></table></figure></p><p>源代码在实现这几行时，首先让每一张图像完成<strong>弱扰动</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img, mask = resize(img, mask, (<span class="number">0.5</span>, <span class="number">2.0</span>))</span><br><span class="line">ignore_value = <span class="number">254</span> <span class="keyword">if</span> self.mode == <span class="string">&#x27;train_u&#x27;</span> <span class="keyword">else</span> <span class="number">255</span></span><br><span class="line">img, mask = crop(img, mask, self.size, ignore_value)</span><br><span class="line">img, mask = hflip(img, mask, p=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>其中<code>img</code>是RGB图像，<code>mask</code>是分割的掩码。现在，这个经过弱扰动的图像<code>img</code>就是$x^w$，同时，我们将其复制两份，得到$x^{s_1}$和$x^{s_2}$。当然$x^{s_1}$和$x^{s_2}$还需要经过强扰动，成为2个<code>strong view</code>，实现<code>Dual-Stream Perturbations</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_w, img_s1, img_s2 = deepcopy(img), deepcopy(img), deepcopy(img)</span><br></pre></td></tr></table></figure></p><p>进行强扰动的代码(<strong>以处理$x^{s_1}$为例</strong>)如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">    <span class="comment"># 随机调整亮度、对比度、饱和度和色调</span></span><br><span class="line">    img_s1 = transforms.ColorJitter(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.25</span>)(img_s1)</span><br><span class="line">img_s1 = transforms.RandomGrayscale(p=<span class="number">0.2</span>)(img_s1)  <span class="comment"># 随机灰度化</span></span><br><span class="line">img_s1 = blur(img_s1, p=<span class="number">0.5</span>)    <span class="comment"># 随机模糊</span></span><br><span class="line">cutmix_box1 = obtain_cutmix_box(img_s1.size[<span class="number">0</span>], p=<span class="number">0.5</span>)  <span class="comment"># 随机获取CutMix的区域</span></span><br></pre></td></tr></table></figure></p><p>因为这些数据增强方法设置了概率，故不同的epoch或者是$x^{s_1}$和$x^{s_2}$之间，增强的效果都是不同的。其中我对于<code>CutMix</code>操作还比较好奇，去查看了函数定义。发现CutMix就是mask掉一块区域(该区域的宽高和位置都是一定程度随机的)，然后用其他图片中<strong>相同位置的区域</strong>来<a href="#padding">填充</a>。</p><p>由于<code>Pascal</code>数据集的标注图像<code>mask</code>中包含254这个无效像素值，没有对应类别，作者使用<code>ignore_mask</code>忽略它：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ignore_mask = Image.fromarray(np.zeros((mask.size[<span class="number">1</span>], mask.size[<span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">img_s1, ignore_mask = normalize(img_s1, ignore_mask)</span><br><span class="line">img_s2 = normalize(img_s2)</span><br><span class="line"></span><br><span class="line">mask = torch.from_numpy(np.array(mask)).long()</span><br><span class="line">ignore_mask[mask == <span class="number">254</span>] = <span class="number">255</span></span><br></pre></td></tr></table></figure><br>取值为255是因为<code>crop</code>操作对哪些裁剪时遇到的padding都设置值为255，同样也是无效区域，这里相当于合并了。于是，经过图像增广等操作后，我们的输入数据可能就包含以下几个部分：</p><ul><li>$x^w$: 即<code>img_w</code>，在return时还需要normalize一下；</li><li>$x^{s_1}$: 即<code>img_s1</code>，经过强扰动，且已经normalize；</li><li>$x^{s_2}$: 即<code>img_s2</code>，经过强扰动，且已经normalize；</li><li>ignore_mask: 用于忽略无效的像素；</li><li>cutmix_box1: 从$x^{s_1}$获取的mask掉的CutMix区域；</li><li>cutmix_box2: 从$x^{s_2}$获取的mask掉的CutMix区域；</li></ul><p>了解增广的细节后，我们可以构建3个数据集，分别是有标签监督数据、无标签数据、和验证数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">trainset_u = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;train_u&#x27;</span>,</span><br><span class="line">                         cfg[<span class="string">&#x27;crop_size&#x27;</span>], args.unlabeled_id_path)</span><br><span class="line">trainset_l = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;train_l&#x27;</span>,</span><br><span class="line">                         cfg[<span class="string">&#x27;crop_size&#x27;</span>], args.labeled_id_path, </span><br><span class="line">                         nsample=<span class="built_in">len</span>(trainset_u.ids))</span><br><span class="line">valset = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;val&#x27;</span>)</span><br></pre></td></tr></table></figure><br>将它们分别转为Dataloader后，通过下面的代码进行分批训练：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loader = <span class="built_in">zip</span>(trainloader_l, trainloader_u, trainloader_u)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, ((img_x, mask_x),</span><br><span class="line">        (img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2),</span><br><span class="line">        (img_u_w_mix, img_u_s1_mix, img_u_s2_mix, ignore_mask_mix, _, _)) </span><br><span class="line">        <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br></pre></td></tr></table></figure><br><u><strong>接下来的分析，都在<a id="canshu">上述循环</a>中，请关注从<code>loader</code>中取出的这些数据！</strong></u></p><p><a id="padding">将CutMix完成 </a></p><p>最后一步，将<code>cutmix</code>操作完成，具体来说，我们用第二个<code>trainloader_u</code>中获取的数据来填充我们的$s_1$和$s_2$：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img_u_s1[cutmix_box1.unsqueeze(<span class="number">1</span>).expand(img_u_s1.shape) == <span class="number">1</span>] = \</span><br><span class="line">    img_u_s1_mix[cutmix_box1.unsqueeze(<span class="number">1</span>).expand(img_u_s1.shape) == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">img_u_s2[cutmix_box2.unsqueeze(<span class="number">1</span>).expand(img_u_s2.shape) == <span class="number">1</span>] = \</span><br><span class="line">    img_u_s2_mix[cutmix_box2.unsqueeze(<span class="number">1</span>).expand(img_u_s2.shape) == <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><h4 id="2-2-Encoder-Decoder生成预测"><a href="#2-2-Encoder-Decoder生成预测" class="headerlink" title=" 2.2 Encoder-Decoder生成预测 "></a><a id="ED"> 2.2 Encoder-Decoder生成预测 </a></h4><p>在图一中，这部分表示为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature of weakly perturbed image</span></span><br><span class="line">feat_w = g(x_w)</span><br><span class="line"><span class="comment"># perturbed feature</span></span><br><span class="line">feat_fp = nn.Dropout2d(<span class="number">0.5</span>)(feat_w)</span><br><span class="line"><span class="comment"># four predictions from four forward streams</span></span><br><span class="line">p_w, p_fp = h(torch.cat((feat_w, feat_fp))).chunk(<span class="number">2</span>)</span><br><span class="line">p_s1, p_s2 = f(torch.cat((x_s1, x_s2))).chunk(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>在<code>unimatch.py</code>中，并没有展现出将$f$拆分为$h(g(x))$的细节，而是直接通过model生成预测，所以<code>dropout2d</code>应该包含在model里了。我们截取了<a id="pred_x">下面代码</a>，作为上述部分的实现，并提供解释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_lb, num_ulb = img_x.shape[<span class="number">0</span>], img_u_w.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># img_x是带标签的监督图像数据，通过计算pred_x可以进行有监督训练.</span></span><br><span class="line">preds, preds_fp = model(torch.cat((img_x, img_u_w)), <span class="literal">True</span>)  <span class="comment"># need_fp=True,进行dropout</span></span><br><span class="line">pred_x, pred_u_w = preds.split([num_lb, num_ulb])   <span class="comment"># pred_u_w =&gt; p_w</span></span><br><span class="line">pred_u_w_fp = preds_fp[num_lb:] <span class="comment"># pred_u_w_fp =&gt; p_fp,进行特征层面的自监督训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pred_u_s1 =&gt; p_s1, pred_u_s2 =&gt; p_s2</span></span><br><span class="line">pred_u_s1, pred_u_s2 = model(torch.cat((img_u_s1, img_u_s2))).chunk(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><h4 id="2-3-Loss计算"><a href="#2-3-Loss计算" class="headerlink" title="2.3 Loss计算"></a>2.3 Loss计算</h4><p>在<a href="#fig1">图1</a>中指代一下部分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hard (one-hot) pseudo mask</span></span><br><span class="line">mask_w = p_w.argmax(dim=<span class="number">1</span>).detach()</span><br><span class="line"><span class="comment"># loss from image- and feature-level perturbation</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">p_s = torch.cat((p_s1, p_s2))</span><br><span class="line">loss_s = criterion(p_s, mask_w.repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">loss_fp = criterion(p_fp, mask_w)</span><br><span class="line"><span class="comment"># final unsupervised loss</span></span><br><span class="line">loss_u = (loss_s + loss_fp) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure></p><p>由于$x^{s_1}$和$x^{s_2}$进行过<code>CutMix</code>，而$x^{w}$并没有做这些强扰动，所以想得到无标签自监督的标签<code>mask_w</code>很复杂，因此损失的计算并不简单。我们基于现有<a href="#canshu">参数</a>逐步分析：</p><p>首先，对于用来填充cutmix的数据<code>img_u_w_mix</code>，我们利用模型预测其分割结果<code>mask_u_w_mix</code>；同时，$x^w$也通过模型获得了<code>pred_u_w</code>(见<a href="#ED">2.2</a>)，我们同样可以获得<code>mask_u_w</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    pred_u_w_mix = model(img_u_w_mix).detach()</span><br><span class="line">    conf_u_w_mix = pred_u_w_mix.softmax(dim=<span class="number">1</span>).<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    mask_u_w_mix = pred_u_w_mix.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">pred_u_w = pred_u_w.detach()</span><br><span class="line">conf_u_w = pred_u_w.softmax(dim=<span class="number">1</span>).<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">mask_u_w = pred_u_w.argmax(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>由于我们知道了$s_1$和$s_2$<code>CutMix</code>框的位置，所以我们直接将上面的两个图像的mask结合，就可以得到自监督label：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于cutmix框不一样，这里分别获得cutmixed1和cutmixed2的mask及其conf等</span></span><br><span class="line">mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = \</span><br><span class="line">    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()</span><br><span class="line">mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = \</span><br><span class="line">    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()</span><br><span class="line"></span><br><span class="line">mask_u_w_cutmixed1[cutmix_box1 == <span class="number">1</span>] = mask_u_w_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line">conf_u_w_cutmixed1[cutmix_box1 == <span class="number">1</span>] = conf_u_w_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line">ignore_mask_cutmixed1[cutmix_box1 == <span class="number">1</span>] = ignore_mask_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">mask_u_w_cutmixed2[cutmix_box2 == <span class="number">1</span>] = mask_u_w_mix[cutmix_box2 == <span class="number">1</span>]</span><br><span class="line">conf_u_w_cutmixed2[cutmix_box2 == <span class="number">1</span>] = conf_u_w_mix[cutmix_box2 == <span class="number">1</span>]</span><br><span class="line">ignore_mask_cutmixed2[cutmix_box2 == <span class="number">1</span>] = ignore_mask_mix[cutmix_box2 == <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><p>最后，<strong>我们给出4个损失</strong>：</p><p>第一个损失：有监督损失。使用img_x的<a href="#pred_x">预测结果</a><code>pred_x</code>和标签<code>mask_x</code>计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_x = criterion_l(pred_x, mask_x)</span><br></pre></td></tr></table></figure></p><p>第二&amp;三个损失：图像层面自监督损失。通过$s_1$和$s_2$的预测结果及其对应label计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)</span><br><span class="line">loss_u_s1 = loss_u_s1 * (</span><br><span class="line">    (conf_u_w_cutmixed1 &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask_cutmixed1 != <span class="number">255</span>))</span><br><span class="line">loss_u_s1 = loss_u_s1.<span class="built_in">sum</span>() / (ignore_mask_cutmixed1 != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)</span><br><span class="line">loss_u_s2 = loss_u_s2 * (</span><br><span class="line">    (conf_u_w_cutmixed2 &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask_cutmixed2 != <span class="number">255</span>))</span><br><span class="line">loss_u_s2 = loss_u_s2.<span class="built_in">sum</span>() / (ignore_mask_cutmixed2 != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br></pre></td></tr></table></figure></p><p>第四个损失：特征层面的自监督损失。通过$x^{fp}$的<a href="#pred_x">预测结果</a><code>pred_u_w_fp</code>和$x^w$的预测结果计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss_u_w_fp = criterion_u(pred_u_w_fp, mask_u_w)</span><br><span class="line">loss_u_w_fp = loss_u_w_fp * ((conf_u_w &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask != <span class="number">255</span>))</span><br><span class="line">loss_u_w_fp = loss_u_w_fp.<span class="built_in">sum</span>() / (ignore_mask != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br></pre></td></tr></table></figure></p><p><strong>Total</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = (loss_x + loss_u_s1 * <span class="number">0.25</span> + loss_u_s2 * <span class="number">0.25</span> + loss_u_w_fp * <span class="number">0.5</span>) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 夏令营经历 </category>
          
          <category> Semantic segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> papers reproduced </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Contrastive Learning based Vision-Language Pre-Training</title>
      <link href="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/"/>
      <url>/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/</url>
      
        <content type="html"><![CDATA[<center><h1>基于对比学习的多模态预训练方法</h1></center><center><h2>——以Vision-Language PTMs为例<h2></h2></h2></center><blockquote><p>前记：这篇文章是我在面试中科大毛震东老师组时写的一份报告，整理一下2023年之前基于对比学习的多模态预训练模型文献。算是给我的<code>Image-to-Poem</code>项目做的一个综述性调查，给自己一个方向。</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">随着大量图像-文本对数据集的涌现，使用对比学习进行多模态模型预训练的方法也愈发成熟。</span><br><span class="line">在本文中，我们首先介绍了图像-文本对比学习任务(ITC)，</span><br><span class="line">接着按照时间线回顾了CLIP、ALBEF、BLIP&amp;BLIP2三个重要的多模态预训练模型。</span><br><span class="line">最后，鉴于多模态预训练模型强大的性能，我们尝试了Image-to-Poem的特殊字幕生成任务。</span><br></pre></td></tr></table></figure><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a><em>1. Introduction</em></h2><p>最近的工作证明，使用互联网中的大量图像-文本对进行多模态模型预训练可以显著提升模型的表征提取能力和泛化性能，但也对图像表征(Representation)和文本表征的融合提出了挑战。如何将图像表征与文本表征进行匹配是重要的问题。</p><p>基本地，这里将多模态模型预训练过程分为3个阶段：</p><ol><li>将图像和文本分别编码为保留语义的表示向量(latent representations)；</li><li>设计一个模型或结构，来模拟两种模式的交融；</li><li>设计有效的预训练任务来预训练Vision-Language Pre-Trained Model.</li></ol><p>图像-文本对比学习(ITC)作为一种有效的预训练任务，欲将成对的图像-文本对的表示向量尽可能地拉近，而将不成对的负例样本对(negatives)的表示向量尽可能地远离。一般地，对比学习的损失函数(image-to-text为例)可表示为如下形式：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{i2t}} = -\Bbb{E}_{(W,V)\in \mathcal{D}}\left[\log\frac{s_\theta (\pmb{h}_v, \pmb{h}_w)}{\sum_{W'} s_\theta (\pmb{h}_v, \pmb{h}_{w'})} \right] \tag{1}</script><p>其中，$(W, V)$是图像-文本对数据集的一个样本，是关于$V$的负样本，$\textit{\pmb{h}}$是图像或文本的表征向量。在后续对多模态的各个预训练模型的调研中，可以发现上述对比学习思想和损失函数很少被改变；但是为了提升ITC任务的有效性，很多方法被引入，例如：正负样本对的定义、ITC任务的出现时机(如align before fusing)、Momentum Distillation等，这将在后续章节详细讨论。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/1.png" alt="图1. 多模态模型结构分类"></p><center>图1. 多模态模型结构分类</center><p>考虑到多模态模型结构具有多样性，需要对后续讨论的模型作一定的限制。根据主流分类，多模态模型分为单流(single-stream)和多流(dual-stream)两类，如图1所示。单流模型通常直接将图像编码向量和文本编码向量<strong>直接拼接</strong>，再接入融合编码器(通常是transformer结构)训练。显然，此类结构不适合引入ITC任务进行预训练，故将其剔除。</p><h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2 Approach"></a><em>2 Approach</em></h2><h3 id="2-1-CLIP-Contrastive-Language-Image-Pre-training"><a href="#2-1-CLIP-Contrastive-Language-Image-Pre-training" class="headerlink" title="2.1 CLIP - Contrastive Language-Image Pre-training"></a>2.1 CLIP - Contrastive Language-Image Pre-training</h3><p>我们首先介绍CLIP模型，是因为其是第一个使用对比学习而将zero-shot分类任务做到先进性能的。CLIP模型借用自然语言处理领域中使用自回归方式进行无监督训练的思想，意图用大量的文本监督信号训练视觉模型。</p><p>具体而言，CLIP模型的对比学习正负样本对定义比较简单：对于一个包含N个图像-文本对的batch数据，正样本为每张图像及其对应的文本(N个)，而其他任意的图像-文本组合都作为负样本(N(N-1)个)。训练方法如图2所示。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/2.png" alt="图2. CLIP模型预训练方法(图左)和zero-shot方法(图右)"></p><center>图2. CLIP模型预训练方法(图左)和zero-shot方法(图右)</center><p>从对比学习算法角度，模型通过几下几步进行训练：</p><ol><li>模态内编码：文本通过Text-Encoder编码为文本表示向量，图像通过Image-Encoder编码为图像表示向量。</li><li>模态间融合：即文本表示向量和图像表示向量的点积，计算相似度。</li><li>对比损失计算：这里使用CrossEntropy Loss作为损失。</li></ol><p>从上述简洁的训练步骤，亦可看出在数据规模足够大的前提下，对比学习是非常有潜力的。除了性能上的优异，对比学习给大模型预训练还带来了以下几个优势：</p><ol><li>训练效率的提升。以往工作表明，在ImageNet上训练一个大模型(<code>ResNeXt101-32x48d</code>)需要大量训练资源，像在4亿图文数据的开放视觉识别任务上，效率更是非常重要。与图像生成对应文本的预训练任务相比，对比学习能够提升至少4倍的效率。</li><li>Zero-shot能力的展现。对比学习通过拉近相关的图像和文本编码，从而使得每张图片总能找到最佳匹配的类别标签，并且不会像监督学习那样受到类别标签的限制。</li></ol><p>尽管CLIP模型的训练方法使得其在分类任务、图文检索任务上有出色的表现，但由于模态间的融合有限(仅仅是相似度计算和对比学习)，很难在QA或者生成任务上有比较好的性能。</p><h3 id="2-2-ALBEF-ALign-the-image-and-text-representation-BEfore-Fusing"><a href="#2-2-ALBEF-ALign-the-image-and-text-representation-BEfore-Fusing" class="headerlink" title="2.2 ALBEF - ALign the image and text representation BEfore Fusing"></a>2.2 ALBEF - ALign the image and text representation BEfore Fusing</h3><p>ALBEF模型的研究动机是极其具有价值的，并且给基于对比学习的多模态预训练做了一个有效过渡，成为了一个新的范式。具体而言，该模型从模型和数据2个角度做了改进：</p><ol><li>模型结构层面：指出利用目标检测器进行区域图像特征抽取的办法，由于文本embedding和图像embedding并未对齐而使得模态融合成为挑战；而利用巨大数据集进行对比学习的方法(如CLIP)也因融合不足而无法应用在更多任务上。</li><li>数据层面：web收集的图像-文本对含有大量噪声，直接进行对比学习可能会过拟合噪声样本。</li></ol><p>图3展示了ALBEF模型的结构。和CLIP模型相似，该模型引入12层的<code>ViT-B16</code>结构作为图像编码器，并引入BERT结构作为文本编码器。不同的是，BERT被拆为了前六层的文本编码器和后六层的multimodal融合器，既体现了融合编码器的重要性，也体现了图像编码器更为重要的实践结论。ALBEF的预训练包含3个任务，因为篇幅限制，我们重点讨论ITC任务。</p><p>首先讨论ITC任务的执行时间：align before fusing。这使得单模态编码器提前学习到低维单模态表示，进而实现更容易的模态融合。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/3.png" alt="图3. ALBEF模型架构"></p><center>图3. ALBEF模型架构</center><p>其次讨论ITC任务的正负样本对定义。借鉴MOCO论文的思想，作者将对比学习看作构建动态字典。以image-to-text角度为例，首先构建一个队列queue用以存放text数据(即key)，然后每一个image数据(即query)都进行字典查找：query总与匹配的key相似(正样本对)，而与其他key相异(负样本对)。在实际的ITC任务中，query是用image-Encoder编码的图像向量，而key则是用一个相同或相近的Encoder(被称为Momentum Model)编码的文本向量。最终的对比损失为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{itc}} = \frac{1}{2}\Bbb{E}_{(I,T)\sim \mathcal{D}}\left[H\left(\pmb{y}^{\text{i2t}}(I), \pmb{p}^{\text{i2t}}(I) \right), H\left(\pmb{y}^{\text{t2i}}(T), \pmb{p}^{\text{t2i}}(T) \right) \right]\tag{2}</script><p>最后我们讨论Momentum Distillation。用于预训练的图像-文本对通常是noisy的：正样本对很可能是弱相关的，而负样本对也可能相互匹配。那么简单地使用对比学习的one-hot label(正负2个类别)将会惩罚所有的负样本对，忽略那些更好的描述文本。因此引入soft label是必要的，具体地，利用Momentum Model生成图像-文本相似度(即soft label)作为伪标签，和ITC任务生成的图像-文本相似度计算KL散度，衡量两者的相似性。可以看出，动量蒸馏鼓励模型捕获那些稳定的语义信息表示，并最大化那些具有相似语义的图像和文本的互信息。加入Momentum Distillation(MoD)后，对比损失更新为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{itc}}^{\text{mod}} = (1-\alpha)\mathcal{L}_{\text{itc}} + \frac{\alpha}{2}\Bbb{E}_{(I,T)\sim \mathcal{D}}\left[\text{KL}\left(\pmb{q}^{\text{i2t}}(I) \Vert \pmb{p}^{\text{i2t}}(I) \right), H\left(\pmb{q}^{\text{t2i}}(T)\Vert \pmb{p}^{\text{t2i}}(T) \right) \right]\tag{3}</script><h3 id="2-3-BLIP-–-Bootstrapping-Language-Image-Pre-training"><a href="#2-3-BLIP-–-Bootstrapping-Language-Image-Pre-training" class="headerlink" title="2.3 BLIP – Bootstrapping Language-Image Pre-training"></a>2.3 BLIP – Bootstrapping Language-Image Pre-training</h3><p>BLIP系列模型设计的初衷，是实现统一的视觉语言理解和生成预训练，以弥补现有模型(Encoder-Based &amp; Encoder-Decoder)的不足。</p><p><figure class="half">    <img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/4.png" width="550"></figure></p><center>图4. BLIP模型结构(上)</center><p>对于BLIP模型(图4左)，它在ALBEF的基础上加入了权值共享，并将MLM任务替换为LM任务以加强模型生成的性能。在ITC任务上，其依旧应用了Momentum Model，以保证文本和视觉特征空间的对齐。此外，论文提出的CapFilt方法不仅可以提高数据集的质量，还可以增加数据集数量，大幅增强了预训练模型的性能，成为了多模态数据处理的新范式。</p><p><figure class="half">    <img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/4-2.png" width="400"></figure></p><center>图4. BLIP-2模型结构(下)</center><p>BLIP-2模型(图4右)期望使用图像和文本单模态里的先进的大规模预训练模型来提供高质量的单模态特征，并保证计算效率足够高。于是它们冻结了Image Encoder和LLM，通过两阶段的预训练步骤取得了先进结果：</p><ol><li>使用轻量的模块Q-Former从image Encoder中捕捉包含丰富文本信息的视觉特征；</li><li>使用冻结的LLM进行语言生成任务。</li></ol><p>有趣的是，Q-Former结构并不直接对图像特征和文本特征进行对比学习，而是定义了一组<code>learned query</code>(如图5所示)，在和Image Encoder的编码向量进行cross-attention后，与文本向量进行对比学习。这被认为是在提取与文本信息高度对应的视觉表示。同时为防止信息泄露，文本信息和query经过了<code>Uni-modal self-attention mask</code>。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/5.png" alt="图5. BLIP2模型的Q-Former示意图"></p><center>图5. BLIP2模型的Q-Former示意图</center><p>最后，BLIP2没有延续Momentum Distillation，这是因为冻结的Image Encoder无需反向传播(即Encoder无需变化)，无需动量变化，而且节省了GPU的容量，可以容纳更多的样本。因此in-batch的负样本就已经足够。关于BLIP2中有趣细节因篇幅原因不再展示。</p><h2 id="3-Our-Work"><a href="#3-Our-Work" class="headerlink" title="3 Our Work"></a>3 Our Work</h2><p>基于对比学习的多模态预训练模型因为捕获了丰富的多模态表征，展现出了强大性能，使得Image captioning成为可能。我们尝试进行图像生成古诗，一种特殊的字幕生成任务。该项目(Image2Poem)已在近期开源至：<a href="https://github.com/weiji-Feng/Image2Poem。">https://github.com/weiji-Feng/Image2Poem。</a></p><blockquote><p>如果你希望了解项目细节(可能性不大)，你可以点击上面的github链接，也可以跳转到我的另一篇博客<br><strong><a href="https://weiji-feng.github.io/2023/05/23/Image2Poem/">&lt;暗格&gt;Image-to-Poem</a></strong>.</p></blockquote><h3 id="3-1-Pre-training-Datasets"><a href="#3-1-Pre-training-Datasets" class="headerlink" title="3.1 Pre-training Datasets"></a>3.1 Pre-training Datasets</h3><p>根据现有资源，我们搜集了109727首来自各时期的绝句，并给出每首古诗的关键词。我们将数据保存为.json格式，每个样本的形式如下所示：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>  </span><br><span class="line">    <span class="attr">&quot;dynasty&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Tang&quot;</span><span class="punctuation">,</span>  </span><br><span class="line">    <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="string">&quot;王维&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;清浅白沙滩|绿蒲尚堪把|家住水东西|浣纱明月下&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;白石滩&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;keywords&quot;</span><span class="punctuation">:</span> <span class="string">&quot;清浅 明月 东西 白沙&quot;</span>  </span><br><span class="line"><span class="punctuation">&#125;</span> </span><br></pre></td></tr></table></figure><br>我们筛选了数据集中的关键词，组成了6000余个核心关键词集合。</p><h3 id="3-2-Model-Architecture"><a href="#3-2-Model-Architecture" class="headerlink" title="3.2 Model Architecture"></a>3.2 Model Architecture</h3><p>由于图像-古诗对数据的匮乏，我们的初代版本使用两阶段的生成方式。</p><p>如图6所示，我们通过两个步骤进行推理：</p><ol><li>CLIP编码：我们首先将关键词集合通过text encoder进行编码，保存以供多次使用。其次，将感兴趣的图像通过image encoder编码，获得图像embedding。我们比较图像embedding和所有关键词embedding的相似度，选择top-k个关键词作为古诗生成的prefix。</li><li>Decoder生成古诗：利用BERT tokenizer对prefix进行分词，进而通过预训练的Decoder模型生成古诗。</li></ol><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/6.png" alt></p><center>图6. CLIP + Decoder的Image2Poem结构</center><p>我们预训练了2个版本的生成式模型：<code>GPT2</code>(Decoder型)和<code>T5</code>(Encoder-Decoder型)。我们将keywords作为模型的输入，期望模型生成符合要求的古诗。训练期间，我们加入一定的MLM策略：对诗中出现在关键词中的字，我们采用15%的概率进行mask掩码，期望让模型学会在古诗生成中包含关键词。</p><h3 id="3-3-Model-Improvement"><a href="#3-3-Model-Improvement" class="headerlink" title="3.3 Model Improvement"></a>3.3 Model Improvement</h3><p>要做模型的改进，我们认为核心是收集高质量图像-古诗对数据集。我们准备进行如下两阶段的数据集获取：</p><ol><li>利用预训练的BLIP2/Chinese-CLIP进行图文检索，获取古诗的对应相关图像；</li><li>对于匹配度不高的图像-古诗对数据，我们考虑Stable Diffusion生成的方式。</li></ol><p>拥有数据后，我们使用图7的结构进行端到端的预训练方式。具体而言：我们冻结预训练的CLIP模型参数和GPT2模型参数，只训练transformer-based的Mapping Network。借鉴BLIP2的思想，我们期望Mapping Network可以对齐图像编码空间和GPT2的文本空间。与BLIP2类似，我们设计了一个固定的learned queries，与CLIP图像编码器的输出进行融合(使用concatenate或者cross-attention)，再将输出作为prefix embedding提供给GPT2模型。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/7.png" alt></p><p><center>图7. 改进的Image2Poem结构</center><br>由于还在进行图像-古诗数据的检索，还没能对改进的结构进行测试，但我们相信这个改进是有意义的。</p>]]></content>
      
      
      <categories>
          
          <category> 夏令营经历 </category>
          
          <category> multi-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Large-Language-Model For Math</title>
      <link href="/2023/06/03/Survey4MATH/"/>
      <url>/2023/06/03/Survey4MATH/</url>
      
        <content type="html"><![CDATA[<h1 id="Large-Language-Model-For-Math"><a href="#Large-Language-Model-For-Math" class="headerlink" title="Large-Language-Model For Math"></a>Large-Language-Model For Math</h1><p>让LLM大模型解决数学问题！ — #TODO</p><p>由于最近在做相关方向的科研，将阅读的论文整理在这里。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">💡 阅读记录格式:</span><br><span class="line"></span><br><span class="line">### Pretraining</span><br><span class="line"></span><br><span class="line">1. which foundation models are based on?</span><br><span class="line">2. what tokenizers are adopted?</span><br><span class="line">3. which datasets are collected specific for &quot;math&quot;?</span><br><span class="line">4. what types of pre-processing methods are introduced?</span><br><span class="line">5. other information that  you think is important</span><br><span class="line"></span><br><span class="line">### Fine-tuning</span><br><span class="line"></span><br><span class="line">1. which datasets are used?</span><br><span class="line">2. what types of pre-processing methods are used?</span><br><span class="line"></span><br><span class="line">### Evaluation</span><br><span class="line"></span><br><span class="line">1. which datasets are used?</span><br><span class="line">2. what type of pre-processing methods are used?</span><br><span class="line">3. what evaluation metrics are used?</span><br></pre></td></tr></table></figure><h3 id="阅读的论文列表："><a href="#阅读的论文列表：" class="headerlink" title="阅读的论文列表："></a>阅读的论文列表：</h3><p><a href="https://www.notion.so/Training-Verifiers-to-Solve-Math-Word-Problems-db6822f1cf9b45ad960b1dbb574ab4b8">Training Verifiers to Solve Math Word Problems</a></p><p><a href="https://www.notion.so/Solving-Quantitative-Reasoning-Problems-with-Language-Models-19b339fda58246fbadb22166a78b6ffd">Solving Quantitative Reasoning Problems with Language Models</a></p><p><a href="https://www.notion.so/MathPrompter-Mathematical-Reasoning-using-Large-Language-Models-92db0f041ac94c53884e799948af207d">MathPrompter: Mathematical Reasoning using Large Language Models</a></p><p><a href="https://www.notion.so/PAL-Program-aided-Language-Models-ddaecc337f414b8ea37985999bdec23c">PAL: Program-aided Language Models</a></p><p><a href="https://www.notion.so/Specializing-Smaller-Language-Models-towards-Multi-Step-Reasoning-6e8af05836f243058cc8d2374162c2c6">Specializing Smaller Language Models towards Multi-Step Reasoning</a></p>]]></content>
      
      
      <categories>
          
          <category> llm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image2Poem</title>
      <link href="/2023/05/23/Image2Poem/"/>
      <url>/2023/05/23/Image2Poem/</url>
      
        <content type="html"><![CDATA[<h1 id="Image-to-Poem"><a href="#Image-to-Poem" class="headerlink" title="Image-to-Poem"></a>Image-to-Poem</h1><p>此情此景，何不吟诗一首？Image-to-Poem帮你完成！</p><p>项目链接：<br><a href="https://github.com/weiji-Feng/Image2Poem">https://github.com/weiji-Feng/Image2Poem</a></p><p>9.28之前可能会忙于升学，本项目暂不更新。希望可以在10.31号之前完成这个项目的全部功能。</p><h2 id="1-项目介绍"><a href="#1-项目介绍" class="headerlink" title="1. 项目介绍"></a>1. 项目介绍</h2><p>图像生成古诗(Image to Poem)，旨在为给定的图像自动生成符合图像内容的古诗句。</p><p>使用对比学习预训练的CLIP模型拥有良好的迁移应用和zero-shot能力，是打通图像-文本多模态的重要模型之一。 本项目使用<a href="https://github.com/openai/CLIP">CLIP模型</a>生成古诗意象关键词向量和图像向量。</p><p>初始版本的生成方法为：搜集一个古诗词意象关键词数据集(close-set)，然后通过text-encoder(图1.右) 生成对应的关键词向量。对给定的一张图像，同样通过Image-encoder即可得到图像向量。比较图像向量和每个关键词向量的余弦相似度，可以得到top-k个相关关键词。将关键词送入语言模型，自动生成一首诗。</p><p>这种提取关键词的操作将<strong>会大大损失图像的语义信息</strong>，进而影响语言模型的古诗生成。但由于图像-古诗对数据集非常匮乏，我们很难&lt;/u&gt;像Dalle模型一样&lt;/u&gt;，直接将CLIP模型Image-encoder的输出向量，通过一个MappingNet(在DALLE-2中就是prior模块)送入解码器(语言模型)。所以如果有更好的想法欢迎指点。<br><img src="https://github.com/openai/CLIP/raw/main/CLIP.png" alt="img.png"></p><p>由于古诗的特殊性，本项目重头训练了一个用于生成古诗文的Language Model，尝试了T5 model（223M）和GPT2 model（118M），现公开该预训练模型以供大家娱乐。</p><p>以上模型均可通过调用 <a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a> 的<code>transformers</code>导入。</p><h2 id="2-引用和致谢"><a href="#2-引用和致谢" class="headerlink" title="2. 引用和致谢"></a>2. 引用和致谢</h2><p>在项目完成期间，我参考并使用了以下项目，这里表示感谢！ </p><ul><li>数据集来源：<a href="https://github.com/THUNLP-AIPoet/CCPM">https://github.com/THUNLP-AIPoet/CCPM</a></li><li>CLIP预训练模型来源： <a href="https://github.com/OFA-Sys/Chinese-CLIP">https://github.com/OFA-Sys/Chinese-CLIP</a></li><li>GPT2预训练部分代码：<a href="https://github.com/Morizeyao/GPT2-Chinese">https://github.com/Morizeyao/GPT2-Chinese</a></li></ul><h2 id="3-使用说明和生成样例"><a href="#3-使用说明和生成样例" class="headerlink" title="3. 使用说明和生成样例"></a>3. 使用说明和生成样例</h2><h3 id="安装依赖库"><a href="#安装依赖库" class="headerlink" title="安装依赖库"></a>安装依赖库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch torchvision torchaudio</span><br><span class="line">pip install transformers</span><br><span class="line">pip install tqdm matplotlib</span><br></pre></td></tr></table></figure><p>如果希望尝试预训练语言模型, 建议安装<code>torch+cudaxx.x</code>的GPU版本。</p><h3 id="快速体验古诗生成"><a href="#快速体验古诗生成" class="headerlink" title="快速体验古诗生成"></a>快速体验古诗生成</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python img2poem.py --image_path ./datasets/images/feiliu.jpg --model_type T5 --model_path ./config/t5_config</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>--image_path</code>: 图片所在位置</li><li><code>--model_type</code>: 模型名称,目前可选用’T5’,’GPT2’</li><li><code>--model_path</code>: 模型所在文件夹</li></ul><h3 id="生成样例"><a href="#生成样例" class="headerlink" title="生成样例"></a>生成样例</h3><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/feiliu.jpg" alt></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">飞鹤度湖山，青松半掩关。水林昼景凤，诸君自有闲。</span><br><span class="line">白苹洲渚流，丹青未有人。水林壑夜深，乱峰高几重。 </span><br><span class="line">仙人问道踪，壑深自坐禅。丹青一片云，月随风水林。   </span><br><span class="line">不见青林路，却忆庐陵西。老松犹未分，钟山水似难。</span><br><span class="line">飞鹤度湖山，青松半掩关。丹壑千人在，犹记林水心。</span><br></pre></td></tr></table></figure><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/chun.jpg" alt="image-20230408233621777" style="zoom:30%;"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">青碧绕瑶池，碧峰回九关。晚风吹画船，映水长成芳。</span><br><span class="line">犹自在藏新，不知是旧人。暮天三百年，桃源一片春。</span><br><span class="line">数年三两枝，却羡玉龙飞。桃源水一津，斜晖又一年。</span><br><span class="line">千骑鹤归飞，一曲茅亭去。天上桃源路，玉龙归白杳。</span><br><span class="line">相逢一笑飞，知有桃源人。何如写玉龙，斜晖送晚风</span><br></pre></td></tr></table></figure><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/pubu.jpg" alt="image-20230408233621777" style="zoom:90%;"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">清江起玉龙，时听瀑布声。飞曲满游子，一生天上流。</span><br><span class="line">萧然见山来，却恐归来晚。时见布泉飞，锦囊深掩门。</span><br><span class="line">时有一花村，俗人如此山。月溪风乱鸣，时有瀑布还。</span><br><span class="line">月明闲送风，瀑布飞仙雪。人间几度林，锦衣还说天。 </span><br><span class="line">谁人识此中，布泉老客行。月满江来路，不须频为谁。</span><br></pre></td></tr></table></figure><h2 id="4-一些解释"><a href="#4-一些解释" class="headerlink" title="4. 一些解释"></a>4. 一些解释</h2><ul><li>对于当前项目的评价<blockquote><p>提取关键词进行古诗生成是一个<strong>损失信息</strong>的过程，尤其是将图像映射到关键词的操作，损失了图像原本的语义(例如只能识别人，而不知道人在做什么)。所以效果上来看仍然差强人意。</p><p>没有给模型一些关于韵律、题材、体裁等的设定，导致不够专业。</p></blockquote></li><li><p>可不可以使用自己的古诗数据集尝试预训练？</p><blockquote><p>可以，不过由于CCPM数据集是<code>.json</code>文件格式,导入方式与<code>.txt</code>不同。所以在<code>datasets.py</code>文件里你需要重新写一下有关文件导入的部分。并且由于预训练方法多样，你也可以修改预训练时的一些策略。</p></blockquote></li><li><p>项目的预训练方法是什么？</p><blockquote><p>首先对于GPT2模型，常规预训练方法就是自回归，本项目尝试了mask关键词的方法，例如：</p><p><code>[CLS]关键词：明月 故乡 [EOS] 举头望明月，低头思故乡[SEP]</code> =&gt; <code>[CLS]关键词：明月 故乡 [EOS] 举头望[MASK][MASK]，低头思[MASK][MASK][SEP]</code></p><p>然后我额外对这些mask token的预测准确率进行了计算，加入了损失函数中。</p><p>对于T5模型，由于是encoder-decoder架构，我使用下列格式创建数据：</p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS][SEP]</code>, y = <code>[CLS]红豆生南国[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS][SEP]</code>, y = <code>[CLS]秋来发故枝[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS]秋来发故枝[EOS][SEP]</code>, y = <code>[CLS]愿君多采撷[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS]秋来发故枝[EOS]愿君多采撷[EOS][SEP]</code>, y = <code>[CLS]此物最相思[EOS][SEP]</code></p></blockquote></li><li><p>通过什么方式进行图像生成古诗？未来有什么进一步更新的方法？</p><blockquote><p>现在的实现比较简单，就是先搜集一个闭环的关键词数据集(<code>keyword.txt</code>)，然后使用CLIP对图像和所有关键词进行编码，计算它们之间的相似度，取相似度最高的K个关键词，然后放置于语言模型进行生成。</p><p>由于<code>图像-古诗对</code>数据集非常匮乏，似乎暂时做不到删去这个闭环关键词数据集。未来如果有充足的数据集，我会使用<code>CLIP-MappingNet-T5/GPT2</code>的模型架构进行训练，例如下图的<a href="https://arxiv.org/pdf/2111.09734.pdf">CLIPCap</a>架构：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*8RLzDpMfi6sLScqx2SguaA.png" alt></p></blockquote></li></ul><p>未来有古诗生成图像的想法，待进一步更新。现有的可以进行古诗生成图像的项目有：<a href="https://huggingface.co/IDEA-CCNL/Taiyi-Diffusion-532M-Nature-Chinese">https://huggingface.co/IDEA-CCNL/Taiyi-Diffusion-532M-Nature-Chinese</a></p>]]></content>
      
      
      <categories>
          
          <category> multi-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
