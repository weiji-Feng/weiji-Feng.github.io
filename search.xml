<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</title>
      <link href="/2023/06/09/unimatch/"/>
      <url>/2023/06/09/unimatch/</url>
      
        <content type="html"><![CDATA[<h1 id="论文复现-Revisiting-Weak-to-Strong-Consistency-in-Semi-Supervised-Semantic-Segmentation"><a href="#论文复现-Revisiting-Weak-to-Strong-Consistency-in-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="论文复现 - Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation"></a>论文复现 - Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</h1><blockquote><p>前记：这篇文章基于20年半监督学习的SOTA：FixMatch。他们发现FixMatch可以在半监督语义分割任务上媲美最近的SOTA，故在此基础上进行了多角度优化，思路很有借鉴意义。</p><p>下面，我们将通过论文介绍和实验复现两部分详细展示论文复现工作。</p></blockquote><h2 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h2><p>因为论文是基于<code>FixMatch</code>做的一些工作，所以我们先回归一下FixMatch</p><h3 id="1-FixMatch"><a href="#1-FixMatch" class="headerlink" title="1. FixMatch"></a>1. FixMatch</h3><p>根据论文的意思，由于半监督学习SSL的先进方法都引入了太多复杂的结构，<code>FixMatch</code>希望可以构建一个simple却又精确的模型。如图1所示，对于一张未标记的图片，模型通过预测<code>weakly-augmented</code>后的图片得到伪标签(注意，只有<strong>置信度高于阈值的才被使用</strong>，否则忽略)，然后最小化<code>strongly-augmented</code>后图片的<u>预测分布和伪标签的“距离”</u>。这里距离的衡量是使用<code>H(p,q): Cross Entropy</code>.<br><img src="/2023/06/09/unimatch/2.png" alt></p><center>图1. FixMatch框架</center><p>那么，为什么可以这么做？为什么<code>weak</code>和<code>strong</code>的预测分布是相近的？于是我们引出<code>FixMatch</code>的2个核心思想：</p><h4 id="Consistency-regularization"><a href="#Consistency-regularization" class="headerlink" title="Consistency regularization"></a>Consistency regularization</h4><p>一致性正则化，它有一个很强的假设，就是<u>同一图像经过不同扰动后输入模型，其输出的预测应该是接近或者类似的</u>。故其loss fuction表示为：</p><script type="math/tex; mode=display">\sum_{b=1}^{\mu B}\Vert p_m(y|\alpha(u_b)) - p_m(y|\alpha(u_b)) \Vert_2^2\tag{1}</script><p>其中$\mu B$代表未标记的数据量，$p_m$是模型，$\alpha$是一个弱扰动(简单的数据增广)。因为数据增广是具有随机性的，所以上式两项上并不一定相同。</p><h4 id="Pseudo-labeling"><a href="#Pseudo-labeling" class="headerlink" title="Pseudo-labeling"></a>Pseudo-labeling</h4><p>伪标签的思想，是希望利用置信度高的数据进行自我训练，从而提高模型性能，具体而言，体现为下面的损失函数：</p><script type="math/tex; mode=display">\frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \tau\right)\text{H}(\hat{q}_b, q_b)\tag{2}</script><p>可以看到，上式设置了一个阈值$\tau$用于控制置信度，只有置信度高于阈值的才进行loss的计算。式子中 $ \hat{q}_b=\arg \max q_b $ ，是指预测分布中分数最高的那个类别，也称为硬标签。</p><p><code>FixMatch</code>结合了这两个思想：即使用弱扰动的图像通过模型生成的伪标签，来监督强扰动的预测结果。具体来说，模型先预测弱扰动图像的分布，并得到硬标签。如果该标签的置信度高于阈值，则将强扰动图像的预测输出和该标签做一个交叉熵损失：</p><script type="math/tex; mode=display">\begin{aligned}q_b &= p_m(y|\alpha (u_b))\\\hat{q}_b &= \arg \max q_b\\\mathcal{L}_u &= \frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \tau\right)\text{H}(\hat{q}_b, p_m(y|\mathcal{A}(u_b)))\end{aligned}\tag{3}</script><p>其中 $\mathcal{A}(\cdot)$ 就是强扰动(强数据增广)。</p><p>最后，数据增广也是<code>FixMatch</code>关键的一环，在原论文中，作者对增广做了如下设置：</p><ul><li>弱扰动：标准的<code>flip-and-shift</code>增广，即水平翻转或垂直翻转；</li><li>强扰动：作者认为基于强化学习的AutoAugment需要很多带标签的数据，并不适合SSL任务。所以作者采用了以下两个增广方式：<ol><li><code>RandAugment</code>：只需要搜索增强操作的数量<code>N</code>和全局的增强幅度<code>M</code>(分为10个等级，10为最强)，代码如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Identity是恒等变换，不做任何增强</span></span><br><span class="line">transforms = [<span class="string">&#x27;Identity&#x27;</span>, <span class="string">&#x27;AutoContrast&#x27;</span>, <span class="string">&#x27;Equalize&#x27;</span>, <span class="string">&#x27;Rotate&#x27;</span>, <span class="string">&#x27;Solarize&#x27;</span>, </span><br><span class="line">              <span class="string">&#x27;Color&#x27;</span>, <span class="string">&#x27;Posterize&#x27;</span>, <span class="string">&#x27;Contrast&#x27;</span>, <span class="string">&#x27;Brightness&#x27;</span>, <span class="string">&#x27;Sharpness&#x27;</span>, </span><br><span class="line">              <span class="string">&#x27;ShearX&#x27;</span>, <span class="string">&#x27;ShearY&#x27;</span>, <span class="string">&#x27;TranslateX&#x27;</span>, <span class="string">&#x27;TranslateY&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">randaugment</span>(<span class="params">N, M</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;Generate a set of distortions.</span></span><br><span class="line"><span class="string">Args:</span></span><br><span class="line"><span class="string">N: Number of augmentation transformations to apply sequentially.</span></span><br><span class="line"><span class="string">M: Magnitude for all the transformations.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">sampled_ops = np.random.choice(transforms, N)</span><br><span class="line"><span class="keyword">return</span> [(op, M) <span class="keyword">for</span> op <span class="keyword">in</span> sampled_ops]</span><br></pre></td></tr></table></figure></li><li><code>CTAugment</code>：一种在线学习的方法。该方法先定义一组transforms(如旋转、裁剪等)以及每种变换可能的幅度(旋转的角度等)，然后维护一个<code>变换-幅度-概率表</code>，记录每种变换和幅度的概率，初始化为均匀分布。对于每一张unlabelled图像，从表中随机采样弱增强(变换+幅度)和强增强(变换+幅度)，然后利用弱增强生成伪标签。如果置信度高于阈值则计算伪标签和强增强的预测分布之间的交叉熵损失。最后，根据损失大小更新概率表，损失小，则提高相应概率。</li></ol></li></ul><h3 id="2-UniMatch"><a href="#2-UniMatch" class="headerlink" title="2. UniMatch"></a>2. UniMatch</h3><p><code>UniMatch</code>建立在FixMatch引入图像级强扰动的思想上。具体来说，作者认为<code>FixMatch</code>的强扰动是性能优越的关键，但仍可以进一步发挥强扰动的潜力。它们做了2个方向的改进：</p><ul><li>同时探索image和feature两个层面的扰动 - 探索更广泛的扰动空间</li><li>Dual-stream 扰动，充分利用预定义的image扰动空间。<br><figure class="half"><img src="/2023/06/09/unimatch/3.png" width="400"></figure><center>图2. FixMatch和UniMatch对比</center></li></ul><h3 id="2-1-UniPerb-Perturbations-for-Images-and-Features"><a href="#2-1-UniPerb-Perturbations-for-Images-and-Features" class="headerlink" title="2.1 UniPerb - Perturbations for Images and Features"></a>2.1 UniPerb - Perturbations for Images and Features</h3><p>即同时对image和feature扰动的方法。作者将模型 $f$ 拆成了<code>encoder</code> $g$ 和<code>decoder</code> $h$，$x^w$ 在通过编码器后得到特征，再施加一个特征扰动 $\mathcal{P}$ 得到特征扰动后的新特征 $FP$。这样经过解码器，我们同时可获得(图像)弱扰动的预测分布、特征强扰动的预测分布、(图像)强扰动的预测分布，如下图所示：</p><figure class="half"><img src="/2023/06/09/unimatch/4.png" width="180"></figure><center>图3. UniPerb</center>使用公式可以表示为：$$\begin{aligned}p^w &= \hat{F}\left(\mathcal{A}^w(x^u) \right);\quad p^s = F\left(\mathcal{A}^s\left(\mathcal{A}^w(x^u)\right) \right)\\e^w &= g(x^w)\\p^{fp} &= h\left(\mathcal{P}\left(e^w \right)\right)\\\mathcal{L}_u &= \frac{1}{B_u}\sum \Bbb{I}\left(\max(p^w)\geq \tau \right)\left(\text{H}(p^w, p^s), \text{H}(p^w, p^{fp}) \right)\end{aligned}\tag{4}$$其中， $x^w$ 是$x^u$经过弱扰动的图像； $\hat F$ 是弱扰动图像的教师模型， $F$ 是强扰动图像使用的学生模型，这里他们俩完全相同。### 2.2 DusPerb - Dual-Stream Perturbations作者受到其他工作的影响，认为为无标签图像数据构建多个view作为输入可以更好的利用扰动空间。简单地，他们为一张图像设置2个强扰动视图：$x^{s_1}$和$x^{s_2}$。因为 $\mathcal{A}^s$ 具有随机性，故这两个视图不同。该模式的结构如下图所示：<figure class="half"><img src="/2023/06/09/unimatch/5.png" width="180"></figure><center>图4. DusPerb</center>作者将该结构的优越性归功于 **对比学习** (而不是单纯doubled unlabeled batch size)：$x^{s_1}$ 和 $x^{s_2}$ 都应该和 $x^w$ 中预测概率最高的类别接近，等价于 $x^{s_1}$ 和 $x^{s_2}$ 互相接近，这可以使用InfoNCE Loss实现：<figure class="half"><img src="/2023/06/09/unimatch/6.png" width="600"></figure><p>最终模型结合了<code>UniPerb</code>和<code>DusPerb</code>，Loss表示为</p><script type="math/tex; mode=display">\mathcal{L}_u = \frac{1}{B_u} \sum \Bbb{I}\left(\max(p^w)\geq \tau \right)\cdot \left( \lambda \text{H}(p^w, p^{fp}) + \frac{\mu}{2}\left(\text{H}(p^w, p^{s_1}) + \text{H}(p^w, p^{s_2})\right) \right)\tag{5}</script><h3 id="3-实验结果和消融研究"><a href="#3-实验结果和消融研究" class="headerlink" title="3. 实验结果和消融研究"></a>3. 实验结果和消融研究</h3><p>由于是新的SOTA，论文在三个数据集上展现出了强劲表现。这里简单摘录在<code>pascal voc 2012</code>数据集上的一些表现(因为下文复现时<u>仅使用该数据集</u> )。</p><h4 id="labelled-data数量"><a href="#labelled-data数量" class="headerlink" title="labelled data数量"></a>labelled data数量</h4><p><figure class="half"><img src="/2023/06/09/unimatch/7.png" width="500"></figure></p><center>图5. 在不同标注数据使用量下，模型在Pascal上的表现</center><p>可以看到UniMatch不仅准确率高，还比较稳定，在标注数据较少(如，92)的情况下依然还有较高精度。</p><h4 id="labelled-data占比"><a href="#labelled-data占比" class="headerlink" title="labelled data占比"></a>labelled data占比</h4><p><figure class="half"><img src="/2023/06/09/unimatch/8.png" width="500"></figure></p><center>图6. 在不同标注数据占比下，模型在Pascal上的表现</center><p>UniMatch在标注数据占比较少(如，1/16)的情况下依然还有较高精度。</p><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>这一块我们简单列举一下论文的结论：</p><ol><li><strong>The improvement of diverse perturbations is non-trivial</strong>：即多种类型的强扰动(2×image+feature)是比简单设置3个image强扰动有效的；</li><li><strong>The improvement of dual-stream perturbation is non-trivial</strong>：论文证明双流扰动的成功不是因为增加了一个batch内的unlabelled data；</li><li><strong>The necessity of separating image- and feature-level perturbations into independent streams</strong>：即分离不同类型的扰动是有效的；</li><li><strong>More perturbation streams</strong>：论文证明图像级多流扰动提升有限，双流以已经足够了；</li></ol><p>…</p><hr><h2 id="实验结果复现"><a href="#实验结果复现" class="headerlink" title="实验结果复现"></a>实验结果复现</h2><p>下文分析和修改的代码源自论文仓库： <a href="https://github.com/LiheYoung/UniMatch">https://github.com/LiheYoung/UniMatch</a> .</p><h3 id="1-下载代码、模型和数据"><a href="#1-下载代码、模型和数据" class="headerlink" title="1. 下载代码、模型和数据"></a>1. 下载代码、模型和数据</h3><h4 id="1-1-代码下载"><a href="#1-1-代码下载" class="headerlink" title="1.1 代码下载"></a>1.1 代码下载</h4><p>关于代码的<code>Installation</code>，直接按照默认方法：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd UniMatch</span><br><span class="line">conda create -n unimatch python=3.10.4</span><br><span class="line">conda activate unimatch</span><br><span class="line">pip install -r requirements.txt # 别急,请先按照下面的第一条修改文件;</span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure><br>值得强调的是，代码其实包含一些Bug，需要简单处理一下：</p><ol><li>在<code>requirements.txt</code>中，需要将<code>sklearn</code>改成<code>scikit-learn</code>，保证pip install 顺利进行；</li><li>在<code>unimatch.py</code>中，切记将下面这行代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--local_rank&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure> 改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&#x27;--local-rank&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure> 不然你的local-rank参数不被识别；</li><li>如果你是单机单卡或者单机多卡(例如我)，可以将<code>train.sh</code>配置为 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1,2,3</span><br><span class="line"></span><br><span class="line">python -m torch.distributed.launch \</span><br><span class="line">    --nnodes 1 \</span><br><span class="line">    --nproc_per_node=$1 \</span><br><span class="line">    $method.py \</span><br><span class="line">    --config=$config --labeled-id-path $labeled_id_path --unlabeled-id-path $unlabeled_id_path \</span><br><span class="line">    --save-path $save_path 2&gt;&amp;1 | tee $save_path/$now.log</span><br></pre></td></tr></table></figure> 无需设置port等参数。</li></ol><h4 id="1-2-预训练模型下载"><a href="#1-2-预训练模型下载" class="headerlink" title="1.2 预训练模型下载"></a>1.2 预训练模型下载</h4><p>预训练的模型在原仓库中有3种：<code>ResNet50</code>/<code>ResNet101</code>/<code>xception</code>，在复现时默认使用resnet101，如果时间允许，我们将尝试其他模型的复现。</p><h4 id="1-3-数据集下载"><a href="#1-3-数据集下载" class="headerlink" title="1.3 数据集下载"></a>1.3 数据集下载</h4><p>数据集由于时间和资源有限，仅仅复现关于<code>Pascal VOC 2012</code>数据集的一些结果。</p><ul><li>Pascal: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar">JPEGImages</a> | <a href="https://drive.google.com/file/d/1ikrDlsai5QSf2GiSUR3f8PZUzyTubcuF/view?usp=sharing">SegmentationClass</a></li></ul><p>其他数据集见原仓库。</p><h3 id="2-训练的实现"><a href="#2-训练的实现" class="headerlink" title="2. 训练的实现"></a>2. 训练的实现</h3><p>我们准备好了数据，可以按照下面的<a id="fig1">算法图</a>完成模型训练：<br><img src="/2023/06/09/unimatch/1.png" alt></p><center>图7. 算法示意图</center><h4 id="2-1-数据增广"><a href="#2-1-数据增广" class="headerlink" title="2.1 数据增广"></a>2.1 数据增广</h4><blockquote><p>这里说的数据增广，其实是指<code>strong view</code>的强扰动和<code>weak view</code>的弱扰动。</p><p>相关文件：<strong>./dataset/semi.py</strong></p></blockquote><p>在<a href="#fig1">图7</a>中指代下面这几行代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one weak view and two strong views as input</span></span><br><span class="line">x_w = aug_w(x)</span><br><span class="line">x_s1, x_s2 = aug_s(x_w), aug_s(x_w)</span><br></pre></td></tr></table></figure></p><p>源代码在实现这几行时，首先让每一张图像完成<strong>弱扰动</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img, mask = resize(img, mask, (<span class="number">0.5</span>, <span class="number">2.0</span>))</span><br><span class="line">ignore_value = <span class="number">254</span> <span class="keyword">if</span> self.mode == <span class="string">&#x27;train_u&#x27;</span> <span class="keyword">else</span> <span class="number">255</span></span><br><span class="line">img, mask = crop(img, mask, self.size, ignore_value)</span><br><span class="line">img, mask = hflip(img, mask, p=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>其中<code>img</code>是RGB图像，<code>mask</code>是分割的掩码。现在，这个经过弱扰动的图像<code>img</code>就是$x^w$，同时，我们将其复制两份，得到$x^{s_1}$和$x^{s_2}$。当然$x^{s_1}$和$x^{s_2}$还需要经过强扰动，成为2个<code>strong view</code>，实现<code>Dual-Stream Perturbations</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_w, img_s1, img_s2 = deepcopy(img), deepcopy(img), deepcopy(img)</span><br></pre></td></tr></table></figure></p><p>进行强扰动的代码(<strong>以处理$x^{s_1}$为例</strong>)如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">    <span class="comment"># 随机调整亮度、对比度、饱和度和色调</span></span><br><span class="line">    img_s1 = transforms.ColorJitter(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.25</span>)(img_s1)</span><br><span class="line">img_s1 = transforms.RandomGrayscale(p=<span class="number">0.2</span>)(img_s1)  <span class="comment"># 随机灰度化</span></span><br><span class="line">img_s1 = blur(img_s1, p=<span class="number">0.5</span>)    <span class="comment"># 随机模糊</span></span><br><span class="line">cutmix_box1 = obtain_cutmix_box(img_s1.size[<span class="number">0</span>], p=<span class="number">0.5</span>)  <span class="comment"># 随机获取CutMix的区域</span></span><br></pre></td></tr></table></figure></p><p>因为这些数据增强方法设置了概率，故不同的epoch或者是$x^{s_1}$和$x^{s_2}$之间，增强的效果都是不同的。其中我对于<code>CutMix</code>操作还比较好奇，去查看了函数定义。发现CutMix就是mask掉一块区域(该区域的宽高和位置都是一定程度随机的)，然后用其他图片中<strong>相同位置的区域</strong>来<a href="#padding">填充</a>。</p><p>由于<code>Pascal</code>数据集的标注图像<code>mask</code>中包含254这个无效像素值，没有对应类别，作者使用<code>ignore_mask</code>忽略它：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ignore_mask = Image.fromarray(np.zeros((mask.size[<span class="number">1</span>], mask.size[<span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">img_s1, ignore_mask = normalize(img_s1, ignore_mask)</span><br><span class="line">img_s2 = normalize(img_s2)</span><br><span class="line"></span><br><span class="line">mask = torch.from_numpy(np.array(mask)).long()</span><br><span class="line">ignore_mask[mask == <span class="number">254</span>] = <span class="number">255</span></span><br></pre></td></tr></table></figure><br>取值为255是因为<code>crop</code>操作对哪些裁剪时遇到的padding都设置值为255，同样也是无效区域，这里相当于合并了。于是，经过图像增广等操作后，我们的输入数据可能就包含以下几个部分：</p><ul><li>$x^w$: 即<code>img_w</code>，在return时还需要normalize一下；</li><li>$x^{s_1}$: 即<code>img_s1</code>，经过强扰动，且已经normalize；</li><li>$x^{s_2}$: 即<code>img_s2</code>，经过强扰动，且已经normalize；</li><li>ignore_mask: 用于忽略无效的像素；</li><li>cutmix_box1: 从$x^{s_1}$获取的mask掉的CutMix区域；</li><li>cutmix_box2: 从$x^{s_2}$获取的mask掉的CutMix区域；</li></ul><p>了解增广的细节后，我们可以构建3个数据集，分别是有标签监督数据、无标签数据、和验证数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">trainset_u = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;train_u&#x27;</span>,</span><br><span class="line">                         cfg[<span class="string">&#x27;crop_size&#x27;</span>], args.unlabeled_id_path)</span><br><span class="line">trainset_l = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;train_l&#x27;</span>,</span><br><span class="line">                         cfg[<span class="string">&#x27;crop_size&#x27;</span>], args.labeled_id_path, </span><br><span class="line">                         nsample=<span class="built_in">len</span>(trainset_u.ids))</span><br><span class="line">valset = SemiDataset(cfg[<span class="string">&#x27;dataset&#x27;</span>], cfg[<span class="string">&#x27;data_root&#x27;</span>], <span class="string">&#x27;val&#x27;</span>)</span><br></pre></td></tr></table></figure><br>将它们分别转为Dataloader后，通过<a id="canshu">下面的代码</a>进行分批训练：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loader = <span class="built_in">zip</span>(trainloader_l, trainloader_u, trainloader_u)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, ((img_x, mask_x),</span><br><span class="line">        (img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2),</span><br><span class="line">        (img_u_w_mix, img_u_s1_mix, img_u_s2_mix, ignore_mask_mix, _, _)) </span><br><span class="line">        <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br></pre></td></tr></table></figure><br><u><strong>接下来的分析，都在上述循环中，请关注从<code>loader</code>中取出的这些数据！</strong></u></p><p><a id="padding">填充CutMix </a></p><p>最后一步，将<code>cutmix</code>操作完成，具体来说，我们用第二个<code>trainloader_u</code>中获取的数据来填充我们的$s_1$和$s_2$：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img_u_s1[cutmix_box1.unsqueeze(<span class="number">1</span>).expand(img_u_s1.shape) == <span class="number">1</span>] = \</span><br><span class="line">    img_u_s1_mix[cutmix_box1.unsqueeze(<span class="number">1</span>).expand(img_u_s1.shape) == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">img_u_s2[cutmix_box2.unsqueeze(<span class="number">1</span>).expand(img_u_s2.shape) == <span class="number">1</span>] = \</span><br><span class="line">    img_u_s2_mix[cutmix_box2.unsqueeze(<span class="number">1</span>).expand(img_u_s2.shape) == <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><h4 id="2-2-模型预测"><a href="#2-2-模型预测" class="headerlink" title="2.2 模型预测"></a>2.2 模型预测</h4><p>在<a href="#fig1">图7</a>中，这部分表示为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature of weakly perturbed image</span></span><br><span class="line">feat_w = g(x_w)</span><br><span class="line"><span class="comment"># perturbed feature</span></span><br><span class="line">feat_fp = nn.Dropout2d(<span class="number">0.5</span>)(feat_w)</span><br><span class="line"><span class="comment"># four predictions from four forward streams</span></span><br><span class="line">p_w, p_fp = h(torch.cat((feat_w, feat_fp))).chunk(<span class="number">2</span>)</span><br><span class="line">p_s1, p_s2 = f(torch.cat((x_s1, x_s2))).chunk(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>在<code>unimatch.py</code>中，并没有展现出将$f$拆分为 $h(g(x))$ 的细节，而是直接通过model生成预测，所以<code>dropout2d</code>应该包含在model里了。我们截取了<a id="pred_x">下面代码</a>，作为上述部分的实现，并提供解释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_lb, num_ulb = img_x.shape[<span class="number">0</span>], img_u_w.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># img_x是带标签的监督图像数据，通过计算pred_x可以进行有监督训练.</span></span><br><span class="line">preds, preds_fp = model(torch.cat((img_x, img_u_w)), <span class="literal">True</span>)  <span class="comment"># need_fp=True,进行dropout</span></span><br><span class="line">pred_x, pred_u_w = preds.split([num_lb, num_ulb])   <span class="comment"># pred_u_w =&gt; p_w</span></span><br><span class="line">pred_u_w_fp = preds_fp[num_lb:] <span class="comment"># pred_u_w_fp =&gt; p_fp,进行特征层面的自监督训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pred_u_s1 =&gt; p_s1, pred_u_s2 =&gt; p_s2</span></span><br><span class="line">pred_u_s1, pred_u_s2 = model(torch.cat((img_u_s1, img_u_s2))).chunk(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><h4 id="2-3-Loss计算"><a href="#2-3-Loss计算" class="headerlink" title="2.3 Loss计算"></a>2.3 Loss计算</h4><p>在<a href="#fig1">图7</a>中指代一下部分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hard (one-hot) pseudo mask</span></span><br><span class="line">mask_w = p_w.argmax(dim=<span class="number">1</span>).detach()</span><br><span class="line"><span class="comment"># loss from image- and feature-level perturbation</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">p_s = torch.cat((p_s1, p_s2))</span><br><span class="line">loss_s = criterion(p_s, mask_w.repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">loss_fp = criterion(p_fp, mask_w)</span><br><span class="line"><span class="comment"># final unsupervised loss</span></span><br><span class="line">loss_u = (loss_s + loss_fp) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure></p><p>由于$x^{s_1}$和$x^{s_2}$进行过<code>CutMix</code>，而$x^{w}$并没有做这些强扰动，所以想得到无标签自监督的标签<code>mask_w</code>很复杂，因此损失的计算并不简单。我们基于现有<a href="#canshu">参数</a>逐步分析：</p><p>首先，对于用来填充cutmix的数据<code>img_u_w_mix</code>，我们利用模型预测其分割结果<code>mask_u_w_mix</code>；同时，$x^w$也通过模型获得了<code>pred_u_w</code>(见<a href="#ED">2.2</a>)，我们同样可以获得<code>mask_u_w</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    pred_u_w_mix = model(img_u_w_mix).detach()</span><br><span class="line">    conf_u_w_mix = pred_u_w_mix.softmax(dim=<span class="number">1</span>).<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    mask_u_w_mix = pred_u_w_mix.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">pred_u_w = pred_u_w.detach()</span><br><span class="line">conf_u_w = pred_u_w.softmax(dim=<span class="number">1</span>).<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">mask_u_w = pred_u_w.argmax(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>由于我们知道了$s_1$和$s_2$<code>CutMix</code>框的位置，所以我们直接将上面的两个图像的mask结合，就可以得到自监督label：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于cutmix框不一样，这里分别获得cutmixed1和cutmixed2的mask及其conf等</span></span><br><span class="line">mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = \</span><br><span class="line">    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()</span><br><span class="line">mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = \</span><br><span class="line">    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()</span><br><span class="line"></span><br><span class="line">mask_u_w_cutmixed1[cutmix_box1 == <span class="number">1</span>] = mask_u_w_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line">conf_u_w_cutmixed1[cutmix_box1 == <span class="number">1</span>] = conf_u_w_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line">ignore_mask_cutmixed1[cutmix_box1 == <span class="number">1</span>] = ignore_mask_mix[cutmix_box1 == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">mask_u_w_cutmixed2[cutmix_box2 == <span class="number">1</span>] = mask_u_w_mix[cutmix_box2 == <span class="number">1</span>]</span><br><span class="line">conf_u_w_cutmixed2[cutmix_box2 == <span class="number">1</span>] = conf_u_w_mix[cutmix_box2 == <span class="number">1</span>]</span><br><span class="line">ignore_mask_cutmixed2[cutmix_box2 == <span class="number">1</span>] = ignore_mask_mix[cutmix_box2 == <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><p>最后，<strong>我们给出4个损失</strong>：</p><p>第一个损失：有监督损失。使用img_x的<a href="#pred_x">预测结果</a><code>pred_x</code>和标签<code>mask_x</code>计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_x = criterion_l(pred_x, mask_x)</span><br></pre></td></tr></table></figure></p><p>第二&amp;三个损失：图像层面自监督损失。通过$s_1$和$s_2$的预测结果及其对应label计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)</span><br><span class="line">loss_u_s1 = loss_u_s1 * (</span><br><span class="line">    (conf_u_w_cutmixed1 &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask_cutmixed1 != <span class="number">255</span>))</span><br><span class="line">loss_u_s1 = loss_u_s1.<span class="built_in">sum</span>() / (ignore_mask_cutmixed1 != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)</span><br><span class="line">loss_u_s2 = loss_u_s2 * (</span><br><span class="line">    (conf_u_w_cutmixed2 &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask_cutmixed2 != <span class="number">255</span>))</span><br><span class="line">loss_u_s2 = loss_u_s2.<span class="built_in">sum</span>() / (ignore_mask_cutmixed2 != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br></pre></td></tr></table></figure></p><p>第四个损失：特征层面的自监督损失。通过$x^{fp}$的<a href="#pred_x">预测结果</a><code>pred_u_w_fp</code>和$x^w$的预测结果计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss_u_w_fp = criterion_u(pred_u_w_fp, mask_u_w)</span><br><span class="line">loss_u_w_fp = loss_u_w_fp * ((conf_u_w &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]) &amp; (ignore_mask != <span class="number">255</span>))</span><br><span class="line">loss_u_w_fp = loss_u_w_fp.<span class="built_in">sum</span>() / (ignore_mask != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br></pre></td></tr></table></figure></p><p><strong>Total</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = (loss_x + loss_u_s1 * <span class="number">0.25</span> + loss_u_s2 * <span class="number">0.25</span> + loss_u_w_fp * <span class="number">0.5</span>) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure></p><h3 id="3-模型结构解析"><a href="#3-模型结构解析" class="headerlink" title="3. 模型结构解析"></a>3. 模型结构解析</h3><p>模型结构在本文中显然是Encoder-Decoder架构，具体而言，我们进行如下分析：</p><h4 id="3-1-Encoder"><a href="#3-1-Encoder" class="headerlink" title="3.1 Encoder"></a>3.1 Encoder</h4><p>该论文的Encoder设置为ResNet和xception，为了便于讨论，只以ResNet101为例。这里不再展示模型的结构，直接看它的<code>forward</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">base_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.conv1(x)       <span class="comment"># (3, 224, 224) =&gt; (128, 112, 112)</span></span><br><span class="line">    x = self.bn1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.maxpool(x)     <span class="comment"># (128, 112, 112) =&gt; (128, 56, 56)</span></span><br><span class="line"></span><br><span class="line">    c1 = self.layer1(x)     <span class="comment"># 3个Bottleneck, (128, 56， 56) =&gt; (64*4, 56, 56)</span></span><br><span class="line">    c2 = self.layer2(c1)    <span class="comment"># 4个Bottleneck, stride=2, (256, 56, 56) =&gt; (128*4, 28, 28)</span></span><br><span class="line">    c3 = self.layer3(c2)    <span class="comment"># 23个Bottleneck, stride=2, (512, 28, 28) =&gt; (256*4, 14, 14)</span></span><br><span class="line">    c4 = self.layer4(c3)    <span class="comment"># 3个Bottleneck, stride=2, (1024, 14, 14) =&gt; (512*4, 7, 7)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c1, c2, c3, c4</span><br></pre></td></tr></table></figure><br>我们以一张大小为<code>(3,224,224)</code>的图片为例，相关提示已经在上面的注释中。通过resnet，我们已得到两种视角的特征：<code>c1</code>和<code>c4</code>。</p><h4 id="3-2-Decoder"><a href="#3-2-Decoder" class="headerlink" title="3.2 Decoder"></a>3.2 Decoder</h4><p>首先介绍Decoder的一个模块<code>ASPPModule</code>，由<code>ASPPConv</code>和<code>ASPPPooling</code>等组合而成。</p><p><code>ASPPConv</code>引入了空洞卷积，其维度计算公式为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">H_out = (H_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1</span><br><span class="line">W_out = (W_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1</span><br></pre></td></tr></table></figure><br><img src="https://picx.zhimg.com/70/v2-a08645e392a6a5cb49e271e5310f0dd8_1440w.awebp?source=172ae18b&amp;biz_tag=Post" alt><br>其实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ASPPConv</span>(<span class="params">in_channels, out_channels, atrous_rate</span>):</span><br><span class="line">    block = nn.Sequential(nn.Conv2d(in_channels, out_channels, <span class="number">3</span>, padding=atrous_rate,</span><br><span class="line">                                    dilation=atrous_rate, bias=<span class="literal">False</span>),</span><br><span class="line">                          nn.BatchNorm2d(out_channels),</span><br><span class="line">                          nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> block</span><br></pre></td></tr></table></figure><br><code>ASPPPooling</code>的代码为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ASPPPooling</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPPPooling, self).__init__()</span><br><span class="line">        self.gap = nn.Sequential(nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">                                 nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                 nn.BatchNorm2d(out_channels),</span><br><span class="line">                                 nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        h, w = x.shape[-<span class="number">2</span>:]</span><br><span class="line">        pool = self.gap(x)</span><br><span class="line">        <span class="keyword">return</span> F.interpolate(pool, (h, w), mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>举例而言，数据维度经过如下变化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">设 x.size = (<span class="number">2048</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line">|__nn.AdaptiveAvgPool2d(<span class="number">1</span>) =&gt;  (<span class="number">2048</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__nn.Conv2d(<span class="number">2048</span>, <span class="number">256</span>, <span class="number">1</span>, bias=<span class="literal">False</span>) =&gt; (<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__nn.BatchNorm2d(out_channels) =&gt; (<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__nn.ReLU(<span class="literal">True</span>) =&gt; (<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">|__F.interpolate =&gt; (<span class="number">256</span>, <span class="number">7</span>, <span class="number">7</span>)     <span class="comment"># 插值</span></span><br></pre></td></tr></table></figure></p><p>最后，<code>ASPPModule</code>通过这些模块组合而成，代码为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ASPPModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, atrous_rates</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPPModule, self).__init__()</span><br><span class="line">        out_channels = in_channels // <span class="number">8</span></span><br><span class="line">        rate1, rate2, rate3 = atrous_rates</span><br><span class="line"></span><br><span class="line">        self.b0 = nn.Sequential(nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                nn.BatchNorm2d(out_channels),</span><br><span class="line">                                nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.b1 = ASPPConv(in_channels, out_channels, rate1)</span><br><span class="line">        self.b2 = ASPPConv(in_channels, out_channels, rate2)</span><br><span class="line">        self.b3 = ASPPConv(in_channels, out_channels, rate3)</span><br><span class="line">        self.b4 = ASPPPooling(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">        self.project = nn.Sequential(nn.Conv2d(<span class="number">5</span> * out_channels, out_channels,</span><br><span class="line">                                               <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                     nn.BatchNorm2d(out_channels),</span><br><span class="line">                                     nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        feat0 = self.b0(x)</span><br><span class="line">        feat1 = self.b1(x)</span><br><span class="line">        feat2 = self.b2(x)</span><br><span class="line">        feat3 = self.b3(x)</span><br><span class="line">        feat4 = self.b4(x)</span><br><span class="line">        y = torch.cat((feat0, feat1, feat2, feat3, feat4), <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.project(y)</span><br></pre></td></tr></table></figure><br>经过该模块的数据，例如<code>(2048,7,7)</code>，最终变为<code>(256, 7, 7)</code>。Decoder的具体实现见3.3.</p><h4 id="3-3-Total-model"><a href="#3-3-Total-model" class="headerlink" title="3.3 Total model"></a>3.3 Total model</h4><p>我们将分析写成了注释，添加在下面的代码中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DeepLabV3Plus</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>):</span><br><span class="line">        <span class="built_in">super</span>(DeepLabV3Plus, self).__init__()</span><br><span class="line">        <span class="comment"># Encoder,默认为resnet101</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;resnet&#x27;</span> <span class="keyword">in</span> cfg[<span class="string">&#x27;backbone&#x27;</span>]:</span><br><span class="line">            self.backbone = resnet.__dict__[cfg[<span class="string">&#x27;backbone&#x27;</span>]](</span><br><span class="line">                pretrained=<span class="literal">True</span>, </span><br><span class="line">                replace_stride_with_dilation=cfg[<span class="string">&#x27;replace_stride_with_dilation&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> cfg[<span class="string">&#x27;backbone&#x27;</span>] == <span class="string">&#x27;xception&#x27;</span></span><br><span class="line">            self.backbone = xception(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        low_channels = <span class="number">256</span>      <span class="comment"># 经过self.head后的通道数,即ASPPModule.out_channels</span></span><br><span class="line">        high_channels = <span class="number">2048</span>    <span class="comment"># 经过resnet后的c4的通道数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># cfg[&#x27;dilations&#x27;] = [6, 12, 18],提供了不同感受野的信息,并经过融合。</span></span><br><span class="line">        self.head = ASPPModule(high_channels, cfg[<span class="string">&#x27;dilations&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 减少c1的通道数，256 =&gt; 48, 浓缩信息+减少参数量</span></span><br><span class="line">        self.reduce = nn.Sequential(nn.Conv2d(low_channels, <span class="number">48</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                    nn.BatchNorm2d(<span class="number">48</span>),</span><br><span class="line">                                    nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 融合c1(低级信息)和c4(高级信息)的信息,强制输出维度为256</span></span><br><span class="line">        self.fuse = nn.Sequential(nn.Conv2d(high_channels // <span class="number">8</span> + <span class="number">48</span>, <span class="number">256</span>, <span class="number">3</span>, </span><br><span class="line">                                            padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                  nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">                                  nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">                                  nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                  nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">                                  nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">        self.classifier = nn.Conv2d(<span class="number">256</span>, cfg[<span class="string">&#x27;nclass&#x27;</span>], <span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, need_fp=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        设x.size = (2,3,224,224), need_fp=True,模拟将s1,s2这2个数据cat后进入模型</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h, w = x.shape[-<span class="number">2</span>:]     <span class="comment"># h=224,w=224</span></span><br><span class="line"></span><br><span class="line">        feats = self.backbone.base_forward(x)   <span class="comment"># 获得4种特征图c1~c4</span></span><br><span class="line">        c1, c4 = feats[<span class="number">0</span>], feats[-<span class="number">1</span>]            <span class="comment"># c1=(2,256,56,56),c4=(2,2048,7,7)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> need_fp:</span><br><span class="line">            <span class="comment"># 这里nn.Dropout2d(0.5)就是构建fp</span></span><br><span class="line">            <span class="comment"># 这里做了一些拼接,例如</span></span><br><span class="line">            <span class="comment"># torch.cat((c1, nn.Dropout2d(0.5)(c1))).size = (4,256,56,56)</span></span><br><span class="line">            <span class="comment"># torch.cat((c4, nn.Dropout2d(0.5)(c4))).size = (4,2048,7,7)</span></span><br><span class="line">            outs = self._decode(torch.cat((c1, nn.Dropout2d(<span class="number">0.5</span>)(c1))),</span><br><span class="line">                                torch.cat((c4, nn.Dropout2d(<span class="number">0.5</span>)(c4))))</span><br><span class="line">            <span class="comment"># outs.size = (4,21,56,56),注意这是因为self.classifier将channel设为class数21</span></span><br><span class="line">            outs = F.interpolate(outs, size=(h, w), mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># out和out_fp分别代表x_w和经过特征扰动后的fp的输出</span></span><br><span class="line">            out, out_fp = outs.chunk(<span class="number">2</span>)    </span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> out, out_fp</span><br><span class="line"></span><br><span class="line">        out = self._decode(c1, c4)      <span class="comment"># out.size = (2,21,56,56)</span></span><br><span class="line">        out = F.interpolate(out, size=(h, w), mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># # out.size = (2,21,224,224)</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_decode</span>(<span class="params">self, c1, c4</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 设c1.size=(256,56,56), c4.size=(2048,7,7), 详见resnet &quot;&quot;&quot;</span></span><br><span class="line">        c4 = self.head(c4)      <span class="comment"># c4: (2048,7,7) =&gt; (256,7,7)</span></span><br><span class="line">        <span class="comment"># c4: (256,7,7) =&gt; (256,56,56)</span></span><br><span class="line">        c4 = F.interpolate(c4, size=c1.shape[-<span class="number">2</span>:], mode=<span class="string">&quot;bilinear&quot;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        c1 = self.reduce(c1)    <span class="comment"># c1: (256,56,56) =&gt; (48,56,56)</span></span><br><span class="line"></span><br><span class="line">        feature = torch.cat([c1, c4], dim=<span class="number">1</span>)    <span class="comment"># feature.size=(256+48,56,56)</span></span><br><span class="line">        feature = self.fuse(feature)            <span class="comment"># =&gt; feature.size=(256,56,56)</span></span><br><span class="line"></span><br><span class="line">        out = self.classifier(feature)          <span class="comment"># 像素级分类，out=(21,56,56)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="4-复现结果"><a href="#4-复现结果" class="headerlink" title="4. 复现结果"></a>4. 复现结果</h3><p>由于时间仓促，目前只复现了backbone为<code>ResNet101</code>在数据集<code>Pascal</code>上的表现，如下表所示:</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Pascal</strong> / UniMatch &#124; ResNet101</th><th style="text-align:center">92</th><th style="text-align:center">183</th><th style="text-align:center">366</th><th style="text-align:center">732</th><th style="text-align:center">1464</th></tr></thead><tbody><tr><td style="text-align:center">Paper</td><td style="text-align:center"><strong>75.2</strong></td><td style="text-align:center"><strong>77.2</strong></td><td style="text-align:center"><strong>78.8</strong></td><td style="text-align:center"><strong>79.9</strong></td><td style="text-align:center"><strong>81.2</strong></td></tr><tr><td style="text-align:center">OurWork</td><td style="text-align:center"><strong>75.2</strong></td><td style="text-align:center">76.8</td><td style="text-align:center">78.5</td><td style="text-align:center">79.2</td><td style="text-align:center">80.8</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Pascal</strong> / UniMatch &#124; ResNet101</th><th style="text-align:center">1/16</th><th style="text-align:center">1/8</th><th style="text-align:center">1/4</th></tr></thead><tbody><tr><td style="text-align:center">Paper &#124; <font color="gray"><strong>321</strong></font></td><td style="text-align:center">76.5</td><td style="text-align:center">77.0</td><td style="text-align:center">77.2</td></tr><tr><td style="text-align:center">OurWork &#124; <font color="gray"><strong>321</strong></font></td><td style="text-align:center"><strong>76.6</strong></td><td style="text-align:center"><strong>77.4</strong></td><td style="text-align:center"><strong>77.4</strong></td></tr></tbody></table></div><p>我们的复现基本接近或者达到论文中的精度，证明有效。我们展示两张复现时的截图，可供参考：<br><img src="/2023/06/09/unimatch/366_101.png" alt></p><center>图8. resnet101 | pascal-366</center><p><img src="/2023/06/09/unimatch/732_101.png" alt></p><center>图8. resnet101 | pascal-732</center><h2 id="3-基于FlexMatch的改进"><a href="#3-基于FlexMatch的改进" class="headerlink" title="3. 基于FlexMatch的改进"></a>3. <a name="基于flexmatch的改进"></a>基于FlexMatch的改进</h2><p>由于作者认为<code>FixMatch</code>足够强大、足够简单，所以以其为baseline。我们尝试使用<code>FlexMatch</code>方法为baseline设计一个类似的<code>UniMatch</code>模型。<br><img src="/2023/06/09/unimatch/9.png" alt></p><center>图9. FlexMatch阈值调整方法</center><p><code>FlexMatch</code>方法，就是将下式固定的 $\tau$ 转化为可以动态调整的形式，但又不显示引入参数：</p><script type="math/tex; mode=display">\mathcal{L}_u = \frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \tau\right)\text{H}(\hat{q}_b, p_m(y|\mathcal{A}(u_b)))\tag{6}</script><p>这种动态调整方法被称为<code>Curriculum Pseudo Labeling (CPL)</code>方法。</p><p>FlexMatch认为一个<u><strong>类别预测的置信度越低，说明对该类的学习仍不够充分，应该降低阈值鼓励学习</strong></u>，即阈值和类别的学习效果有关。论文中使用预测属于该类且置信度大于阈值的无标签数据数量衡量一个类别的学习效果：<br><a id="tag7"></a></p><script type="math/tex; mode=display">\sigma_{t}(c) = \sum_{n=1}^{N}\Bbb{I}\left(\max(p_{m,t}(y|u_n))\geq \tau\right)\Bbb{I}\left(\arg \max(p_{m,t}(y|u_n)) = c \right)\tag{7}</script><p>&lt;/a&gt;</p><p>其中 $t$ 是指step t时刻。我们将上式得到的学习效果 $\sigma_{t}(c)$ 归一化，得到step t时每个类别的阈值：<br><a id="tag8"></a></p><script type="math/tex; mode=display">\beta_t(c) = \frac{\sigma_t(c)}{\max_{c} \sigma_t} \tag{8}</script><p>&lt;/a&gt;<br><a id="tag9"></a></p><script type="math/tex; mode=display">\mathcal{T}_t(c) = \beta_t(c)\cdot\tau\tag{9}</script><p>&lt;/a&gt;</p><p>实际上这个动态阈值$\mathcal{T}_t(c)$还会施加一个非线性函数：</p><script type="math/tex; mode=display">\mathcal{T}_t(c) = \mathcal{M}\left(\beta_t(c)\right)\cdot \tau\tag{10}</script><p>最后损失函数修改为：</p><script type="math/tex; mode=display">\mathcal{L}_{u,t} = \frac{1}{\mu B}\sum_{b=1}^{\mu B}\Bbb{I}\left(\max(q_b)\geq \mathcal{T}_t(\arg \max q_b) \right)\text{H}(\hat{q}_b, p_m(y|\mathcal{A}(u_b)))\tag{11}</script><h3 id="1-改进的代码"><a href="#1-改进的代码" class="headerlink" title="1. 改进的代码"></a>1. 改进的代码</h3><p>通过调研<code>TorchSSL</code>代码库，我们可以对<code>FlexMatch</code>方法有更清晰的认识。我们考虑在<code>fixmatch.py</code>和<code>unimatch.py</code>上修改代码，加入动态阈值。</p><h4 id="1-1-fixmatch-py"><a href="#1-1-fixmatch-py" class="headerlink" title="1.1 fixmatch.py"></a>1.1 fixmatch.py</h4><p>在每一轮开始之前，我们要预定义2个变量：</p><ul><li><code>selected_label</code>：一个存储分类情况的变量。在flexmatch的源码实现中，该参数将记录<strong>所有未标记图片</strong>的类别硬标签。但在语义分割任务中，一张图片的类别标签大小为<code>(W,H)</code>，一旦图像数量较大则会导致空间占用较多、运行速度变慢，所以这里采用一个<code>队列queue</code>来实现它。当队列已满时，将最早进入队列的batch移除，将新的batch移入。默认的队列长度<code>queue_length</code>为<code>batch_size</code>的100倍。</li><li><code>classwise_acc</code>：记录每个类别的学习情况，即公式(<a href="#tag7">7</a>)的$\sigma_t(c)$。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># selected_label.size = (N,W,H),记录每个像素的类别</span></span><br><span class="line"><span class="comment"># classwise_acc.size = (C,), 记录每个类别的准确率</span></span><br><span class="line">queue_length = cfg[<span class="string">&quot;batch_size&quot;</span>] * cfg[<span class="string">&quot;queue_num&quot;</span>]     <span class="comment"># 记录队列总长度</span></span><br><span class="line">head_length = cfg[<span class="string">&quot;batch_size&quot;</span>]                         <span class="comment"># 记录队列的头</span></span><br><span class="line">selected_label = torch.ones(</span><br><span class="line">    (queue_length, cfg[<span class="string">&quot;crop_size&quot;</span>], cfg[<span class="string">&quot;crop_size&quot;</span>]), dtype=torch.long</span><br><span class="line">    ) * cfg[<span class="string">&quot;nclass&quot;</span>]</span><br><span class="line">selected_label = selected_label.cuda().detach()</span><br><span class="line">classwise_acc = torch.zeros((cfg[<span class="string">&quot;nclass&quot;</span>],)).cuda()</span><br></pre></td></tr></table></figure><p>在每一个step计算loss之前，我们需要根据式(<a href="#tag8">8</a>)得到归一化值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pseudo_counter = torch.bincount(selected_label.reshape(-<span class="number">1</span>)) <span class="comment"># 各类别预测数量</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">max</span>(pseudo_counter) &lt; selected_label.shape[<span class="number">0</span>] * (cfg[<span class="string">&quot;crop_size&quot;</span>] ** <span class="number">2</span>):</span><br><span class="line">    classwise_acc = pseudo_counter[:cfg[<span class="string">&quot;nclass&quot;</span>]] / <span class="built_in">max</span>(pseudo_counter)</span><br></pre></td></tr></table></figure><br>接着我们根据式(<a href="#tag9">9</a>,10)可以得到动态阈值，并以此计算loss：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># u_w_cutmixed_thresh为动态阈值</span></span><br><span class="line">u_w_cutmixed_thresh = torch.nn.functional.one_hot(</span><br><span class="line">    mask_u_w_cutmixed, num_classes=cfg[<span class="string">&quot;nclass&quot;</span>]).to(torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># u_w_cutmixed_thresh.size = (B,W,H,C)</span></span><br><span class="line">u_w_cutmixed_thresh =  torch.matmul(u_w_cutmixed_thresh, classwise_acc)  <span class="comment"># size=(B,W,H)</span></span><br><span class="line">u_w_cutmixed_thresh = <span class="number">0.95</span> * u_w_cutmixed_thresh / (<span class="number">2.</span> - u_w_cutmixed_thresh)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mask_u_w_cutmixed是硬标签(B,W,H)，pred_u_s是软标签(B,C,W,H)</span></span><br><span class="line">loss_u_s = criterion_u(pred_u_s, mask_u_w_cutmixed)     </span><br><span class="line">loss_u_s = loss_u_s * ((conf_u_w_cutmixed - u_w_cutmixed_thresh &gt;= <span class="number">0</span>) &amp; \</span><br><span class="line">                                       (ignore_mask_cutmixed != <span class="number">255</span>))</span><br><span class="line">loss_u_s = loss_u_s.<span class="built_in">sum</span>() / (ignore_mask_cutmixed != <span class="number">255</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新selected_label</span></span><br><span class="line">select = (conf_u_w_cutmixed &gt;= cfg[<span class="string">&#x27;conf_thresh&#x27;</span>]).long()</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(select.shape[<span class="number">0</span>]):</span><br><span class="line">    selected_label[head_length-cfg[<span class="string">&quot;batch_size&quot;</span>]:head_length][k][select[k] == <span class="number">1</span>] =\</span><br><span class="line">                                                mask_u_w_cutmixed[k][select[k] == <span class="number">1</span>]</span><br></pre></td></tr></table></figure><br>一个batch的阈值矩阵大小为<code>(B,W,H)</code>，我们使用one-hot编码使其可以直接与<code>classwise_acc</code>相乘。在计算完loss之后，我们要更新<code>selected_label</code>(记录队列中图像的语义分割标签)，以供下一个step使用。一般我们只需要更新队列末尾的那个batch即可。</p><p>最后，我们需要代码来完成队列的push和pop，以实现动态的变化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">new_batch_data = cfg[<span class="string">&quot;nclass&quot;</span>] * torch.ones(</span><br><span class="line">    (cfg[<span class="string">&quot;batch_size&quot;</span>], cfg[<span class="string">&quot;crop_size&quot;</span>], cfg[<span class="string">&quot;crop_size&quot;</span>]), dtype=torch.long).cuda()</span><br><span class="line"><span class="keyword">if</span> head_length &lt; queue_length:</span><br><span class="line">    head_length += cfg[<span class="string">&quot;batch_size&quot;</span>]    <span class="comment"># 添加新数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    selected_label[:-cfg[<span class="string">&quot;batch_size&quot;</span>]] = selected_label.clone()[cfg[<span class="string">&quot;batch_size&quot;</span>]:]</span><br><span class="line">    selected_label[-cfg[<span class="string">&quot;batch_size&quot;</span>]:] = new_batch_data</span><br></pre></td></tr></table></figure></p><h3 id="2-简单的实验验证"><a href="#2-简单的实验验证" class="headerlink" title="2. 简单的实验验证"></a>2. 简单的实验验证</h3><p>由于时间比较局促，目前只验证了<code>model=ResNet101</code>，<code>dataset=Pascal</code>中的部分实验：</p><p><strong>实验1：在crop=321的情况下：</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Pascal</strong> / ResNet101</th><th style="text-align:center">92</th><th style="text-align:center">183</th><th style="text-align:center">366</th><th style="text-align:center">732</th><th style="text-align:center">1464</th></tr></thead><tbody><tr><td style="text-align:center">Paper - <strong>UniMatch</strong></td><td style="text-align:center"><strong>75.2</strong></td><td style="text-align:center"><strong>77.2</strong></td><td style="text-align:center"><strong>78.8</strong></td><td style="text-align:center"><strong>79.9</strong></td><td style="text-align:center"><strong>81.2</strong></td></tr><tr><td style="text-align:center">OurWork - <strong>FlexUniMatch</strong></td><td style="text-align:center">/</td><td style="text-align:center">76.8</td><td style="text-align:center">76.2</td><td style="text-align:center">78.5</td><td style="text-align:center">/</td></tr></tbody></table></div><p><strong>实验2：FixMatch vs. FlexMatch</strong></p><p>目前只测试了一组数据，在<code>pascal-crop_321-732-resnet101</code>的设置下，结果为<code>76.79</code>，与<code>FixMatch</code>的对应值<code>77.8</code>还有不小的差距。</p><h3 id="3-一些实验感悟与未来探讨"><a href="#3-一些实验感悟与未来探讨" class="headerlink" title="3. 一些实验感悟与未来探讨"></a>3. 一些实验感悟与未来探讨</h3><p>很遗憾，在实验中并没有把FlexMatch方法做到SOTA。我对造成这个问题的原因的进行了简单分析：</p><ol><li>我们为了提高效率选择了使用一个队列，损失了较多数据的信息，队列中的类别可能不能反映整体数据分布；</li><li>我们没有调整任何超参数(即，和<code>UniMatch</code>完全一致)，可能会导致lr等不合适的情况；</li><li>尽管<code>FlexMatch</code>并没有显式增加参数，但由于对于动态阈值调整的变量$\beta_t(c)$涉及到<strong>全部数据的类别信息</strong>、以及非线性函数的选择，在语义分割的像素级分类上应用并不简单。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 夏令营经历 </category>
          
          <category> Semantic segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> papers reproduced </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Contrastive Learning based Vision-Language Pre-Training</title>
      <link href="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/"/>
      <url>/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/</url>
      
        <content type="html"><![CDATA[<center><h1>基于对比学习的多模态预训练方法</h1></center><center><h2>——以Vision-Language PTMs为例<h2></h2></h2></center><blockquote><p>前记：这篇文章是我在面试中科大毛震东老师组时写的一份报告，整理一下2023年之前基于对比学习的多模态预训练模型文献。算是给我的<code>Image-to-Poem</code>项目做的一个综述性调查，给自己一个方向。</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">随着大量图像-文本对数据集的涌现，使用对比学习进行多模态模型预训练的方法也愈发成熟。</span><br><span class="line">在本文中，我们首先介绍了图像-文本对比学习任务(ITC)，</span><br><span class="line">接着按照时间线回顾了CLIP、ALBEF、BLIP&amp;BLIP2三个重要的多模态预训练模型。</span><br><span class="line">最后，鉴于多模态预训练模型强大的性能，我们尝试了Image-to-Poem的特殊字幕生成任务。</span><br></pre></td></tr></table></figure><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a><em>1. Introduction</em></h2><p>最近的工作证明，使用互联网中的大量图像-文本对进行多模态模型预训练可以显著提升模型的表征提取能力和泛化性能，但也对图像表征(Representation)和文本表征的融合提出了挑战。如何将图像表征与文本表征进行匹配是重要的问题。</p><p>基本地，这里将多模态模型预训练过程分为3个阶段：</p><ol><li>将图像和文本分别编码为保留语义的表示向量(latent representations)；</li><li>设计一个模型或结构，来模拟两种模式的交融；</li><li>设计有效的预训练任务来预训练Vision-Language Pre-Trained Model.</li></ol><p>图像-文本对比学习(ITC)作为一种有效的预训练任务，欲将成对的图像-文本对的表示向量尽可能地拉近，而将不成对的负例样本对(negatives)的表示向量尽可能地远离。一般地，对比学习的损失函数(image-to-text为例)可表示为如下形式：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{i2t}} = -\Bbb{E}_{(W,V)\in \mathcal{D}}\left[\log\frac{s_\theta (\pmb{h}_v, \pmb{h}_w)}{\sum_{W'} s_\theta (\pmb{h}_v, \pmb{h}_{w'})} \right] \tag{1}</script><p>其中，$(W, V)$是图像-文本对数据集的一个样本，是关于$V$的负样本，$\textit{\pmb{h}}$是图像或文本的表征向量。在后续对多模态的各个预训练模型的调研中，可以发现上述对比学习思想和损失函数很少被改变；但是为了提升ITC任务的有效性，很多方法被引入，例如：正负样本对的定义、ITC任务的出现时机(如align before fusing)、Momentum Distillation等，这将在后续章节详细讨论。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/1.png" alt="图1. 多模态模型结构分类"></p><center>图1. 多模态模型结构分类</center><p>考虑到多模态模型结构具有多样性，需要对后续讨论的模型作一定的限制。根据主流分类，多模态模型分为单流(single-stream)和多流(dual-stream)两类，如图1所示。单流模型通常直接将图像编码向量和文本编码向量<strong>直接拼接</strong>，再接入融合编码器(通常是transformer结构)训练。显然，此类结构不适合引入ITC任务进行预训练，故将其剔除。</p><h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2 Approach"></a><em>2 Approach</em></h2><h3 id="2-1-CLIP-Contrastive-Language-Image-Pre-training"><a href="#2-1-CLIP-Contrastive-Language-Image-Pre-training" class="headerlink" title="2.1 CLIP - Contrastive Language-Image Pre-training"></a>2.1 CLIP - Contrastive Language-Image Pre-training</h3><p>我们首先介绍CLIP模型，是因为其是第一个使用对比学习而将zero-shot分类任务做到先进性能的。CLIP模型借用自然语言处理领域中使用自回归方式进行无监督训练的思想，意图用大量的文本监督信号训练视觉模型。</p><p>具体而言，CLIP模型的对比学习正负样本对定义比较简单：对于一个包含N个图像-文本对的batch数据，正样本为每张图像及其对应的文本(N个)，而其他任意的图像-文本组合都作为负样本(N(N-1)个)。训练方法如图2所示。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/2.png" alt="图2. CLIP模型预训练方法(图左)和zero-shot方法(图右)"></p><center>图2. CLIP模型预训练方法(图左)和zero-shot方法(图右)</center><p>从对比学习算法角度，模型通过几下几步进行训练：</p><ol><li>模态内编码：文本通过Text-Encoder编码为文本表示向量，图像通过Image-Encoder编码为图像表示向量。</li><li>模态间融合：即文本表示向量和图像表示向量的点积，计算相似度。</li><li>对比损失计算：这里使用CrossEntropy Loss作为损失。</li></ol><p>从上述简洁的训练步骤，亦可看出在数据规模足够大的前提下，对比学习是非常有潜力的。除了性能上的优异，对比学习给大模型预训练还带来了以下几个优势：</p><ol><li>训练效率的提升。以往工作表明，在ImageNet上训练一个大模型(<code>ResNeXt101-32x48d</code>)需要大量训练资源，像在4亿图文数据的开放视觉识别任务上，效率更是非常重要。与图像生成对应文本的预训练任务相比，对比学习能够提升至少4倍的效率。</li><li>Zero-shot能力的展现。对比学习通过拉近相关的图像和文本编码，从而使得每张图片总能找到最佳匹配的类别标签，并且不会像监督学习那样受到类别标签的限制。</li></ol><p>尽管CLIP模型的训练方法使得其在分类任务、图文检索任务上有出色的表现，但由于模态间的融合有限(仅仅是相似度计算和对比学习)，很难在QA或者生成任务上有比较好的性能。</p><h3 id="2-2-ALBEF-ALign-the-image-and-text-representation-BEfore-Fusing"><a href="#2-2-ALBEF-ALign-the-image-and-text-representation-BEfore-Fusing" class="headerlink" title="2.2 ALBEF - ALign the image and text representation BEfore Fusing"></a>2.2 ALBEF - ALign the image and text representation BEfore Fusing</h3><p>ALBEF模型的研究动机是极其具有价值的，并且给基于对比学习的多模态预训练做了一个有效过渡，成为了一个新的范式。具体而言，该模型从模型和数据2个角度做了改进：</p><ol><li>模型结构层面：指出利用目标检测器进行区域图像特征抽取的办法，由于文本embedding和图像embedding并未对齐而使得模态融合成为挑战；而利用巨大数据集进行对比学习的方法(如CLIP)也因融合不足而无法应用在更多任务上。</li><li>数据层面：web收集的图像-文本对含有大量噪声，直接进行对比学习可能会过拟合噪声样本。</li></ol><p>图3展示了ALBEF模型的结构。和CLIP模型相似，该模型引入12层的<code>ViT-B16</code>结构作为图像编码器，并引入BERT结构作为文本编码器。不同的是，BERT被拆为了前六层的文本编码器和后六层的multimodal融合器，既体现了融合编码器的重要性，也体现了图像编码器更为重要的实践结论。ALBEF的预训练包含3个任务，因为篇幅限制，我们重点讨论ITC任务。</p><p>首先讨论ITC任务的执行时间：align before fusing。这使得单模态编码器提前学习到低维单模态表示，进而实现更容易的模态融合。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/3.png" alt="图3. ALBEF模型架构"></p><center>图3. ALBEF模型架构</center><p>其次讨论ITC任务的正负样本对定义。借鉴MOCO论文的思想，作者将对比学习看作构建动态字典。以image-to-text角度为例，首先构建一个队列queue用以存放text数据(即key)，然后每一个image数据(即query)都进行字典查找：query总与匹配的key相似(正样本对)，而与其他key相异(负样本对)。在实际的ITC任务中，query是用image-Encoder编码的图像向量，而key则是用一个相同或相近的Encoder(被称为Momentum Model)编码的文本向量。最终的对比损失为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{itc}} = \frac{1}{2}\Bbb{E}_{(I,T)\sim \mathcal{D}}\left[H\left(\pmb{y}^{\text{i2t}}(I), \pmb{p}^{\text{i2t}}(I) \right), H\left(\pmb{y}^{\text{t2i}}(T), \pmb{p}^{\text{t2i}}(T) \right) \right]\tag{2}</script><p>最后我们讨论Momentum Distillation。用于预训练的图像-文本对通常是noisy的：正样本对很可能是弱相关的，而负样本对也可能相互匹配。那么简单地使用对比学习的one-hot label(正负2个类别)将会惩罚所有的负样本对，忽略那些更好的描述文本。因此引入soft label是必要的，具体地，利用Momentum Model生成图像-文本相似度(即soft label)作为伪标签，和ITC任务生成的图像-文本相似度计算KL散度，衡量两者的相似性。可以看出，动量蒸馏鼓励模型捕获那些稳定的语义信息表示，并最大化那些具有相似语义的图像和文本的互信息。加入Momentum Distillation(MoD)后，对比损失更新为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{itc}}^{\text{mod}} = (1-\alpha)\mathcal{L}_{\text{itc}} + \frac{\alpha}{2}\Bbb{E}_{(I,T)\sim \mathcal{D}}\left[\text{KL}\left(\pmb{q}^{\text{i2t}}(I) \Vert \pmb{p}^{\text{i2t}}(I) \right), H\left(\pmb{q}^{\text{t2i}}(T)\Vert \pmb{p}^{\text{t2i}}(T) \right) \right]\tag{3}</script><h3 id="2-3-BLIP-–-Bootstrapping-Language-Image-Pre-training"><a href="#2-3-BLIP-–-Bootstrapping-Language-Image-Pre-training" class="headerlink" title="2.3 BLIP – Bootstrapping Language-Image Pre-training"></a>2.3 BLIP – Bootstrapping Language-Image Pre-training</h3><p>BLIP系列模型设计的初衷，是实现统一的视觉语言理解和生成预训练，以弥补现有模型(Encoder-Based &amp; Encoder-Decoder)的不足。</p><p><figure class="half">    <img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/4.png" width="550"></figure></p><center>图4. BLIP模型结构(上)</center><p>对于BLIP模型(图4左)，它在ALBEF的基础上加入了权值共享，并将MLM任务替换为LM任务以加强模型生成的性能。在ITC任务上，其依旧应用了Momentum Model，以保证文本和视觉特征空间的对齐。此外，论文提出的CapFilt方法不仅可以提高数据集的质量，还可以增加数据集数量，大幅增强了预训练模型的性能，成为了多模态数据处理的新范式。</p><p><figure class="half">    <img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/4-2.png" width="400"></figure></p><center>图4. BLIP-2模型结构(下)</center><p>BLIP-2模型(图4右)期望使用图像和文本单模态里的先进的大规模预训练模型来提供高质量的单模态特征，并保证计算效率足够高。于是它们冻结了Image Encoder和LLM，通过两阶段的预训练步骤取得了先进结果：</p><ol><li>使用轻量的模块Q-Former从image Encoder中捕捉包含丰富文本信息的视觉特征；</li><li>使用冻结的LLM进行语言生成任务。</li></ol><p>有趣的是，Q-Former结构并不直接对图像特征和文本特征进行对比学习，而是定义了一组<code>learned query</code>(如图5所示)，在和Image Encoder的编码向量进行cross-attention后，与文本向量进行对比学习。这被认为是在提取与文本信息高度对应的视觉表示。同时为防止信息泄露，文本信息和query经过了<code>Uni-modal self-attention mask</code>。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/5.png" alt="图5. BLIP2模型的Q-Former示意图"></p><center>图5. BLIP2模型的Q-Former示意图</center><p>最后，BLIP2没有延续Momentum Distillation，这是因为冻结的Image Encoder无需反向传播(即Encoder无需变化)，无需动量变化，而且节省了GPU的容量，可以容纳更多的样本。因此in-batch的负样本就已经足够。关于BLIP2中有趣细节因篇幅原因不再展示。</p><h2 id="3-Our-Work"><a href="#3-Our-Work" class="headerlink" title="3 Our Work"></a>3 Our Work</h2><p>基于对比学习的多模态预训练模型因为捕获了丰富的多模态表征，展现出了强大性能，使得Image captioning成为可能。我们尝试进行图像生成古诗，一种特殊的字幕生成任务。该项目(Image2Poem)已在近期开源至：<a href="https://github.com/weiji-Feng/Image2Poem。">https://github.com/weiji-Feng/Image2Poem。</a></p><blockquote><p>如果你希望了解项目细节(可能性不大)，你可以点击上面的github链接，也可以跳转到我的另一篇博客<br><strong><a href="https://weiji-feng.github.io/2023/05/23/Image2Poem/">&lt;暗格&gt;Image-to-Poem</a></strong>.</p></blockquote><h3 id="3-1-Pre-training-Datasets"><a href="#3-1-Pre-training-Datasets" class="headerlink" title="3.1 Pre-training Datasets"></a>3.1 Pre-training Datasets</h3><p>根据现有资源，我们搜集了109727首来自各时期的绝句，并给出每首古诗的关键词。我们将数据保存为.json格式，每个样本的形式如下所示：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>  </span><br><span class="line">    <span class="attr">&quot;dynasty&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Tang&quot;</span><span class="punctuation">,</span>  </span><br><span class="line">    <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="string">&quot;王维&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;清浅白沙滩|绿蒲尚堪把|家住水东西|浣纱明月下&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;白石滩&quot;</span><span class="punctuation">,</span>   </span><br><span class="line">    <span class="attr">&quot;keywords&quot;</span><span class="punctuation">:</span> <span class="string">&quot;清浅 明月 东西 白沙&quot;</span>  </span><br><span class="line"><span class="punctuation">&#125;</span> </span><br></pre></td></tr></table></figure><br>我们筛选了数据集中的关键词，组成了6000余个核心关键词集合。</p><h3 id="3-2-Model-Architecture"><a href="#3-2-Model-Architecture" class="headerlink" title="3.2 Model Architecture"></a>3.2 Model Architecture</h3><p>由于图像-古诗对数据的匮乏，我们的初代版本使用两阶段的生成方式。</p><p>如图6所示，我们通过两个步骤进行推理：</p><ol><li>CLIP编码：我们首先将关键词集合通过text encoder进行编码，保存以供多次使用。其次，将感兴趣的图像通过image encoder编码，获得图像embedding。我们比较图像embedding和所有关键词embedding的相似度，选择top-k个关键词作为古诗生成的prefix。</li><li>Decoder生成古诗：利用BERT tokenizer对prefix进行分词，进而通过预训练的Decoder模型生成古诗。</li></ol><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/6.png" alt></p><center>图6. CLIP + Decoder的Image2Poem结构</center><p>我们预训练了2个版本的生成式模型：<code>GPT2</code>(Decoder型)和<code>T5</code>(Encoder-Decoder型)。我们将keywords作为模型的输入，期望模型生成符合要求的古诗。训练期间，我们加入一定的MLM策略：对诗中出现在关键词中的字，我们采用15%的概率进行mask掩码，期望让模型学会在古诗生成中包含关键词。</p><h3 id="3-3-Model-Improvement"><a href="#3-3-Model-Improvement" class="headerlink" title="3.3 Model Improvement"></a>3.3 Model Improvement</h3><p>要做模型的改进，我们认为核心是收集高质量图像-古诗对数据集。我们准备进行如下两阶段的数据集获取：</p><ol><li>利用预训练的BLIP2/Chinese-CLIP进行图文检索，获取古诗的对应相关图像；</li><li>对于匹配度不高的图像-古诗对数据，我们考虑Stable Diffusion生成的方式。</li></ol><p>拥有数据后，我们使用图7的结构进行端到端的预训练方式。具体而言：我们冻结预训练的CLIP模型参数和GPT2模型参数，只训练transformer-based的Mapping Network。借鉴BLIP2的思想，我们期望Mapping Network可以对齐图像编码空间和GPT2的文本空间。与BLIP2类似，我们设计了一个固定的learned queries，与CLIP图像编码器的输出进行融合(使用concatenate或者cross-attention)，再将输出作为prefix embedding提供给GPT2模型。</p><p><img src="/2023/06/04/Contrastive-Learning-based-Vision-Language-Pre-Training/7.png" alt></p><p><center>图7. 改进的Image2Poem结构</center><br>由于还在进行图像-古诗数据的检索，还没能对改进的结构进行测试，但我们相信这个改进是有意义的。</p>]]></content>
      
      
      <categories>
          
          <category> multi-modal </category>
          
          <category> 夏令营经历 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Large-Language-Model For Math</title>
      <link href="/2023/06/03/Survey4MATH/"/>
      <url>/2023/06/03/Survey4MATH/</url>
      
        <content type="html"><![CDATA[<h1 id="Large-Language-Model-For-Math"><a href="#Large-Language-Model-For-Math" class="headerlink" title="Large-Language-Model For Math"></a>Large-Language-Model For Math</h1><p>让LLM大模型解决数学问题！ — #TODO</p><p>由于最近在做相关方向的科研，将阅读的论文整理在这里。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">💡 阅读记录格式:</span><br><span class="line"></span><br><span class="line">### Pretraining</span><br><span class="line"></span><br><span class="line">1. which foundation models are based on?</span><br><span class="line">2. what tokenizers are adopted?</span><br><span class="line">3. which datasets are collected specific for &quot;math&quot;?</span><br><span class="line">4. what types of pre-processing methods are introduced?</span><br><span class="line">5. other information that  you think is important</span><br><span class="line"></span><br><span class="line">### Fine-tuning</span><br><span class="line"></span><br><span class="line">1. which datasets are used?</span><br><span class="line">2. what types of pre-processing methods are used?</span><br><span class="line"></span><br><span class="line">### Evaluation</span><br><span class="line"></span><br><span class="line">1. which datasets are used?</span><br><span class="line">2. what type of pre-processing methods are used?</span><br><span class="line">3. what evaluation metrics are used?</span><br></pre></td></tr></table></figure><h3 id="阅读的论文列表："><a href="#阅读的论文列表：" class="headerlink" title="阅读的论文列表："></a>阅读的论文列表：</h3><p><a href="https://www.notion.so/Training-Verifiers-to-Solve-Math-Word-Problems-db6822f1cf9b45ad960b1dbb574ab4b8">Training Verifiers to Solve Math Word Problems</a></p><p><a href="https://www.notion.so/Solving-Quantitative-Reasoning-Problems-with-Language-Models-19b339fda58246fbadb22166a78b6ffd">Solving Quantitative Reasoning Problems with Language Models</a></p><p><a href="https://www.notion.so/MathPrompter-Mathematical-Reasoning-using-Large-Language-Models-92db0f041ac94c53884e799948af207d">MathPrompter: Mathematical Reasoning using Large Language Models</a></p><p><a href="https://www.notion.so/PAL-Program-aided-Language-Models-ddaecc337f414b8ea37985999bdec23c">PAL: Program-aided Language Models</a></p><p><a href="https://www.notion.so/Specializing-Smaller-Language-Models-towards-Multi-Step-Reasoning-6e8af05836f243058cc8d2374162c2c6">Specializing Smaller Language Models towards Multi-Step Reasoning</a></p>]]></content>
      
      
      <categories>
          
          <category> llm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image2Poem</title>
      <link href="/2023/05/23/Image2Poem/"/>
      <url>/2023/05/23/Image2Poem/</url>
      
        <content type="html"><![CDATA[<h1 id="Image-to-Poem"><a href="#Image-to-Poem" class="headerlink" title="Image-to-Poem"></a>Image-to-Poem</h1><p>此情此景，何不吟诗一首？Image-to-Poem帮你完成！</p><p>项目链接：<br><a href="https://github.com/weiji-Feng/Image2Poem">https://github.com/weiji-Feng/Image2Poem</a></p><p>9.28之前可能会忙于升学，本项目暂不更新。希望可以在10.31号之前完成这个项目的全部功能。</p><h2 id="1-项目介绍"><a href="#1-项目介绍" class="headerlink" title="1. 项目介绍"></a>1. 项目介绍</h2><p>图像生成古诗(Image to Poem)，旨在为给定的图像自动生成符合图像内容的古诗句。</p><p>使用对比学习预训练的CLIP模型拥有良好的迁移应用和zero-shot能力，是打通图像-文本多模态的重要模型之一。 本项目使用<a href="https://github.com/openai/CLIP">CLIP模型</a>生成古诗意象关键词向量和图像向量。</p><p>初始版本的生成方法为：搜集一个古诗词意象关键词数据集(close-set)，然后通过text-encoder(图1.右) 生成对应的关键词向量。对给定的一张图像，同样通过Image-encoder即可得到图像向量。比较图像向量和每个关键词向量的余弦相似度，可以得到top-k个相关关键词。将关键词送入语言模型，自动生成一首诗。</p><p>这种提取关键词的操作将<strong>会大大损失图像的语义信息</strong>，进而影响语言模型的古诗生成。但由于图像-古诗对数据集非常匮乏，我们很难&lt;/u&gt;像Dalle模型一样&lt;/u&gt;，直接将CLIP模型Image-encoder的输出向量，通过一个MappingNet(在DALLE-2中就是prior模块)送入解码器(语言模型)。所以如果有更好的想法欢迎指点。<br><img src="https://github.com/openai/CLIP/raw/main/CLIP.png" alt="img.png"></p><p>由于古诗的特殊性，本项目重头训练了一个用于生成古诗文的Language Model，尝试了T5 model（223M）和GPT2 model（118M），现公开该预训练模型以供大家娱乐。</p><p>以上模型均可通过调用 <a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a> 的<code>transformers</code>导入。</p><h2 id="2-引用和致谢"><a href="#2-引用和致谢" class="headerlink" title="2. 引用和致谢"></a>2. 引用和致谢</h2><p>在项目完成期间，我参考并使用了以下项目，这里表示感谢！ </p><ul><li>数据集来源：<a href="https://github.com/THUNLP-AIPoet/CCPM">https://github.com/THUNLP-AIPoet/CCPM</a></li><li>CLIP预训练模型来源： <a href="https://github.com/OFA-Sys/Chinese-CLIP">https://github.com/OFA-Sys/Chinese-CLIP</a></li><li>GPT2预训练部分代码：<a href="https://github.com/Morizeyao/GPT2-Chinese">https://github.com/Morizeyao/GPT2-Chinese</a></li></ul><h2 id="3-使用说明和生成样例"><a href="#3-使用说明和生成样例" class="headerlink" title="3. 使用说明和生成样例"></a>3. 使用说明和生成样例</h2><h3 id="安装依赖库"><a href="#安装依赖库" class="headerlink" title="安装依赖库"></a>安装依赖库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch torchvision torchaudio</span><br><span class="line">pip install transformers</span><br><span class="line">pip install tqdm matplotlib</span><br></pre></td></tr></table></figure><p>如果希望尝试预训练语言模型, 建议安装<code>torch+cudaxx.x</code>的GPU版本。</p><h3 id="快速体验古诗生成"><a href="#快速体验古诗生成" class="headerlink" title="快速体验古诗生成"></a>快速体验古诗生成</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python img2poem.py --image_path ./datasets/images/feiliu.jpg --model_type T5 --model_path ./config/t5_config</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>--image_path</code>: 图片所在位置</li><li><code>--model_type</code>: 模型名称,目前可选用’T5’,’GPT2’</li><li><code>--model_path</code>: 模型所在文件夹</li></ul><h3 id="生成样例"><a href="#生成样例" class="headerlink" title="生成样例"></a>生成样例</h3><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/feiliu.jpg" alt></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">飞鹤度湖山，青松半掩关。水林昼景凤，诸君自有闲。</span><br><span class="line">白苹洲渚流，丹青未有人。水林壑夜深，乱峰高几重。 </span><br><span class="line">仙人问道踪，壑深自坐禅。丹青一片云，月随风水林。   </span><br><span class="line">不见青林路，却忆庐陵西。老松犹未分，钟山水似难。</span><br><span class="line">飞鹤度湖山，青松半掩关。丹壑千人在，犹记林水心。</span><br></pre></td></tr></table></figure><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/chun.jpg" alt="image-20230408233621777" style="zoom:30%;"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">青碧绕瑶池，碧峰回九关。晚风吹画船，映水长成芳。</span><br><span class="line">犹自在藏新，不知是旧人。暮天三百年，桃源一片春。</span><br><span class="line">数年三两枝，却羡玉龙飞。桃源水一津，斜晖又一年。</span><br><span class="line">千骑鹤归飞，一曲茅亭去。天上桃源路，玉龙归白杳。</span><br><span class="line">相逢一笑飞，知有桃源人。何如写玉龙，斜晖送晚风</span><br></pre></td></tr></table></figure><p><img src="https://github.com/weiji-Feng/Image2Poem/raw/main/datasets/images/pubu.jpg" alt="image-20230408233621777" style="zoom:90%;"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">清江起玉龙，时听瀑布声。飞曲满游子，一生天上流。</span><br><span class="line">萧然见山来，却恐归来晚。时见布泉飞，锦囊深掩门。</span><br><span class="line">时有一花村，俗人如此山。月溪风乱鸣，时有瀑布还。</span><br><span class="line">月明闲送风，瀑布飞仙雪。人间几度林，锦衣还说天。 </span><br><span class="line">谁人识此中，布泉老客行。月满江来路，不须频为谁。</span><br></pre></td></tr></table></figure><h2 id="4-一些解释"><a href="#4-一些解释" class="headerlink" title="4. 一些解释"></a>4. 一些解释</h2><ul><li>对于当前项目的评价<blockquote><p>提取关键词进行古诗生成是一个<strong>损失信息</strong>的过程，尤其是将图像映射到关键词的操作，损失了图像原本的语义(例如只能识别人，而不知道人在做什么)。所以效果上来看仍然差强人意。</p><p>没有给模型一些关于韵律、题材、体裁等的设定，导致不够专业。</p></blockquote></li><li><p>可不可以使用自己的古诗数据集尝试预训练？</p><blockquote><p>可以，不过由于CCPM数据集是<code>.json</code>文件格式,导入方式与<code>.txt</code>不同。所以在<code>datasets.py</code>文件里你需要重新写一下有关文件导入的部分。并且由于预训练方法多样，你也可以修改预训练时的一些策略。</p></blockquote></li><li><p>项目的预训练方法是什么？</p><blockquote><p>首先对于GPT2模型，常规预训练方法就是自回归，本项目尝试了mask关键词的方法，例如：</p><p><code>[CLS]关键词：明月 故乡 [EOS] 举头望明月，低头思故乡[SEP]</code> =&gt; <code>[CLS]关键词：明月 故乡 [EOS] 举头望[MASK][MASK]，低头思[MASK][MASK][SEP]</code></p><p>然后我额外对这些mask token的预测准确率进行了计算，加入了损失函数中。</p><p>对于T5模型，由于是encoder-decoder架构，我使用下列格式创建数据：</p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS][SEP]</code>, y = <code>[CLS]红豆生南国[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS][SEP]</code>, y = <code>[CLS]秋来发故枝[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS]秋来发故枝[EOS][SEP]</code>, y = <code>[CLS]愿君多采撷[EOS][SEP]</code></p><p>x = <code>[CLS]关键词：红豆 南国 发 愿君[EOS]红豆生南国[EOS]秋来发故枝[EOS]愿君多采撷[EOS][SEP]</code>, y = <code>[CLS]此物最相思[EOS][SEP]</code></p></blockquote></li><li><p>通过什么方式进行图像生成古诗？未来有什么进一步更新的方法？</p><blockquote><p>现在的实现比较简单，就是先搜集一个闭环的关键词数据集(<code>keyword.txt</code>)，然后使用CLIP对图像和所有关键词进行编码，计算它们之间的相似度，取相似度最高的K个关键词，然后放置于语言模型进行生成。</p><p>由于<code>图像-古诗对</code>数据集非常匮乏，似乎暂时做不到删去这个闭环关键词数据集。未来如果有充足的数据集，我会使用<code>CLIP-MappingNet-T5/GPT2</code>的模型架构进行训练，例如下图的<a href="https://arxiv.org/pdf/2111.09734.pdf">CLIPCap</a>架构：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*8RLzDpMfi6sLScqx2SguaA.png" alt></p></blockquote></li></ul><p>未来有古诗生成图像的想法，待进一步更新。现有的可以进行古诗生成图像的项目有：<a href="https://huggingface.co/IDEA-CCNL/Taiyi-Diffusion-532M-Nature-Chinese">https://huggingface.co/IDEA-CCNL/Taiyi-Diffusion-532M-Nature-Chinese</a></p>]]></content>
      
      
      <categories>
          
          <category> multi-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
